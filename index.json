[{"categories":["云原生"],"content":"全局关闭重试 istio默认重试2次 apiVersion: v1 data: mesh: |- ... ... defaultHttpRetryPolicy: attempts: 0 ... ... ","date":"2023-07-17","objectID":"/posts/03a16b/:0:1","tags":["istio"],"title":"istio全局配置","uri":"/posts/03a16b/"},{"categories":["云原生"],"content":"全局配置Access log apiVersion: v1 data: mesh: |- accessLogEncoding: JSON accessLogFile: /dev/stdout accessLogFormat: \"{\\\"authority\\\":\\\"%REQ(:AUTHORITY)%\\\",\\\"bytes_received\\\":\\\"%BYTES_RECEIVED%\\\",\\\"bytes_sent\\\":\\\"%BYTES_SENT%\\\",\\\"downstream_local_address\\\":\\\"%DOWNSTREAM_LOCAL_ADDRESS%\\\",\\\"downstream_remote_address\\\":\\\"%DOWNSTREAM_REMOTE_ADDRESS%\\\",\\\"duration\\\":\\\"%DURATION%\\\",\\\"istio_policy_status\\\":\\\"%DYNAMIC_METADATA(istio.mixer:status)%\\\",\\\"method\\\":\\\"%REQ(:METHOD)%\\\",\\\"path\\\":\\\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\\\",\\\"protocol\\\":\\\"%PROTOCOL%\\\",\\\"request_id\\\":\\\"%REQ(X-REQUEST-ID)%\\\",\\\"requested_server_name\\\":\\\"%REQUESTED_SERVER_NAME%\\\",\\\"response_code\\\":\\\"%RESPONSE_CODE%\\\",\\\"response_flags\\\":\\\"%RESPONSE_FLAGS%\\\",\\\"route_name\\\":\\\"%ROUTE_NAME%\\\",\\\"start_time\\\":\\\"%START_TIME%\\\",\\\"upstream_cluster\\\":\\\"%UPSTREAM_CLUSTER%\\\",\\\"upstream_host\\\":\\\"%UPSTREAM_HOST%\\\",\\\"upstream_local_address\\\":\\\"%UPSTREAM_LOCAL_ADDRESS%\\\",\\\"upstream_service_time\\\":\\\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\\\",\\\"upstream_transport_failure_reason\\\":\\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\",\\\"user_agent\\\":\\\"%REQ(USER-AGENT)%\\\",\\\"x_forwarded_for\\\":\\\"%REQ(X-FORWARDED-FOR)%\\\"}\" ... ","date":"2023-07-17","objectID":"/posts/03a16b/:0:2","tags":["istio"],"title":"istio全局配置","uri":"/posts/03a16b/"},{"categories":["云原生"],"content":"Sidecar 生命周期 Istio 中 Envoy 就绪后启动应用容器 在 kubernetes 中，Pod 中的 containers 启动就绪顺序是不确定的。如果应用容器就绪的时候，代理容器没有就绪，那么流量会失败。 Istio 1.7版本中引入配置 holdApplicationUntilProxyStarts，它使 Sidecar 注入器在 Pod 的容器列表的开始处注入 Sidecar，并配置它阻止所有其他容器的启动，直到代理准备好为止。默认情况下禁用此选项。 (Issue #11130) Istio 中 Envoy 的退出机制以及配置优雅退出 缺省情况下，在收到 SIGTERM 后，Istio-agent 会在等待 terminationDrainDuration (缺省 5S）后退出，由于 Envoy 是 Istio-agent 的子进程，Envoy 也会随之退出。该缺省行为可能对于一些耗时较长的关键业务有影响，导致正在进行业务处理的链接被强制中断。 Istio 1.12 版本中引入环境变量 EXIT_ON_ZERO_ACTIVE_CONNECTIONS 以在排空期间跟踪活动连接，并在活动连接变为零时退出，而不是等待整个排空持续进行。默认情况下这是禁用的issues #34855 apiVersion: v1 data: mesh: |- defaultConfig: holdApplicationUntilProxyStarts: true proxyMetadata: EXIT_ON_ZERO_ACTIVE_CONNECTIONS: \"true\" istio全局配置 ","date":"2023-07-17","objectID":"/posts/03a16b/:0:3","tags":["istio"],"title":"istio全局配置","uri":"/posts/03a16b/"},{"categories":["云原生"],"content":"常用脚本 由于线上主机初始化没有安装 socat, 在使用istioctl是无法转发端口,所以直接导出 Envoy 的配置，然后在使用 istioctl操作。 ","date":"2023-07-17","objectID":"/posts/5890/:1:0","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"导出config kubectl exec -ti -n zsl-test mydemo-my-demo-sgcanshu-4vb7b -c istio-proxy -- curl http://127.0.0.1:15000/config_dump \u003e config_dump.json ","date":"2023-07-17","objectID":"/posts/5890/:1:1","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"查看sidecar证书是否正常 # ./istioctl proxy-config secret -f config_dump.json RESOURCE NAME TYPE STATUS VALID CERT SERIAL NUMBER NOT AFTER NOT BEFORE default Cert Chain ACTIVE true 219345628773408727192682202221320774018 2023-07-18T01:56:01Z 2023-07-17T01:54:01Z ROOTCA CA ACTIVE true 17390732964404848583 2031-12-13T09:40:24Z 2021-12-15T09:40:24Z ","date":"2023-07-17","objectID":"/posts/5890/:1:2","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"查看sidecar证书详细 ./istioctl proxy-config secret -f config_dump.json -o json| jq '[.dynamicActiveSecrets[] | select(.name == \"default\")][0].secret.tlsCertificate.certificateChain.inlineBytes' -r | base64 -d | openssl x509 -noout -text Certificate: Data: Version: 3 (0x2) Serial Number: a5:04:6e:74:53:d1:3e:4c:c9:03:67:67:2e:2e:a1:82 Signature Algorithm: sha256WithRSAEncryption Issuer: O=Istio, CN=Intermediate CA, L=dev Validity Not Before: Jul 17 01:54:01 2023 GMT Not After : Jul 18 01:56:01 2023 GMT Subject: Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:ce:0d:56:32:6d:62:c0:27:df:eb:fa:21:03:1d: da:86:44:10:e5:89:5d:b1:2d:05:d7:5b:0d:4f:68: 6e:27:39:46:2f:1c:6a:fd:ee:b3:29:e8:d8:5e:db: 8c:d3:23:64:71:64:9a:11:d5:65:81:08:d0:55:24: 2e:88:1d:08:d3:3c:20:57:a9:c4:9e:fe:8d:4e:8e: 81:03:e0:e8:f2:0d:05:13:a5:1b:9b:54:18:73:ea: 13:b2:9b:f0:63:34:23:77:eb:db:bc:fb:a8:56:2d: 30:be:0c:55:ce:9b:38:ca:ba:05:03:9e:6a:88:26: 4f:6c:49:18:00:d0:c7:41:a8:6d:73:96:76:ce:a5: ee:35:98:8a:b7:d0:c1:37:11:1e:3f:ff:42:e2:29: 23:9c:3e:0c:ad:9a:70:56:b6:4f:dd:24:a4:17:19: 33:c1:7b:cb:4d:2a:10:2a:20:6c:1f:1b:78:55:5a: a1:88:c0:d8:00:65:be:1c:df:5d:30:02:77:0c:c3: df:8e:59:78:2e:a1:31:29:2b:2c:ce:7f:80:74:18: 4a:e3:db:34:b3:db:7e:1d:02:15:6d:b1:46:6f:e2: 4f:60:c4:d9:1c:e7:29:86:6e:3c:b7:7a:12:e7:38: 71:6e:36:ae:46:68:c7:a8:ea:7e:85:1f:6c:3b:0e: ab:81 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:C0:C7:D4:2D:86:25:EF:D9:AF:0A:76:7B:CB:99:52:FB:67:C7:E0:6E X509v3 Subject Alternative Name: critical URI:spiffe://cluster.local/ns/zsl-test/sa/default Signature Algorithm: sha256WithRSAEncryption 10:9d:85:38:7f:7f:7b:b2:1a:e1:69:de:68:f9:87:f2:64:c8: 0b:c6:ab:9c:9b:bf:2a:d3:c6:f1:b8:e1:fd:25:15:cf:05:5d: 17:73:79:44:fe:dd:b4:28:cf:70:6d:f7:58:91:85:27:33:02: 7b:95:1a:90:19:0c:1c:7b:03:6a:fa:da:a6:9b:59:a5:23:40: 4b:03:34:aa:20:65:18:de:8c:14:19:f4:7c:cb:04:83:a0:af: c5:43:bf:93:94:ae:51:b7:92:a3:17:b3:10:4f:a0:f4:44:d4: b3:24:aa:6b:1e:fd:cb:7a:50:9c:7d:1b:ac:be:dc:b3:2f:1e: 2f:76:ea:a5:81:a2:a8:ee:0b:d2:a6:cb:15:e4:23:08:66:e3: 3f:66:c2:ca:9a:71:d2:d9:5e:20:0d:bc:21:21:94:e3:68:1d: fb:c8:de:d2:7c:78:68:7d:d1:5f:44:26:98:cc:3c:41:9b:dd: cd:5f:73:cb:8a:56:a5:72:e5:21:27:49:a2:8b:3b:1c:06:c3: f8:7b:98:10:7c:59:b2:6c:05:4e:ec:eb:d8:9f:33:19:4b:3e: 98:08:17:fa:bf:1a:3a:b7:76:8e:c2:7b:84:be:d2:f8:b8:55: b1:f9:72:7b:e5:91:50:1a:09:bf:1c:b3:05:72:53:7b:f4:e2: bd:66:d3:7e:5c:70:50:8b:b1:91:04:30:d5:52:74:fd:2a:18: 56:7d:a2:83:a9:25:d7:d4:0d:f4:45:5f:1f:a2:bd:52:7e:bf: 1b:99:92:63:20:cb:6f:18:24:83:4b:fa:1b:ab:49:44:11:58: c2:ca:9f:4e:c4:8e:ea:18:09:a7:84:6b:75:ef:ea:fd:54:d9: 1e:36:b2:89:70:35:fb:f1:b3:c5:5e:e5:75:7b:a4:8b:1d:f6: 3b:13:92:90:de:09:3d:15:dd:e8:3c:e9:e6:50:28:17:88:85: 12:c4:80:c9:fe:ca:10:ae:eb:37:ed:5c:13:a4:f5:0d:6e:41: 4f:84:b3:94:1e:ea:a0:52:47:1d:6c:8e:fe:f5:1e:91:0e:7a: fe:31:17:97:b5:04:1c:d4:27:af:d1:df:3f:e4:38:2d:78:11: aa:d0:54:50:e8:4d:bf:10:55:7b:3d:29:18:e4:a8:f1:f7:e5: c1:b1:41:de:6c:8b:d1:0e:ca:2f:69:5d:b8:08:da:fa:30:f9: 1a:5f:c2:93:56:52:1d:39:41:f7:0c:c9:c7:d7:bc:33:99:cc: 91:52:95:0b:35:0b:a9:10:29:66:0a:93:4e:50:ee:83:25:4c: 92:ff:6f:94:3d:f2:39:9c:3a:5b:0a:38:c1:c5:bd:50:b8:4d: 27:ea:f9:8f:89:e4:4c:8a ","date":"2023-07-17","objectID":"/posts/5890/:1:3","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"常用yaml ","date":"2023-07-17","objectID":"/posts/5890/:2:0","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"sidecar 注入相关 为指定 workload 取消 sidecar 自动注入 kubectl label ns zsl-test istio-injection=enabled template: metadata: annotations: sidecar.istio.io/inject: \"false\" ","date":"2023-07-17","objectID":"/posts/5890/:2:1","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"proxy 相关 自定义资源 template: metadata: annotations: \"sidecar.istio.io/proxyCPU\": \"10m\" \"sidecar.istio.io/proxyCPULimit\": \"2\" \"sidecar.istio.io/proxyMemory\": \"32Mi\" \"sidecar.istio.io/proxyMemoryLimit\": \"1Gi\" 自定义日志级别 template: metadata: annotations: \"sidecar.istio.io/logLevel\": debug # 可选: trace, debug, info, warning, error, critical, off \"sidecar.istio.io/componentLogLevel\": \"ext_authz:trace,filter:debug\" 不劫持部分流量 template: metadata: annotations: traffic.sidecar.istio.io/excludeOutboundIPRanges: \"10.10.31.1/32,10.10.31.2/32\" # 不劫持的出站目标地址 traffic.sidecar.istio.io/excludeInterfaces: \"\" # 过滤网卡接口 traffic.sidecar.istio.io/excludeInboundPorts: \"\" # 不劫持的入站端口 traffic.sidecar.istio.io/excludeOutboundPorts: \"\" # 不劫持的出站端口 traffic.sidecar.istio.io/includeInboundPorts: \"\" # 只劫持指定入站端口 traffic.sidecar.istio.io/includeOutboundPorts: \"\" # 只劫持指定出站端口 traffic.sidecar.istio.io/includeOutboundIPRanges: \"\" # 只劫持指定出站目标地址 ","date":"2023-07-17","objectID":"/posts/5890/:2:2","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"全局禁用mTLS apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: istio-system spec: mtls: mode: DISABLE ","date":"2023-07-17","objectID":"/posts/5890/:2:3","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"常用EnvoyFilter ","date":"2023-07-17","objectID":"/posts/5890/:3:0","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"请求限制 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: http-options namespace: istio-system spec: configPatches: - applyTo: NETWORK_FILTER match: context: ANY listener: filterChain: filter: name: \"envoy.http_connection_manager\" patch: operation: MERGE value: typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\" max_request_headers_kb: 96 # 96KB, 请求 header 最大限制 - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.http_connection_manager\" patch: operation: INSERT_BEFORE value: name: \"envoy.filters.http.buffer\" typed_config: '@type': \"type.googleapis.com/envoy.extensions.filters.http.buffer.v3.Buffer\" max_request_bytes: 1048576 # 1MB, 请求最大限制 ","date":"2023-07-17","objectID":"/posts/5890/:3:1","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"保留请求头大小写 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: header-casing namespace: istio-system spec: configPatches: - applyTo: CLUSTER match: context: SIDECAR_INBOUND patch: operation: MERGE value: typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: '@type': type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http_protocol_options: header_key_format: stateful_formatter: name: preserve_case typed_config: '@type': type.googleapis.com/envoy.extensions.http.header_formatters.preserve_case.v3.PreserveCaseFormatterConfig - applyTo: NETWORK_FILTER match: listener: filterChain: filter: name: envoy.filters.network.http_connection_manager patch: operation: MERGE value: typed_config: '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager http_protocol_options: header_key_format: stateful_formatter: name: preserve_case typed_config: '@type': type.googleapis.com/envoy.extensions.http.header_formatters.preserve_case.v3.PreserveCaseFormatterConfig ","date":"2023-07-17","objectID":"/posts/5890/:3:2","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"高频使用链接 ","date":"2023-07-17","objectID":"/posts/5890/:4:0","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"istio 相关 云原生学院B站视频 istio 端口列表 istio annotation 列表 istio 全局配置 istio 与 k8s 版本兼容性矩阵 ","date":"2023-07-17","objectID":"/posts/5890/:4:1","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["云原生"],"content":"Envoy 相关 Envoy 15000 管理端口接口列表 Envoy 响应码标识 ","date":"2023-07-17","objectID":"/posts/5890/:4:2","tags":["istio","servicemesh"],"title":"istio常用操作","uri":"/posts/5890/"},{"categories":["kubernetes源码"],"content":"基于1.27.0版本 ","date":"2023-07-07","objectID":"/posts/44f039/:0:0","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"Kind 创建集群 cat \u003c\u003c EOF \u003e dev.yaml kind: Cluster apiVersion: \"kind.x-k8s.io/v1alpha4\" kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration metadata: name: dev imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: \"10.8.0.0/16\" serviceSubnet: \"10.9.0.0/16\" nodes: - role: control-plane image: kindest/node:v1.27.0@sha256:c6b22e613523b1af67d4bc8a0c38a4c3ea3a2b8fbc5b367ae36345c9cb844518 extraPortMappings: - containerPort: 2379 hostPort: 2379 protocol: TCP EOF kind create cluster --name dev --config=dev.yaml ","date":"2023-07-07","objectID":"/posts/44f039/:1:0","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"配置文件 将kind创建的kubernetes集群配置拷贝到本地 docker cp dev-control-plane:/etc/kubernetes/ ./config ","date":"2023-07-07","objectID":"/posts/44f039/:2:0","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"Vscode 配置调试模式 { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"kube-scheduler\", \"type\": \"go\", \"cwd\": \"${workspaceRoot}\", \"request\": \"launch\", \"mode\": \"auto\", \"program\": \"cmd/kube-scheduler/scheduler.go\", \"env\": { \"PATH\": \"${PATH}:~/.gvm/gos/go1.20/bin/\" }, \"args\": [ \"--authentication-kubeconfig=config/scheduler.conf\", \"--authorization-kubeconfig=config/scheduler.conf\", \"--kubeconfig=config/scheduler.conf\", \"--leader-elect=true\", \"--bind-address=127.0.0.1\" ] }, { \"name\": \"kube-apiserver\", \"type\": \"go\", \"cwd\": \"${workspaceRoot}\", \"request\": \"launch\", \"mode\": \"auto\", \"program\": \"cmd/kube-scheduler/apiserver.go\", \"env\": { \"PATH\": \"${PATH}:~/.gvm/gos/go1.20/bin/\" }, \"args\": [ \"--advertise-address=0.0.0.0\", \"--allow-privileged=true\", \"--authorization-mode=Node,RBAC\", \"--client-ca-file=config/pki/ca.crt\", \"--enable-admission-plugins=NodeRestriction\", \"--enable-bootstrap-token-auth=true\", \"--etcd-cafile=config/pki/etcd/ca.crt\", \"--etcd-certfile=config/pki/apiserver-etcd-client.crt\", \"--etcd-keyfile=config/pki/apiserver-etcd-client.key\", \"--etcd-servers=https://127.0.0.1:2379\", \"--kubelet-client-certificate=config/pki/apiserver-kubelet-client.crt\", \"--kubelet-client-key=config/pki/apiserver-kubelet-client.key\", \"--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\", \"--proxy-client-cert-file=config/pki/front-proxy-client.crt\", \"--proxy-client-key-file=config/pki/front-proxy-client.key\", \"--requestheader-allowed-names=front-proxy-client\", \"--requestheader-client-ca-file=config/pki/front-proxy-ca.crt\", \"--requestheader-extra-headers-prefix=X-Remote-Extra-\", \"--requestheader-group-headers=X-Remote-Group\", \"--requestheader-username-headers=X-Remote-User\", \"--secure-port=16443\", \"--service-account-issuer=https://kubernetes.default.svc.cluster.local\", \"--service-account-key-file=config/pki/sa.pub\", \"--service-account-signing-key-file=config/pki/sa.key\", \"--service-cluster-ip-range=10.9.0.0/16\", \"--tls-cert-file=config/pki/apiserver.crt\", \"--tls-private-key-file=config/pki/apiserver.key\" ] } ] } ","date":"2023-07-07","objectID":"/posts/44f039/:3:0","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"验证 ApiServer ","date":"2023-07-07","objectID":"/posts/44f039/:4:0","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"查看 API 接口 curl -k https://127.0.0.1:16443/ { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\", \"reason\": \"Forbidden\", \"details\": {}, \"code\": 403 } 这是由于匿名用户 anonymous 没有权限 ","date":"2023-07-07","objectID":"/posts/44f039/:4:1","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"匿名用户绑定admin权限 kubectl create clusterrolebinding DevAdmin --user system:anonymous --clusterrole cluster-admin ","date":"2023-07-07","objectID":"/posts/44f039/:4:2","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":["kubernetes源码"],"content":"再次查看 API 接口 curl -k https://127.0.0.1:16443/ { \"paths\": [ \"/.well-known/openid-configuration\", \"/api\", \"/api/v1\", \"/apis\", \"/apis/\", \"/apis/admissionregistration.k8s.io\", \"/apis/admissionregistration.k8s.io/v1\", \"/apis/apiextensions.k8s.io\", \"/apis/apiextensions.k8s.io/v1\", \"/apis/apiregistration.k8s.io\", \"/apis/apiregistration.k8s.io/v1\", \"/apis/apps\", \"/apis/apps/v1\", \"/apis/authentication.k8s.io\", \"/apis/authentication.k8s.io/v1\", \"/apis/authorization.k8s.io\", \"/apis/authorization.k8s.io/v1\", \"/apis/autoscaling\", \"/apis/autoscaling/v1\", \"/apis/autoscaling/v2\", \"/apis/batch\", \"/apis/batch/v1\", \"/apis/certificates.k8s.io\", \"/apis/certificates.k8s.io/v1\", \"/apis/coordination.k8s.io\", \"/apis/coordination.k8s.io/v1\", \"/apis/discovery.k8s.io\", \"/apis/discovery.k8s.io/v1\", \"/apis/events.k8s.io\", \"/apis/events.k8s.io/v1\", \"/apis/flowcontrol.apiserver.k8s.io\", \"/apis/flowcontrol.apiserver.k8s.io/v1beta2\", \"/apis/flowcontrol.apiserver.k8s.io/v1beta3\", \"/apis/networking.k8s.io\", \"/apis/networking.k8s.io/v1\", \"/apis/node.k8s.io\", \"/apis/node.k8s.io/v1\", \"/apis/policy\", \"/apis/policy/v1\", \"/apis/rbac.authorization.k8s.io\", \"/apis/rbac.authorization.k8s.io/v1\", \"/apis/scheduling.k8s.io\", \"/apis/scheduling.k8s.io/v1\", \"/apis/storage.k8s.io\", \"/apis/storage.k8s.io/v1\", \"/healthz\", \"/healthz/autoregister-completion\", \"/healthz/etcd\", \"/healthz/log\", \"/healthz/ping\", \"/healthz/poststarthook/aggregator-reload-proxy-client-cert\", \"/healthz/poststarthook/apiservice-discovery-controller\", \"/healthz/poststarthook/apiservice-openapi-controller\", \"/healthz/poststarthook/apiservice-openapiv3-controller\", \"/healthz/poststarthook/apiservice-registration-controller\", \"/healthz/poststarthook/apiservice-status-available-controller\", \"/healthz/poststarthook/bootstrap-controller\", \"/healthz/poststarthook/crd-informer-synced\", \"/healthz/poststarthook/generic-apiserver-start-informers\", \"/healthz/poststarthook/kube-apiserver-autoregistration\", \"/healthz/poststarthook/priority-and-fairness-config-consumer\", \"/healthz/poststarthook/priority-and-fairness-config-producer\", \"/healthz/poststarthook/priority-and-fairness-filter\", \"/healthz/poststarthook/rbac/bootstrap-roles\", \"/healthz/poststarthook/scheduling/bootstrap-system-priority-classes\", \"/healthz/poststarthook/start-apiextensions-controllers\", \"/healthz/poststarthook/start-apiextensions-informers\", \"/healthz/poststarthook/start-cluster-authentication-info-controller\", \"/healthz/poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector\", \"/healthz/poststarthook/start-kube-aggregator-informers\", \"/healthz/poststarthook/start-kube-apiserver-admission-initializer\", \"/healthz/poststarthook/start-kube-apiserver-identity-lease-controller\", \"/healthz/poststarthook/start-kube-apiserver-identity-lease-garbage-collector\", \"/healthz/poststarthook/start-legacy-token-tracking-controller\", \"/healthz/poststarthook/start-system-namespaces-controller\", \"/healthz/poststarthook/storage-object-count-tracker-hook\", \"/livez\", \"/livez/autoregister-completion\", \"/livez/etcd\", \"/livez/log\", \"/livez/ping\", \"/livez/poststarthook/aggregator-reload-proxy-client-cert\", \"/livez/poststarthook/apiservice-discovery-controller\", \"/livez/poststarthook/apiservice-openapi-controller\", \"/livez/poststarthook/apiservice-openapiv3-controller\", \"/livez/poststarthook/apiservice-registration-controller\", \"/livez/poststarthook/apiservice-status-available-controller\", \"/livez/poststarthook/bootstrap-controller\", \"/livez/poststarthook/crd-informer-synced\", \"/livez/poststarthook/generic-apiserver-start-informers\", \"/livez/poststarthook/kube-apiserver-autoregistration\", \"/livez/poststarthook/priority-and-fairness-config-consumer\", \"/livez/poststarthook/priority-and-fairness-config-producer\", \"/livez/poststarthook/priority-and-fairness-filter\", \"/livez/poststarthook/rbac/bootstrap-roles\", \"/livez/poststarthook/scheduling/bootstrap-system-priority-classes\", \"/livez/poststarthook/start-apiextensions-controllers\", \"/livez/poststarthook/sta","date":"2023-07-07","objectID":"/posts/44f039/:4:3","tags":["kubernetes"],"title":"01-利用vscode与kind搭建kubernetes开发环境","uri":"/posts/44f039/"},{"categories":[],"content":"环境准备 利用 kind 快速创建 kubernetes 集群环境 LOCAL_IP=172.25.163.181 ","date":"2023-06-30","objectID":"/posts/02c894/:1:0","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"创建控制面集群 cat \u003c\u003c EOF \u003e controler.yaml kind: Cluster apiVersion: \"kind.x-k8s.io/v1alpha4\" kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration metadata: name: controler imageRepository: registry.aliyuncs.com/google_containers networking: apiServerAddress: ${LOCAL_IP} podSubnet: \"10.8.0.0/16\" serviceSubnet: \"10.9.0.0/16\" nodes: - role: control-plane image: kindest/node:v1.21.1@sha256:69860bda5563ac81e3c0057d654b5253219618a22ec3a346306239bba8cfa1a6 extraPortMappings: - containerPort: 5443 hostPort: 5443 protocol: TCP EOF kind create cluster --name controler --config=controler.yaml ","date":"2023-06-30","objectID":"/posts/02c894/:1:1","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"创建成员(member1)集群 cat \u003c\u003c EOF \u003e member1.yaml kind: Cluster apiVersion: \"kind.x-k8s.io/v1alpha4\" kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration metadata: name: member1 imageRepository: registry.aliyuncs.com/google_containers networking: apiServerAddress: ${LOCAL_IP} podSubnet: \"10.10.0.0/16\" serviceSubnet: \"10.11.0.0/16\" nodes: - role: control-plane image: kindest/node:v1.20.15@sha256:a32bf55309294120616886b5338f95dd98a2f7231519c7dedcec32ba29699394 EOF kind create cluster --name member1 --config=member1.yaml ","date":"2023-06-30","objectID":"/posts/02c894/:1:2","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"创建成员(member2)集群 cat \u003c\u003c EOF \u003e member2.yaml kind: Cluster apiVersion: \"kind.x-k8s.io/v1alpha4\" kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration metadata: name: member2 imageRepository: registry.aliyuncs.com/google_containers networking: apiServerAddress: ${LOCAL_IP} podSubnet: \"10.12.0.0/16\" serviceSubnet: \"10.13.0.0/16\" nodes: - role: control-plane image: kindest/node:v1.20.15@sha256:a32bf55309294120616886b5338f95dd98a2f7231519c7dedcec32ba29699394 EOF kind create cluster --name member2 --config=member2.yaml ","date":"2023-06-30","objectID":"/posts/02c894/:1:3","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"获取集群kubeconfig kind get kubeconfig --name=member1 \u003e member1.conf kind get kubeconfig --name=member2 \u003e member2.conf ","date":"2023-06-30","objectID":"/posts/02c894/:1:4","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"拉取镜像并加载到控制面集群 docker pull docker.io/karmada/karmada-aggregated-apiserver:v1.6.0 docker pull registry.k8s.io/kube-apiserver:v1.25.4 docker pull docker.io/karmada/karmada-scheduler:v1.6.0 docker pull docker.io/karmada/karmada-webhook:v1.6.0 docker pull registry.k8s.io/kube-controller-manager:v1.25.4 docker pull registry.k8s.io/etcd:3.5.3-0 docker pull docker.io/karmada/karmada-controller-manager:v1.6.0 docker pull docker.io/cfssl/cfssl:latest docker pull docker.io/bitnami/kubectl:latest kind load docker-image docker.io/karmada/karmada-aggregated-apiserver:v1.6.0 --name controler kind load docker-image registry.k8s.io/kube-apiserver:v1.25.4 --name controler kind load docker-image docker.io/karmada/karmada-scheduler:v1.6.0 --name controler kind load docker-image docker.io/karmada/karmada-webhook:v1.6.0 --name controler kind load docker-image registry.k8s.io/kube-controller-manager:v1.25.4 --name controler kind load docker-image registry.k8s.io/etcd:3.5.3-0 --name controler kind load docker-image docker.io/karmada/karmada-controller-manager:v1.6.0 --name controler kind load docker-image cfssl/cfssl:v1.6.4 --name controler kind load docker-image docker.io/bitnami/kubectl:1.25.4 --name controler docker exec -ti controler-control-plane bash root@controler-control-plane:/# crictl images IMAGE TAG IMAGE ID SIZE docker.io/karmada/karmada-aggregated-apiserver v1.6.0 3582a8dea29f3 91.4MB docker.io/karmada/karmada-controller-manager v1.6.0 e1df9f3a30f17 92MB docker.io/karmada/karmada-scheduler v1.6.0 1e3b84078d2d1 70.6MB docker.io/karmada/karmada-webhook v1.6.0 da3eec9e5aed7 67.5MB docker.io/kindest/kindnetd v20210326-1e038dc5 f37b7c809e5dc 54.8MB docker.io/rancher/local-path-provisioner v0.0.14 2b703ea309660 12.3MB k8s.gcr.io/build-image/debian-base v2.1.0 3cc9c70b44747 22.8MB k8s.gcr.io/coredns/coredns v1.8.0 1a1f05a2cd7c2 11.6MB k8s.gcr.io/etcd 3.4.13-0 05b738aa1bc63 135MB k8s.gcr.io/kube-apiserver v1.21.1 18e61c783b417 118MB k8s.gcr.io/kube-controller-manager v1.21.1 0c6dccae49de8 113MB k8s.gcr.io/kube-proxy v1.21.1 4bbef4ca108cd 128MB k8s.gcr.io/kube-scheduler v1.21.1 8c783dd252088 48.7MB k8s.gcr.io/pause 3.5 f7ff3c4042631 253kB registry.k8s.io/etcd 3.5.3-0 a9a710bb96df0 180MB registry.k8s.io/kube-apiserver v1.25.4 8e49cdf98f4d1 125MB registry.k8s.io/kube-controller-manager v1.25.4 8296621317758 114MB ","date":"2023-06-30","objectID":"/posts/02c894/:1:5","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"控制集群安装Karmada helm repo add karmada-charts https://raw.githubusercontent.com/karmada-io/karmada/master/charts helm --namespace karmada-system upgrade -i karmada karmada-charts/karmada --version=1.6.0 --create-namespace kubectl get pod -n karmada-system NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 72s karmada-aggregated-apiserver-5dbbcbb459-j95p7 1/1 Running 0 72s karmada-apiserver-7796648bc4-ksl6j 1/1 Running 0 72s karmada-controller-manager-57f965864b-2gbjm 1/1 Running 0 30s karmada-kube-controller-manager-578888599b-hctr9 1/1 Running 2 72s karmada-scheduler-76b769f497-6x9nn 1/1 Running 0 72s karmada-webhook-7fd6fd5cb8-dt929 1/1 Running 1 72s 获取Karmada集群的kubeconfig kubectl get secret -n karmada-system karmada-kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u003e karmada-apiserver kubecm add -f karmada-apiserver kubecm switch karmada-apiserver ","date":"2023-06-30","objectID":"/posts/02c894/:1:6","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"安装CLI工具 kubectl krew install karmada curl -s https://raw.githubusercontent.com/karmada-io/karmada/master/hack/install-cli.sh | sudo bash ","date":"2023-06-30","objectID":"/posts/02c894/:1:7","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":[],"content":"添加成员集群 kubectl karmada join member1 --cluster-context=kind-member1 kubectl karmada join member2 --cluster-context=kind-member2 ➜ demo git:(v1.6.0) ✗ docker cp member2-control-plane:/etc/kubernetes/admin.conf member1.conf ➜ demo git:(v1.6.0) ✗ docker cp member2-control-plane:/etc/kubernetes/admin.conf member2.conf ➜ demo git:(v1.6.0) ✗ docker cp member1-control-plane:/etc/kubernetes/admin.conf member2.conf ➜ demo git:(v1.6.0) ✗ docker cp member1.conf controler-control-plane:/tmp ➜ demo git:(v1.6.0) ✗ docker cp member2.conf controler-control-plane:/tmp ","date":"2023-06-30","objectID":"/posts/02c894/:1:8","tags":[],"title":"karmada快速体验","uri":"/posts/02c894/"},{"categories":["Karmada"],"content":"安装 Karmada 可以通过参考快速开始来安装Karmada ","date":"2023-06-29","objectID":"/posts/2296fc/:1:0","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"成员集群网络 确保至少有两个集群被添加到 Karmada，并且成员集群之间的容器网络可相互连接。 如果你使用 hack/local-up-karmada.sh 脚本来部署 Karmada，Karmada 将有三个成员集群，member1 和 member2 的容器网络将被连接。 你可以使用 Submariner 或其他相关的开源项目来连接成员集群之间的网络。 注意：为了防止路由冲突，集群之间的Pod和Service CIDR需要满足不重叠。 ","date":"2023-06-29","objectID":"/posts/2296fc/:2:0","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"安装 ServiceExport 和 ServiceImport CRD export KUBECONFIG=~/.kube/karmada.config kubectl config use-context karmada-apiserver kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/v0.1.0/config/crd/multicluster.x-k8s.io_serviceexports.yaml ","date":"2023-06-29","objectID":"/posts/2296fc/:3:0","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"Karmada 控制面分发到成员集群 cat policy.yaml # propagate ServiceExport CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceexport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceexports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2 --- # propagate ServiceImport CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceimport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceimports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2 --- # propagate propagationpolicies CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: propagationpolicies-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: propagationpolicies.policy.karmada.io placement: clusterAffinity: clusterNames: - member1 - member2 kubectl --kubeconfig ~/.kube/karmada.config --context karmada-apiserver policy.yaml 查看成员集群crd ➜ demo git:(v1.6.0) ✗ kubectl --kubeconfig ~/.kube/members.config --context member2 get crd NAME CREATED AT propagationpolicies.policy.karmada.io 2023-06-29T09:14:38Z serviceexports.multicluster.x-k8s.io 2023-06-29T08:56:26Z serviceimports.multicluster.x-k8s.io 2023-06-29T08:56:26Z ➜ demo git:(v1.6.0) ✗ kubectl --kubeconfig ~/.kube/members.config --context member1 get crd NAME CREATED AT propagationpolicies.policy.karmada.io 2023-06-29T09:14:38Z serviceexports.multicluster.x-k8s.io 2023-06-29T08:56:26Z serviceimports.multicluster.x-k8s.io 2023-06-29T08:56:26Z ","date":"2023-06-29","objectID":"/posts/2296fc/:3:1","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"示例 ","date":"2023-06-29","objectID":"/posts/2296fc/:4:0","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"在member1集群上部署服务 apiVersion: apps/v1 kind: Deployment metadata: name: serve spec: replicas: 1 selector: matchLabels: app: serve template: metadata: labels: app: serve spec: containers: - name: serve image: jeremyot/serve:0a40de8 args: - \"--message='hello from cluster member1 (Node: {{env \\\"NODE_NAME\\\"}} Pod: {{env \\\"POD_NAME\\\"}} Address: {{addr}})'\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name --- apiVersion: v1 kind: Service metadata: name: serve spec: ports: - port: 80 targetPort: 8080 selector: app: serve --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: mcs-workload spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: serve - apiVersion: v1 kind: Service name: serve placement: clusterAffinity: clusterNames: - member1 查看Pod信息 kubectl --kubeconfig ~/.kube/members.config --context member1 get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES serve-5899cfd5cd-lj6ls 1/1 Running 0 26m 10.10.0.5 member1-control-plane \u003cnone\u003e \u003cnone\u003e ","date":"2023-06-29","objectID":"/posts/2296fc/:4:1","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"导出服务到 member2 集群 member1 导出服务 在karmada控制平面上创建一个 ServiceExport 对象，然后创建一个 PropagationPolicy ，将 ServiceExport 对象分发到 member1 集群。 apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: serve --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: serve-export-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport name: serve placement: clusterAffinity: clusterNames: - member1 member2 导入服务 在karmada控制平面上创建一个 ServiceImport 对象，然后创建一个 PropagationPlicy 来分发 ServiceImport 对象到 member2 集群。 apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport metadata: name: serve spec: type: ClusterSetIP ports: - port: 80 protocol: TCP --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: serve-import-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport name: serve placement: clusterAffinity: clusterNames: - member2 ","date":"2023-06-29","objectID":"/posts/2296fc/:4:2","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["Karmada"],"content":"从 member2 集群获取服务 在 member2 集群上启动一个Pod request来访问派生服务的ClusterIP # 我们可以在member2集群中找到对应的派生服务。 $ kubectl --kubeconfig ~/.kube/members.config --context member2 get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE derived-serve ClusterIP 10.13.74.91 \u003cnone\u003e 80/TCP 31m kubernetes ClusterIP 10.13.0.1 \u003cnone\u003e 443/TCP 75m 例如，如果我们使用派生的服务名来持续访问该服务10s，将会得到如下输出: kubectl --kubeconfig ~/.kube/members.config --context member2 run -i --rm --restart=Never --image=jeremyot/request:0a40de8 request -- --duration=10s --address=derived-serve If you don't see a command prompt, try pressing enter. 2023/06/29 10:16:47 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:48 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:49 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:50 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:51 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:52 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:53 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:54 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 2023/06/29 10:16:55 'hello from cluster member1 (Node: member1-control-plane Pod: serve-5899cfd5cd-lj6ls Address: 10.10.0.5)' 可以看到请求返回信息跟 member1 部署的 Pod 信息是一致的 ","date":"2023-06-29","objectID":"/posts/2296fc/:4:3","tags":["Multi-Cluster"],"title":"Karmada 多集群服务发现","uri":"/posts/2296fc/"},{"categories":["监控体系"],"content":"Loki 可配置内容较多，相对复杂。本文通过分析 Loki 默认配置文件来了解一下。 ","date":"2023-06-27","objectID":"/posts/839f7c/:0:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"默认配置 # A comma-separated list of components to run. The default value 'all' runs Loki # in single binary mode. The value 'read' is an alias to run only read-path # related components such as the querier and query-frontend, but all in the same # process. The value 'write' is an alias to run only write-path related # components such as the distributor and compactor, but all in the same process. # Supported values: all, compactor, distributor, ingester, querier, # query-scheduler, ingester-querier, query-frontend, index-gateway, ruler, # table-manager, read, write. A full list of available targets can be printed # when running Loki with the '-list-targets' command line flag. # CLI flag: -target [target: \u003cstring\u003e | default = \"all\"] # Enables authentication through the X-Scope-OrgID header, which must be present # if true. If false, the OrgID will always be set to 'fake'. # CLI flag: -auth.enabled [auth_enabled: \u003cboolean\u003e | default = true] # The amount of virtual memory in bytes to reserve as ballast in order to # optimize garbage collection. Larger ballasts result in fewer garbage # collection passes, reducing CPU overhead at the cost of heap size. The ballast # will not consume physical memory, because it is never read from. It will, # however, distort metrics, because it is counted as live memory. # CLI flag: -config.ballast-bytes [ballast_bytes: \u003cint\u003e | default = 0] # Configures the server of the launched module(s). [server: \u003cserver\u003e] # Configures the distributor. [distributor: \u003cdistributor\u003e] # Configures the querier. Only appropriate when running all modules or just the # querier. [querier: \u003cquerier\u003e] # The query_scheduler block configures the Loki query scheduler. When configured # it separates the tenant query queues from the query-frontend. [query_scheduler: \u003cquery_scheduler\u003e] # The frontend block configures the Loki query-frontend. [frontend: \u003cfrontend\u003e] # The query_range block configures the query splitting and caching in the Loki # query-frontend. [query_range: \u003cquery_range\u003e] # The ruler block configures the Loki ruler. [ruler: \u003cruler\u003e] # The ingester_client block configures how the distributor will connect to # ingesters. Only appropriate when running all components, the distributor, or # the querier. [ingester_client: \u003cingester_client\u003e] # The ingester block configures the ingester and how the ingester will register # itself to a key value store. [ingester: \u003cingester\u003e] # The index_gateway block configures the Loki index gateway server, responsible # for serving index queries without the need to constantly interact with the # object store. [index_gateway: \u003cindex_gateway\u003e] # The storage_config block configures one of many possible stores for both the # index and chunks. Which configuration to be picked should be defined in # schema_config block. [storage_config: \u003cstorage_config\u003e] # The chunk_store_config block configures how chunks will be cached and how long # to wait before saving them to the backing store. [chunk_store_config: \u003cchunk_store_config\u003e] # Configures the chunk index schema and where it is stored. [schema_config: \u003cschema_config\u003e] # The compactor block configures the compactor component, which compacts index # shards for performance. [compactor: \u003ccompactor\u003e] # The limits_config block configures global and per-tenant limits in Loki. [limits_config: \u003climits_config\u003e] # The frontend_worker configures the worker - running within the Loki querier - # picking up and executing queries enqueued by the query-frontend. [frontend_worker: \u003cfrontend_worker\u003e] # The table_manager block configures the table manager for retention. [table_manager: \u003ctable_manager\u003e] # Configuration for memberlist client. Only applies if the selected kvstore is # memberlist. # # When a memberlist config with atleast 1 join_members is defined, kvstore of # type memberlist is automatically selected for all the components that require # a ring unless otherwise specified in the component's configuration section. [memberlist: \u003cmemberlist\u003e] # Configurati","date":"2023-06-27","objectID":"/posts/839f7c/:1:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"target 在 Loki系统架构中提到 Loki 是微服务架构，它所有组件都是可以独立运行。Loki 内部针对这些组件定义一组别名，可以通过这个参 数指定运行哪些组件,默认值: all 。 可以通过一下指令查看支持的别名以及别名对应的组件。 loki -list-targets $ /usr/bin/loki -config.file=/etc/loki/config/config.yaml -list-targets all cache-generation-loader compactor distributor ingester ingester-querier querier query-frontend query-scheduler ruler usage-report backend compactor index-gateway ingester-querier query-scheduler ruler usage-report cache-generation-loader compactor usage-report distributor usage-report index-gateway usage-report ingester usage-report ingester-querier overrides-exporter querier cache-generation-loader ingester-querier query-scheduler usage-report query-frontend cache-generation-loader query-scheduler usage-report query-scheduler usage-report read cache-generation-loader compactor index-gateway ingester-querier querier query-frontend query-scheduler ruler usage-report ruler ingester-querier usage-report table-manager usage-report usage-report write distributor ingester usage-report ","date":"2023-06-27","objectID":"/posts/839f7c/:2:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"auth_enabled 是否开启多租户认证,默认开启。如果开启会增加一个请求头 X-Scope-OrgID, 值为租户信息，如果不开启，X-Scope-OrgID 会设置为 fake。 ","date":"2023-06-27","objectID":"/posts/839f7c/:3:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"common # 定义后，指定前缀将出现在端点路径的前面 [path_prefix: \u003cstring\u003e | default = \"\"] # 不同 Loki 组件使用的公共存储配置 storage: # 支持 AWS S3 协议类型的对象存储，比如minio # The s3_storage_config block configures the connection to Amazon S3 object # storage backend. # The CLI flags prefix for this block configuration is: common [s3: \u003cs3_storage_config\u003e] # The gcs_storage_config block configures the connection to Google Cloud # Storage object storage backend. # The CLI flags prefix for this block configuration is: common.storage #Google Cloud Storage 配置 [gcs: \u003cgcs_storage_config\u003e] # The azure_storage_config block configures the connection to Azure object # storage backend. # The CLI flags prefix for this block configuration is: common.storage # Azure Storage 对象存储配置 [azure: \u003cazure_storage_config\u003e] # The alibabacloud_storage_config block configures the connection to Alibaba # Cloud Storage object storage backend. # 阿里云 OSS 对象存储配置 [alibabacloud: \u003calibabacloud_storage_config\u003e] # The bos_storage_config block configures the connection to Baidu Object # Storage (BOS) object storage backend. # The CLI flags prefix for this block configuration is: common. # 百度云 OSS 对象存储配置 [bos: \u003cbos_storage_config\u003e] # The swift_storage_config block configures the connection to OpenStack Object # Storage (Swift) object storage backend. # The CLI flags prefix for this block configuration is: common.storage # OpenStack 对象存储配置 [swift: \u003cswift_storage_config\u003e] # 本地文件系统配置 filesystem: # Directory to store chunks in. # CLI flag: -common.storage.filesystem.chunk-directory [chunks_directory: \u003cstring\u003e | default = \"\"] # Directory to store rules in. # CLI flag: -common.storage.filesystem.rules-directory [rules_directory: \u003cstring\u003e | default = \"\"] hedging: # If set to a non-zero value a second request will be issued at the provided # duration. Default is 0 (disabled) # CLI flag: -common.storage.hedge-requests-at [at: \u003cduration\u003e | default = 0s] # The maximum of hedge requests allowed. # CLI flag: -common.storage.hedge-requests-up-to [up_to: \u003cint\u003e | default = 2] # The maximum of hedge requests allowed per seconds. # CLI flag: -common.storage.hedge-max-per-second [max_per_second: \u003cint\u003e | default = 5] # 如果为 true，则 ingester、compactor 和 query_scheduler 的 ring tokens 将保存到 path_prefix 目录中的文件中。如果将其设置为 true 且 path_prefix 前缀为空，Loki 将出错。 [persist_tokens: \u003cboolean\u003e] # 传入到 ingester 组件的数据副本数 [replication_factor: \u003cint\u003e] # 所有 Loki rings 使用的通用 ring 配置。 # 如果指定了通用的 ring 环，则其值用于定义任何未定义的 ring 值。 # 例如，当公共部分中定义的 `heartbeat_period`，但是 distributor 的 ring 本身没有配置 `heartbeat_period`，则公共配置生效。 ring: kvstore: # Backend storage to use for the ring. Supported values are: consul, etcd, # inmemory, memberlist, multi. # CLI flag: -common.storage.ring.store [store: \u003cstring\u003e | default = \"consul\"] # The prefix for the keys in the store. Should end with a /. # CLI flag: -common.storage.ring.prefix [prefix: \u003cstring\u003e | default = \"collectors/\"] # Configuration for a Consul client. Only applies if the selected kvstore is # consul. # The CLI flags prefix for this block configuration is: common.storage.ring [consul: \u003cconsul\u003e] # Configuration for an ETCD v3 client. Only applies if the selected kvstore # is etcd. # The CLI flags prefix for this block configuration is: common.storage.ring [etcd: \u003cetcd\u003e] multi: # Primary backend storage used by multi-client. # CLI flag: -common.storage.ring.multi.primary [primary: \u003cstring\u003e | default = \"\"] # Secondary backend storage used by multi-client. # CLI flag: -common.storage.ring.multi.secondary [secondary: \u003cstring\u003e | default = \"\"] # Mirror writes to secondary store. # CLI flag: -common.storage.ring.multi.mirror-enabled [mirror_enabled: \u003cboolean\u003e | default = false] # Timeout for storing value to secondary store. # CLI flag: -common.storage.ring.multi.mirror-timeout [mirror_timeout: \u003cduration\u003e | default = 2s] # Period at which to heartbeat to the ring. 0 = disabled. # CLI flag: -common.storage.ring.heartbeat-period [heartbeat_period: \u003cduration\u003e | default = 15s] # The heartbeat timeout after which compactors are considered","date":"2023-06-27","objectID":"/posts/839f7c/:4:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"AWS S3 对象存储配置(aws_storage_config) dynamodb: # DynamoDB endpoint URL with escaped Key and Secret encoded. If only region is # specified as a host, proper endpoint will be deduced. Use # inmemory:///\u003ctable-name\u003e to use a mock in-memory implementation. # CLI flag: -dynamodb.url [dynamodb_url: \u003curl\u003e] # DynamoDB table management requests per second limit. # CLI flag: -dynamodb.api-limit [api_limit: \u003cfloat\u003e | default = 2] # DynamoDB rate cap to back off when throttled. # CLI flag: -dynamodb.throttle-limit [throttle_limit: \u003cfloat\u003e | default = 10] metrics: # Use metrics-based autoscaling, via this query URL # CLI flag: -metrics.url [url: \u003cstring\u003e | default = \"\"] # Queue length above which we will scale up capacity # CLI flag: -metrics.target-queue-length [target_queue_length: \u003cint\u003e | default = 100000] # Scale up capacity by this multiple # CLI flag: -metrics.scale-up-factor [scale_up_factor: \u003cfloat\u003e | default = 1.3] # Ignore throttling below this level (rate per second) # CLI flag: -metrics.ignore-throttle-below [ignore_throttle_below: \u003cfloat\u003e | default = 1] # query to fetch ingester queue length # CLI flag: -metrics.queue-length-query [queue_length_query: \u003cstring\u003e | default = \"sum(avg_over_time(cortex_ingester_flush_queue_length{job=\\\"cortex/ingester\\\"}[2m]))\"] # query to fetch throttle rates per table # CLI flag: -metrics.write-throttle-query [write_throttle_query: \u003cstring\u003e | default = \"sum(rate(cortex_dynamo_throttled_total{operation=\\\"DynamoDB.BatchWriteItem\\\"}[1m])) by (table) \u003e 0\"] # query to fetch write capacity usage per table # CLI flag: -metrics.usage-query [write_usage_query: \u003cstring\u003e | default = \"sum(rate(cortex_dynamo_consumed_capacity_total{operation=\\\"DynamoDB.BatchWriteItem\\\"}[15m])) by (table) \u003e 0\"] # query to fetch read capacity usage per table # CLI flag: -metrics.read-usage-query [read_usage_query: \u003cstring\u003e | default = \"sum(rate(cortex_dynamo_consumed_capacity_total{operation=\\\"DynamoDB.QueryPages\\\"}[1h])) by (table) \u003e 0\"] # query to fetch read errors per table # CLI flag: -metrics.read-error-query [read_error_query: \u003cstring\u003e | default = \"sum(increase(cortex_dynamo_failures_total{operation=\\\"DynamoDB.QueryPages\\\",error=\\\"ProvisionedThroughputExceededException\\\"}[1m])) by (table) \u003e 0\"] # Number of chunks to group together to parallelise fetches (zero to disable) # CLI flag: -dynamodb.chunk-gang-size [chunk_gang_size: \u003cint\u003e | default = 10] # Max number of chunk-get operations to start in parallel # CLI flag: -dynamodb.chunk.get-max-parallelism [chunk_get_max_parallelism: \u003cint\u003e | default = 32] backoff_config: # Minimum backoff time # CLI flag: -dynamodb.min-backoff [min_period: \u003cduration\u003e | default = 100ms] # Maximum backoff time # CLI flag: -dynamodb.max-backoff [max_period: \u003cduration\u003e | default = 50s] # Maximum number of times to retry an operation # CLI flag: -dynamodb.max-retries [max_retries: \u003cint\u003e | default = 20] # KMS key used for encrypting DynamoDB items. DynamoDB will use an Amazon # owned KMS key if not provided. # CLI flag: -dynamodb.kms-key-id [kms_key_id: \u003cstring\u003e | default = \"\"] # S3 endpoint URL with escaped Key and Secret encoded. If only region is # specified as a host, proper endpoint will be deduced. Use # inmemory:///\u003cbucket-name\u003e to use a mock in-memory implementation. # CLI flag: -s3.url [s3: \u003curl\u003e] # Set this to `true` to force the request to use path-style addressing. # CLI flag: -s3.force-path-style [s3forcepathstyle: \u003cboolean\u003e | default = false] # Comma separated list of bucket names to evenly distribute chunks over. # Overrides any buckets specified in s3.url flag # CLI flag: -s3.buckets [bucketnames: \u003cstring\u003e | default = \"\"] # S3 Endpoint to connect to. # CLI flag: -s3.endpoint [endpoint: \u003cstring\u003e | default = \"\"] # AWS region to use. # CLI flag: -s3.region [region: \u003cstring\u003e | default = \"\"] # AWS Access Key ID # CLI flag: -s3.access-key-id [access_key_id: \u003cstring\u003e | default = \"\"] # AWS Secret Access Key # CLI flag: -s3.secret-access-key [secret_access_key: \u003cstring\u003e | default = \"\"] # AWS Ses","date":"2023-06-27","objectID":"/posts/839f7c/:4:1","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"阿里云OSS存储配置 (alibabacloud_storage_config) # Name of OSS bucket. # CLI flag: -common.storage.oss.bucketname [bucket: \u003cstring\u003e | default = \"\"] # oss Endpoint to connect to. # CLI flag: -common.storage.oss.endpoint [endpoint: \u003cstring\u003e | default = \"\"] # alibabacloud Access Key ID # CLI flag: -common.storage.oss.access-key-id [access_key_id: \u003cstring\u003e | default = \"\"] # alibabacloud Secret Access Key # CLI flag: -common.storage.oss.secret-access-key [secret_access_key: \u003cstring\u003e | default = \"\"] ","date":"2023-06-27","objectID":"/posts/839f7c/:4:2","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"distributor ring: kvstore: # Backend storage to use for the ring. Supported values are: consul, etcd, # inmemory, memberlist, multi. # CLI flag: -distributor.ring.store [store: \u003cstring\u003e | default = \"consul\"] # The prefix for the keys in the store. Should end with a /. # CLI flag: -distributor.ring.prefix [prefix: \u003cstring\u003e | default = \"collectors/\"] # Configuration for a Consul client. Only applies if the selected kvstore is # consul. # The CLI flags prefix for this block configuration is: distributor.ring [consul: \u003cconsul\u003e] # Configuration for an ETCD v3 client. Only applies if the selected kvstore # is etcd. # The CLI flags prefix for this block configuration is: distributor.ring [etcd: \u003cetcd\u003e] multi: # Primary backend storage used by multi-client. # CLI flag: -distributor.ring.multi.primary [primary: \u003cstring\u003e | default = \"\"] # Secondary backend storage used by multi-client. # CLI flag: -distributor.ring.multi.secondary [secondary: \u003cstring\u003e | default = \"\"] # Mirror writes to secondary store. # CLI flag: -distributor.ring.multi.mirror-enabled [mirror_enabled: \u003cboolean\u003e | default = false] # Timeout for storing value to secondary store. # CLI flag: -distributor.ring.multi.mirror-timeout [mirror_timeout: \u003cduration\u003e | default = 2s] # Period at which to heartbeat to the ring. 0 = disabled. # CLI flag: -distributor.ring.heartbeat-period [heartbeat_period: \u003cduration\u003e | default = 5s] # The heartbeat timeout after which distributors are considered unhealthy # within the ring. 0 = never (timeout disabled). # CLI flag: -distributor.ring.heartbeat-timeout [heartbeat_timeout: \u003cduration\u003e | default = 1m] # Name of network interface to read address from. # CLI flag: -distributor.ring.instance-interface-names [instance_interface_names: \u003clist of strings\u003e | default = [\u003cprivate network interfaces\u003e]] rate_store: # The max number of concurrent requests to make to ingester stream apis # CLI flag: -distributor.rate-store.max-request-parallelism [max_request_parallelism: \u003cint\u003e | default = 200] # The interval on which distributors will update current stream rates from # ingesters # CLI flag: -distributor.rate-store.stream-rate-update-interval [stream_rate_update_interval: \u003cduration\u003e | default = 1s] # Timeout for communication between distributors and any given ingester when # updating rates # CLI flag: -distributor.rate-store.ingester-request-timeout [ingester_request_timeout: \u003cduration\u003e | default = 500ms] ","date":"2023-06-27","objectID":"/posts/839f7c/:5:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"ingester # Configures how the lifecycle of the ingester will operate and where it will # register for discovery. lifecycler: ring: kvstore: # Backend storage to use for the ring. Supported values are: consul, etcd, # inmemory, memberlist, multi. # CLI flag: -ring.store [store: \u003cstring\u003e | default = \"consul\"] # The prefix for the keys in the store. Should end with a /. # CLI flag: -ring.prefix [prefix: \u003cstring\u003e | default = \"collectors/\"] # Configuration for a Consul client. Only applies if the selected kvstore # is consul. [consul: \u003cconsul\u003e] # Configuration for an ETCD v3 client. Only applies if the selected # kvstore is etcd. [etcd: \u003cetcd\u003e] multi: # Primary backend storage used by multi-client. # CLI flag: -multi.primary [primary: \u003cstring\u003e | default = \"\"] # Secondary backend storage used by multi-client. # CLI flag: -multi.secondary [secondary: \u003cstring\u003e | default = \"\"] # Mirror writes to secondary store. # CLI flag: -multi.mirror-enabled [mirror_enabled: \u003cboolean\u003e | default = false] # Timeout for storing value to secondary store. # CLI flag: -multi.mirror-timeout [mirror_timeout: \u003cduration\u003e | default = 2s] # The heartbeat timeout after which ingesters are skipped for reads/writes. # 0 = never (timeout disabled). # CLI flag: -ring.heartbeat-timeout [heartbeat_timeout: \u003cduration\u003e | default = 1m] # The number of ingesters to write to and read from. # CLI flag: -distributor.replication-factor [replication_factor: \u003cint\u003e | default = 3] # True to enable the zone-awareness and replicate ingested samples across # different availability zones. # CLI flag: -distributor.zone-awareness-enabled [zone_awareness_enabled: \u003cboolean\u003e | default = false] # Comma-separated list of zones to exclude from the ring. Instances in # excluded zones will be filtered out from the ring. # CLI flag: -distributor.excluded-zones [excluded_zones: \u003cstring\u003e | default = \"\"] # Number of tokens for each ingester. # CLI flag: -ingester.num-tokens [num_tokens: \u003cint\u003e | default = 128] # Period at which to heartbeat to consul. 0 = disabled. # CLI flag: -ingester.heartbeat-period [heartbeat_period: \u003cduration\u003e | default = 5s] # Heartbeat timeout after which instance is assumed to be unhealthy. 0 = # disabled. # CLI flag: -ingester.heartbeat-timeout [heartbeat_timeout: \u003cduration\u003e | default = 1m] # Observe tokens after generating to resolve collisions. Useful when using # gossiping ring. # CLI flag: -ingester.observe-period [observe_period: \u003cduration\u003e | default = 0s] # Period to wait for a claim from another member; will join automatically # after this. # CLI flag: -ingester.join-after [join_after: \u003cduration\u003e | default = 0s] # Minimum duration to wait after the internal readiness checks have passed but # before succeeding the readiness endpoint. This is used to slowdown # deployment controllers (eg. Kubernetes) after an instance is ready and # before they proceed with a rolling update, to give the rest of the cluster # instances enough time to receive ring updates. # CLI flag: -ingester.min-ready-duration [min_ready_duration: \u003cduration\u003e | default = 15s] # Name of network interface to read address from. # CLI flag: -ingester.lifecycler.interface [interface_names: \u003clist of strings\u003e | default = [\u003cprivate network interfaces\u003e]] # Duration to sleep for before exiting, to ensure metrics are scraped. # CLI flag: -ingester.final-sleep [final_sleep: \u003cduration\u003e | default = 0s] # File path where tokens are stored. If empty, tokens are not stored at # shutdown and restored at startup. # CLI flag: -ingester.tokens-file-path [tokens_file_path: \u003cstring\u003e | default = \"\"] # The availability zone where this instance is running. # CLI flag: -ingester.availability-zone [availability_zone: \u003cstring\u003e | default = \"\"] # Unregister from the ring upon clean shutdown. It can be useful to disable # for rolling restarts with consistent naming in conjunction with # -distributor.extend-writes=false. # CLI flag: -ingester.unregister-on-shutdown [unregister_on_shutdown: \u003cboolean\u003e | default = true] # When enabled","date":"2023-06-27","objectID":"/posts/839f7c/:6:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"querier # Maximum duration for which the live tailing requests are served. # CLI flag: -querier.tail-max-duration [tail_max_duration: \u003cduration\u003e | default = 1h] # Time to wait before sending more than the minimum successful query requests. # CLI flag: -querier.extra-query-delay [extra_query_delay: \u003cduration\u003e | default = 0s] # Maximum lookback beyond which queries are not sent to ingester. 0 means all # queries are sent to ingester. # CLI flag: -querier.query-ingesters-within [query_ingesters_within: \u003cduration\u003e | default = 3h] engine: # Deprecated: Use querier.query-timeout instead. Timeout for query execution. # CLI flag: -querier.engine.timeout [timeout: \u003cduration\u003e | default = 5m] # The maximum amount of time to look back for log lines. Used only for instant # log queries. # CLI flag: -querier.engine.max-lookback-period [max_look_back_period: \u003cduration\u003e | default = 30s] # The maximum number of concurrent queries allowed. # CLI flag: -querier.max-concurrent [max_concurrent: \u003cint\u003e | default = 10] # Only query the store, and not attempt any ingesters. This is useful for # running a standalone querier pool operating only against stored data. # CLI flag: -querier.query-store-only [query_store_only: \u003cboolean\u003e | default = false] # When true, queriers only query the ingesters, and not stored data. This is # useful when the object store is unavailable. # CLI flag: -querier.query-ingester-only [query_ingester_only: \u003cboolean\u003e | default = false] # When true, allow queries to span multiple tenants. # CLI flag: -querier.multi-tenant-queries-enabled [multi_tenant_queries_enabled: \u003cboolean\u003e | default = false] # When true, querier limits sent via a header are enforced. # CLI flag: -querier.per-request-limits-enabled [per_request_limits_enabled: \u003cboolean\u003e | default = false] ","date":"2023-06-27","objectID":"/posts/839f7c/:7:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"ruler # URL of alerts return path. # CLI flag: -ruler.external.url [external_url: \u003curl\u003e] # Labels to add to all alerts. [external_labels: \u003clist of Labels\u003e] # The grpc_client block configures the gRPC client used to communicate between # two Loki components. # The CLI flags prefix for this block configuration is: ruler.client [ruler_client: \u003cgrpc_client\u003e] # How frequently to evaluate rules. # CLI flag: -ruler.evaluation-interval [evaluation_interval: \u003cduration\u003e | default = 1m] # How frequently to poll for rule changes. # CLI flag: -ruler.poll-interval [poll_interval: \u003cduration\u003e | default = 1m] # Deprecated: Use -ruler-storage. CLI flags and their respective YAML config # options instead. storage: # Method to use for backend rule storage (configdb, azure, gcs, s3, swift, # local, bos) # CLI flag: -ruler.storage.type [type: \u003cstring\u003e | default = \"\"] # Configures backend rule storage for Azure. # The CLI flags prefix for this block configuration is: ruler.storage [azure: \u003cazure_storage_config\u003e] # Configures backend rule storage for AlibabaCloud Object Storage (OSS). # The CLI flags prefix for this block configuration is: ruler [alibabacloud: \u003calibabacloud_storage_config\u003e] # Configures backend rule storage for GCS. # The CLI flags prefix for this block configuration is: ruler.storage [gcs: \u003cgcs_storage_config\u003e] # Configures backend rule storage for S3. # The CLI flags prefix for this block configuration is: ruler [s3: \u003cs3_storage_config\u003e] # Configures backend rule storage for Baidu Object Storage (BOS). # The CLI flags prefix for this block configuration is: ruler.storage [bos: \u003cbos_storage_config\u003e] # Configures backend rule storage for Swift. # The CLI flags prefix for this block configuration is: ruler.storage [swift: \u003cswift_storage_config\u003e] # Configures backend rule storage for a local file system directory. local: # Directory to scan for rules # CLI flag: -ruler.storage.local.directory [directory: \u003cstring\u003e | default = \"\"] # File path to store temporary rule files. # CLI flag: -ruler.rule-path [rule_path: \u003cstring\u003e | default = \"/rules\"] # Comma-separated list of Alertmanager URLs to send notifications to. Each # Alertmanager URL is treated as a separate group in the configuration. Multiple # Alertmanagers in HA per group can be supported by using DNS resolution via # '-ruler.alertmanager-discovery'. # CLI flag: -ruler.alertmanager-url [alertmanager_url: \u003cstring\u003e | default = \"\"] # Use DNS SRV records to discover Alertmanager hosts. # CLI flag: -ruler.alertmanager-discovery [enable_alertmanager_discovery: \u003cboolean\u003e | default = false] # How long to wait between refreshing DNS resolutions of Alertmanager hosts. # CLI flag: -ruler.alertmanager-refresh-interval [alertmanager_refresh_interval: \u003cduration\u003e | default = 1m] # If enabled requests to Alertmanager will utilize the V2 API. # CLI flag: -ruler.alertmanager-use-v2 [enable_alertmanager_v2: \u003cboolean\u003e | default = false] # List of alert relabel configs. [alert_relabel_configs: \u003crelabel_config...\u003e] # Capacity of the queue for notifications to be sent to the Alertmanager. # CLI flag: -ruler.notification-queue-capacity [notification_queue_capacity: \u003cint\u003e | default = 10000] # HTTP timeout duration when sending notifications to the Alertmanager. # CLI flag: -ruler.notification-timeout [notification_timeout: \u003cduration\u003e | default = 10s] alertmanager_client: # Path to the client certificate file, which will be used for authenticating # with the server. Also requires the key path to be configured. # CLI flag: -ruler.alertmanager-client.tls-cert-path [tls_cert_path: \u003cstring\u003e | default = \"\"] # Path to the key file for the client certificate. Also requires the client # certificate to be configured. # CLI flag: -ruler.alertmanager-client.tls-key-path [tls_key_path: \u003cstring\u003e | default = \"\"] # Path to the CA certificates file to validate server certificate against. If # not set, the host's root CA certificates are used. # CLI flag: -ruler.alertmanager-client.tls-ca-path [tls_ca_path: \u003cstring\u003e | defau","date":"2023-06-27","objectID":"/posts/839f7c/:8:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"schema_config 主要定义的是 Loki 数据存储的策略，可以在指定时间将数据写入到新的存储，从而避免数据迁移问题。 # 用于 chunk 索引 schemas 配置 configs: - [\u003cperiod_config\u003e] ","date":"2023-06-27","objectID":"/posts/839f7c/:9:0","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["监控体系"],"content":"period_config # The date of the first day that index buckets should be created. Use a date in # the past if this is your only period_config, otherwise use a date when you # want the schema to switch over. In YYYY-MM-DD format, for example: 2018-04-15. [from: \u003cdaytime\u003e] # store and object_store below affect which \u003cstorage_config\u003e key is used. # Which store to use for the index. Either aws, aws-dynamo, gcp, bigtable, # bigtable-hashed, cassandra, boltdb or boltdb-shipper. [store: \u003cstring\u003e | default = \"\"] # Which store to use for the chunks. Either aws, azure, gcp, bigtable, gcs, # cassandra, swift, filesystem or a named_store (refer to named_stores_config). # If omitted, defaults to the same value as store. [object_store: \u003cstring\u003e | default = \"\"] # The schema version to use, current recommended schema is v11. [schema: \u003cstring\u003e | default = \"\"] # Configures how the index is updated and stored. index: # Table prefix for all period tables. [prefix: \u003cstring\u003e | default = \"\"] # Table period. [period: \u003cduration\u003e] # A map to be added to all managed tables. [tags: \u003cmap of string to string\u003e] # Configured how the chunks are updated and stored. chunks: # Table prefix for all period tables. [prefix: \u003cstring\u003e | default = \"\"] # Table period. [period: \u003cduration\u003e] # A map to be added to all managed tables. [tags: \u003cmap of string to string\u003e] # How many shards will be created. Only used if schema is v10 or greater. [row_shards: \u003cint\u003e] Loki 对于数据存储的目标是向后兼容，通过修改 Schema 配置允许以增量方式升级到新的存储模式。首先，我们需要在schema_config 中创建一个新的 configs 条目，要记住的是新加的存储模式起始时间必须是将来的某个时间点，这样 Table Manager 就可以在之前创建所需的表，并确保不会查询现有数据。否则在查询时会因丢失旧的日志索引造成无法检索。例如我们要把2023年7月28日之后的 Loki 日志切换到 S3 上，那么按照如下配置后，重启服务。 schema_config: configs: - from: \"2022-01-11\" index: period: 24h prefix: loki_index_ object_store: s3 schema: v12 store: boltdb-shipper - from: 2023-07-28 object_store: s3 schema: v13 index: prefix: index_ period: 720h chunks: prefix: chunks_ period: 720h 上面的配置意思就是对于2023年07月28日之前保存的所有数据，Loki 使用 v12 的 schema，到点之后就采用 v13 的 schema 策略来存储日志。 Loki2.0版本之后，对于使用boltdb存储索引部分做了较大的重构，采用新的boltdb-shipper模式，可以让Loki的索引存储在S3上，而彻底摆脱Cassandra或者谷歌的BigTable。此后服务的横向扩展将变得更加容易。关于bolt-shipper的更多细节，可以参考：https://grafana.com/docs/loki/latest/operations/storage/boltdb-shipper/ ","date":"2023-06-27","objectID":"/posts/839f7c/:9:1","tags":["loki"],"title":"Loki配置解析","uri":"/posts/839f7c/"},{"categories":["hpa"],"content":"KEDA 在2020年11月4号release了2.0版本，包含了一些新的比较有用的特性，比如ScaledObject/ScaledJob中支持多触发器、支持HPA原始的CPU、Memory scaler等。 具体的安装使用请参考上一篇文章使用keda完成基于事件的弹性伸缩，这篇文章主要深入的看下KEDA内部机制以及是如何工作的。 我们先提出几个问题，带着问题去看代码，方便我们理解整个机制： KEDA是如何获取到多种事件的指标，以及如何判断扩缩容的？ KEDA是如何做到将应用的副本数缩容0，依据是什么？ ","date":"2023-06-25","objectID":"/posts/ea3024/:0:0","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"代码结构 对一些主要目录说明，其他一些MD文件主要是文字说明： ├── BRANDING.md ├── BUILD.md //如何在本地编译和运行 ├── CHANGELOG.md ├── CONTRIBUTING.md //如何参与贡献次项目 ├── CREATE-NEW-SCALER.md ├── Dockerfile ├── Dockerfile.adapter ├── GOVERNANCE.md ├── LICENSE ├── MAINTAINERS.md ├── Makefile // 构建编译相关命令 ├── PROJECT ├── README.md ├── RELEASE-PROCESS.MD ├── adapter // keda-metrics-apiserver 组件入口 ├── api // 自定义资源定义，例如ScaledObject的定义 ├── bin ├── config //组件yaml资源，通过kustomization工具生成 ├── controllers //kubebuilder 中controller 代码控制crd资源 ├── go.mod ├── go.sum ├── hack ├── images ├── main.go //keda-operator controller入口 ├── pkg //包含组件核心代码实现 ├── tests //e2e测试 ├── tools ├── vendor └── version keda中主要是两个组件keda-operator以及keda-metrics-apiserver。 keda-operator ： 负责创建/更新HPA以及通过Loop控制应用副本数 keda-metrics-apiserver：实现external-metrics接口，以对接给HPA的external类型的指标查询（比如各种prometheus指标，mysql等） ","date":"2023-06-25","objectID":"/posts/ea3024/:1:0","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"keda-operator 项目中用到了kubebuilder SDK，用来完成这个Operator的编写。 对于k8s中的自定义controller不了解的可以看看这边文章：如何在Kubernetes中创建一个自定义Controller?。 keda controller的主要流程，画了幅图： 组件启动入口在于main.go文件中： 通过controller-runtime组件启动两个自定义controller：ScaledObjectReconciler,ScaledJobReconciler: mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, MetricsBindAddress: metricsAddr, HealthProbeBindAddress: \":8081\", Port: 9443, LeaderElection: enableLeaderElection, LeaderElectionID: \"operator.keda.sh\", }) ... // Add readiness probe err = mgr.AddReadyzCheck(\"ready-ping\", healthz.Ping) ... // Add liveness probe err = mgr.AddHealthzCheck(\"health-ping\", healthz.Ping) .... //注册 ScaledObject 处理的controller if err = (\u0026controllers.ScaledObjectReconciler{ Client: mgr.GetClient(), Log: ctrl.Log.WithName(\"controllers\").WithName(\"ScaledObject\"), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \"unable to create controller\", \"controller\", \"ScaledObject\") os.Exit(1) } ////注册 ScaledJob 处理的controller if err = (\u0026controllers.ScaledJobReconciler{ Client: mgr.GetClient(), Log: ctrl.Log.WithName(\"controllers\").WithName(\"ScaledJob\"), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \"unable to create controller\", \"controller\", \"ScaledJob\") os.Exit(1) } if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \"problem running manager\") os.Exit(1) } ","date":"2023-06-25","objectID":"/posts/ea3024/:2:0","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"ScaledObjectReconciler 处理 我们主要关注Reconcile方法，当ScaledObject发生变化时将会触发该方法： 方法内部主要功能实现： ... // 处理删除ScaledObject的情况 if scaledObject.GetDeletionTimestamp() != nil { //进入垃圾回收（比如停止goroutine中Loop，恢复原有副本数） return ctrl.Result{}, r.finalizeScaledObject(reqLogger, scaledObject) } // 给ScaledObject资源加上Finalizer：finalizer.keda.sh if err := r.ensureFinalizer(reqLogger, scaledObject); err != nil { return ctrl.Result{}, err } ... // 真正处理ScaledObject资源 msg, err := r.reconcileScaledObject(reqLogger, scaledObject) // 设置Status字段说明 conditions := scaledObject.Status.Conditions.DeepCopy() if err != nil { reqLogger.Error(err, msg) conditions.SetReadyCondition(metav1.ConditionFalse, \"ScaledObjectCheckFailed\", msg) conditions.SetActiveCondition(metav1.ConditionUnknown, \"UnkownState\", \"ScaledObject check failed\") } else { reqLogger.V(1).Info(msg) conditions.SetReadyCondition(metav1.ConditionTrue, \"ScaledObjectReady\", msg) } kedacontrollerutil.SetStatusConditions(r.Client, reqLogger, scaledObject, \u0026conditions) return ctrl.Result{}, err r.reconcileScaledObject方法： 这个方法中主要两个动作： ensureHPAForScaledObjectExists创建HPA资源 进入requestScaleLoop（不断的检测scaler 是否active，进行副本数的修改） ensureHPAForScaledObjectExists 通过跟踪进入到newHPAForScaledObject方法: scaledObjectMetricSpecs, err := r.getScaledObjectMetricSpecs(logger, scaledObject) ...省略代码 hpa := \u0026autoscalingv2beta2.HorizontalPodAutoscaler{ Spec: autoscalingv2beta2.HorizontalPodAutoscalerSpec{ MinReplicas: getHPAMinReplicas(scaledObject), MaxReplicas: getHPAMaxReplicas(scaledObject), Metrics: scaledObjectMetricSpecs, Behavior: behavior, ScaleTargetRef: autoscalingv2beta2.CrossVersionObjectReference{ Name: scaledObject.Spec.ScaleTargetRef.Name, Kind: gvkr.Kind, APIVersion: gvkr.GroupVersion().String(), }}, ObjectMeta: metav1.ObjectMeta{ Name: getHPAName(scaledObject), Namespace: scaledObject.Namespace, Labels: labels, }, TypeMeta: metav1.TypeMeta{ APIVersion: \"v2beta2\", }, } 可以看到创建ScalerObject其实最终也是创建了HPA，其实还是通过HPA本身的特性来控制应用的弹性伸缩。 其中getScaledObjectMetricSpecs方法中就是获取到triggers中的metrics指标。 这里有区分一下External的metrics和resource metrics，因为CPU/Memory scaler是通过resource metrics 来获取的。 requestScaleLoop requestScaleLoop方法中用来循环check Scaler中的IsActive状态并作出对应的处理，比如修改副本数，直接来看最终的处理吧： 这里有两种模型来触发RequestScale： Pull模型：即主动的调用scaler 中的IsActive方法 Push模型：由Scaler来触发，PushScaler多了一个Run方法，通过channel传入active状态。 IsActive是由Scaler实现的，比如对于prometheus来说，可能指标为0则为false 这个具体的scaler实现后续再讲，我们来看看RequestScale做了什么事： //当前副本数为0，并是所有scaler属于active状态，则修改副本数为MinReplicaCount 或 1 if currentScale.Spec.Replicas == 0 \u0026\u0026 isActive { e.scaleFromZero(ctx, logger, scaledObject, currentScale) } else if !isActive \u0026\u0026 currentScale.Spec.Replicas \u003e 0 \u0026\u0026 (scaledObject.Spec.MinReplicaCount == nil || *scaledObject.Spec.MinReplicaCount == 0) { // 所有scaler都处理not active状态，并且当前副本数大于0，且MinReplicaCount设定为0 // 则缩容副本数为0 e.scaleToZero(ctx, logger, scaledObject, currentScale) } else if !isActive \u0026\u0026 scaledObject.Spec.MinReplicaCount != nil \u0026\u0026 currentScale.Spec.Replicas \u003c *scaledObject.Spec.MinReplicaCount { // 所有scaler都处理not active状态，并且当前副本数小于MinReplicaCount，则修改为MinReplicaCount currentScale.Spec.Replicas = *scaledObject.Spec.MinReplicaCount err := e.updateScaleOnScaleTarget(ctx, scaledObject, currentScale) .... } else if isActive { // 处理active状态，并且副本数大于0，则更新LastActiveTime e.updateLastActiveTime(ctx, logger, scaledObject) } else { // 不处理 logger.V(1).Info(\"ScaleTarget no change\") } ","date":"2023-06-25","objectID":"/posts/ea3024/:2:1","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"ScaledJobReconciler 处理 ScaledJobReconciler相比ScalerObject少了创建HPA的步骤，其余的步骤主要是通过checkScaledJobScalers，RequestJobScale两个方法来判断Job创建： checkScaledJobScalers 方法，用于计算isActive，maxValue的值 RequestJobScale 方法，用于负责创建Job，里面还涉及到三种扩容策略 这里直接看代码吧，不贴代码了。 如何停止Loop 这里有个问题就是startPushScalers和startScaleLoop都是在Goroutine中处理的，所以当ScaleObject/ScalerJob被删除的时候，这里需要能够被删除，这里就用到了context.Cancel方法，在Goroutine启动的时候就将，context保存在scaleLoopContexts *sync.Map中(如果已经有了，就先Cancel一次)，在删除资源的时候，进行删除: func (h *scaleHandler) DeleteScalableObject(scalableObject interface{}) error { withTriggers, err := asDuckWithTriggers(scalableObject) if err != nil { h.logger.Error(err, \"error duck typing object into withTrigger\") return err } key := generateKey(withTriggers) result, ok := h.scaleLoopContexts.Load(key) if ok { cancel, ok := result.(context.CancelFunc) if ok { cancel() } h.scaleLoopContexts.Delete(key) } else { h.logger.V(1).Info(\"ScaleObject was not found in controller cache\", \"key\", key) } return nil } ps: 这里的妙啊，学到了 ","date":"2023-06-25","objectID":"/posts/ea3024/:2:2","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"keda-metrics-apiserver keda-metrics-apiserver实现了ExternalMetricsProvider接口： type ExternalMetricsProvider interface { GetExternalMetric(namespace string, metricSelector labels.Selector, info ExternalMetricInfo) (*external_metrics.ExternalMetricValueList, error) ListAllExternalMetrics() []ExternalMetricInfo } GetExternalMetric 用于返回Scaler的指标，调用scaler.GetMetrics方法 ListAllExternalMetrics 返回所有支持的external metrics，例如prometheus，mysql等 当代码写好之后，再通过apiservice注册到apiservier上(当然还涉及到鉴权，这里不啰嗦了)： apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: labels: app.kubernetes.io/name: v1beta1.external.metrics.k8s.io app.kubernetes.io/version: latest app.kubernetes.io/part-of: keda-operator name: v1beta1.external.metrics.k8s.io spec: service: name: keda-metrics-apiserver namespace: keda group: external.metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 ","date":"2023-06-25","objectID":"/posts/ea3024/:3:0","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"实现一个Scaler 其实有两种Scaler，即上面将的一个pull，一个push的模型，PushScaler多了一个Run方法： 实现一个Scaler，主要实现以下接口： // Scaler interface type Scaler interface { // 返回external_metrics.ExternalMetricValue对象，其实就是用于 keda-metrics-apiserver中获取到scaler的指标 GetMetrics(ctx context.Context, metricName string, metricSelector labels.Selector) ([]external_metrics.ExternalMetricValue, error) // 返回v2beta2.MetricSpec 结构，主要用于ScalerObject描述创建HPA的类型和Target指标等 GetMetricSpecForScaling() []v2beta2.MetricSpec // 返回该Scaler是否Active，可能会影响Loop中直接修改副本数 IsActive(ctx context.Context) (bool, error) //调用完一次上面的方法就会调用一次Close Close() error } // PushScaler interface type PushScaler interface { Scaler // 通过scaler实现Run方法，往active channel中，写入值，而非上面的直接调用IsActive放回 Run(ctx context.Context, active chan\u003c- bool) } ","date":"2023-06-25","objectID":"/posts/ea3024/:4:0","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["hpa"],"content":"总结 回过头来我们解答下在开头留下的问题： KEDA是如何获取到多种事件的指标，以及如何判断扩缩容的？ 答：keda controler中生成了external 类型的hpa，并且实现了external metrics 的api KEDA是如何做到将应用的副本数缩容0，依据是什么？ 答： keda 内部有个loop，不断的check isActive状态，会主动的修改应用副本 ","date":"2023-06-25","objectID":"/posts/ea3024/:5:0","tags":["kedan"],"title":"源码剖析：KEDA是如何工作的?","uri":"/posts/ea3024/"},{"categories":["operator"],"content":" Shell Operator 是个冷僻又有点用的东西。方便运维监听kubernetes事件，并基于这些事件做一些简单任务处理；并且shell语言基本上大部分运维人员都懂，而不需要太高的学习成本。 ","date":"2023-06-19","objectID":"/posts/12b628/:0:0","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"运行原理 shell-operator 部署在 Pod 中。在 Pod 中有一个 /hooks 的一个子目录，其中存储了可执行文件，它们可以用 Bash、Python、Ruby等编写的，我们称这些可执行文件为hooks。Shell-opeator 订阅 Kubernetes 事件并执行这些钩子来响应我们感兴趣的事件。 ","date":"2023-06-19","objectID":"/posts/12b628/:1:0","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"监听 Pod 新增 ","date":"2023-06-19","objectID":"/posts/12b628/:2:0","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"配置脚本 mkdir -p shell-operator-demo/hooks vim shell-operator-demo/hooks/pod-hooks.sh #!/usr/bin/env bash if [[ $1 == \"--config\" ]] ; then cat \u003c\u003cEOF configVersion: v1 kubernetes: - apiVersion: v1 kind: Pod executeHookOnEvent: - Added EOF else type=$(jq -r '.[0].type' $BINDING_CONTEXT_PATH) if [[ $type == \"Event\" ]] ; then podName=$(jq -r '.[0].object.metadata.name' $BINDING_CONTEXT_PATH) echo \"Pod '${podName}' added\" fi fi ","date":"2023-06-19","objectID":"/posts/12b628/:2:1","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"创建 Dockerfile FROM ghcr.io/flant/shell-operator:latest ADD hooks /hooks ","date":"2023-06-19","objectID":"/posts/12b628/:2:2","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"构建镜像 docker build -t registry.cn-hangzhou.aliyuncs.com/seam/shell-operator:monitor-pods . docker push registry.cn-hangzhou.aliyuncs.com/seam/shell-operator:monitor-pods ","date":"2023-06-19","objectID":"/posts/12b628/:2:3","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"配置权限 (RBAC) --- apiVersion: v1 kind: ServiceAccount metadata: name: monitor-pods-acc --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: monitor-pods rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: monitor-pods roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: monitor-pods subjects: - kind: ServiceAccount name: monitor-pods-acc namespace: example-monitor-pods ","date":"2023-06-19","objectID":"/posts/12b628/:2:4","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"运行 Pod --- apiVersion: v1 kind: Pod metadata: name: shell-operator spec: containers: - name: shell-operator image: registry.cn-hangzhou.aliyuncs.com/seam/shell-operator:monitor-pods imagePullPolicy: Always serviceAccountName: monitor-pods-acc ","date":"2023-06-19","objectID":"/posts/12b628/:2:5","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"扩容 kbernetes-dashboard kubectl -n kube-system scale --replicas=1 deploy/kubernetes-dashboard ","date":"2023-06-19","objectID":"/posts/12b628/:2:6","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"查看 shell-operator 的日志 ","date":"2023-06-19","objectID":"/posts/12b628/:2:7","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"监听自定义 CRD 自定义 users.loki.alongparty.cn 资源，包含 name 和 password 两个字段，监听 User 的事件变更生成 htpasswd,并生成 secret 配置 ","date":"2023-06-19","objectID":"/posts/12b628/:3:0","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"自定义资源(CRD) apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u003cplural\u003e.\u003cgroup\u003e name: users.loki.alongparty.cn spec: # group name to use for REST API: /apis/\u003cgroup\u003e/\u003cversion\u003e group: loki.alongparty.cn # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: name: type: string password: type: string # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cplural\u003e plural: users # singular name to be used as an alias on the CLI and for display singular: user # kind is normally the CamelCased singular type. Your resource manifests use this. kind: User # shortNames allow shorter string to match your resource on the CLI shortNames: - user ","date":"2023-06-19","objectID":"/posts/12b628/:3:1","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"监听 CRD #!/usr/bin/env bash if [[ $1 == \"--config\" ]] ; then cat \u003c\u003cEOF { \"configVersion\": \"v1\", \"kubernetes\": [ { \"name\":\"OnCreateDelete\", \"apiVersion\": \"loki.alongparty.cn/v1\", \"kind\": \"User\", \"executeHookOnEvent\": [ \"Added\", \"Deleted\" ] }, { \"name\":\"OnModified\", \"apiVersion\": \"loki.alongparty.cn/v1\", \"kind\": \"User\", \"executeHookOnEvent\": [ \"Modified\" ] } ] } EOF else bindingName=$(jq -r '.[0].binding' $BINDING_CONTEXT_PATH) if [[ $bindingName == \"onStartup\" ]] ; then echo \"namespace-hook is triggered on startup.\" exit 0 fi type=$(jq -r '.[0].type' ${BINDING_CONTEXT_PATH}) if [[ $type == \"Synchronization\" ]] ; then : handle existing objects : jq '.[0].objects | ... ' exit 0 fi resourceEvent=$(jq -r '.[0].watchEvent' $BINDING_CONTEXT_PATH) resourceName=$(jq -r '.[0].object.metadata.name' $BINDING_CONTEXT_PATH) kind=$(jq -r '.[0].object.kind' ${BINDING_CONTEXT_PATH}) name=$(jq -r '.[0].object.spec.name' $BINDING_CONTEXT_PATH) password=$(jq -r '.[0].object.spec.password' $BINDING_CONTEXT_PATH) kubectl get secret loki-gateway IsSecret=$? if [[ \"${IsSecret}\" != 0 ]] ; then echo ${IsSecret} kubectl create secret generic loki-gateway fi kubectl get secret loki-gateway -o=jsonpath=\"{.data.\\.htpasswd}\" | base64 -d \u003e /tmp/htpasswd if [[ $bindingName == \"OnModified\" ]] ; then echo \"${kind}/${resourceName} object were modified\" echo \"${name} ${password}\" htpasswd -D /tmp/htpasswd \"${name}\" \u003e\u003e/dev/null htpasswd -b /tmp/htpasswd \"${name}\" \"${password}\" else if [[ $resourceEvent == \"Added\" ]] ; then echo \"${kind}/${resourceName} object were created\" echo \"${name} ${password}\" htpasswd -b /tmp/htpasswd \"${name}\" \"${password}\" else echo \"${kind}/${resourceName} object were deleted\" echo \"${name} ${password}\" htpasswd -D /tmp/htpasswd \"${name}\" fi fi kubectl patch secret loki-gateway --patch=\"{\\\"data\\\": { \\\".htpasswd\\\": \\\"$(cat /tmp/htpasswd | base64 -i -)\\\"}}\" echo \"secret patched successfully\" fi ","date":"2023-06-19","objectID":"/posts/12b628/:3:2","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"构建镜像 FROM ghcr.io/flant/shell-operator:latest ADD hooks /hooks RUN apk add apache2-utils docker build -t registry.cn-hangzhou.aliyuncs.com/seam/shell-operator:crds . docker push registry.cn-hangzhou.aliyuncs.com/seam/shell-operator:crds ","date":"2023-06-19","objectID":"/posts/12b628/:3:3","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"配置权限 (RBAC) --- apiVersion: v1 kind: ServiceAccount metadata: name: crd-simple-acc --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: crd-simple rules: - apiGroups: [\"loki.alongparty.cn\"] resources: [\"users\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"*\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: crd-simple roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: crd-simple subjects: - kind: ServiceAccount name: crd-simple-acc namespace: default ","date":"2023-06-19","objectID":"/posts/12b628/:3:4","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"运行 Operator --- apiVersion: v1 kind: Pod metadata: name: shell-operator spec: containers: - name: shell-operator image: registry.cn-hangzhou.aliyuncs.com/seam/shell-operator:crds imagePullPolicy: Always serviceAccountName: crd-simple-acc ","date":"2023-06-19","objectID":"/posts/12b628/:3:5","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"配置资源对象 apiVersion: \"loki.alongparty.cn/v1\" kind: User metadata: name: user01 labels: name: user01 spec: name: \"user01\" password: \"password01\" ","date":"2023-06-19","objectID":"/posts/12b628/:3:6","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"查看 operator 日志 ... ... ... INFO[2023-06-20T12:06:38+08:00] queue task HookRun:main:kubernetes:crd.sh:OnModified binding=kubernetes event.id=eaf0bf72-b246-4418-b9ff-76652c8355f0 queue=main INFO[2023-06-20T12:06:38+08:00] Execute hook binding=OnModified event=kubernetes hook=crd.sh queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] NAME TYPE DATA AGE binding=OnModified event=kubernetes hook=crd.sh output=stdout queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] loki-gateway Opaque 1 8m51s binding=OnModified event=kubernetes hook=crd.sh output=stdout queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] User/user02 object were modified binding=OnModified event=kubernetes hook=crd.sh output=stdout queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] user05 password002 binding=OnModified event=kubernetes hook=crd.sh output=stdout queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] User user05 not found binding=OnModified event=kubernetes hook=crd.sh output=stderr queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] Adding password for user user05 binding=OnModified event=kubernetes hook=crd.sh output=stderr queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] secret/loki-gateway patched binding=OnModified event=kubernetes hook=crd.sh output=stdout queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] secret patched successfully binding=OnModified event=kubernetes hook=crd.sh output=stdout queue=main task=HookRun INFO[2023-06-20T12:06:39+08:00] Hook executed successfully binding=OnModified event=kubernetes hook=crd.sh queue=main task=HookRun ","date":"2023-06-19","objectID":"/posts/12b628/:3:7","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"查看secret kubectl get secret loki-gateway -o=jsonpath=\"{.data.\\.htpasswd}\" | base64 -d user02:$apr1$j/FmH28w$DfBvoRMyVoqURgSc24W8.. user01:$apr1$LNyYXi/1$RFBEW/Rko6fbYVhuwGxwF/ user03:$apr1$F7r6YH7R$dvW5yQWAliuPWrlunaFRu/ user04:$apr1$Up7gY4fi$MqgqclnAdITB0AABJMg6q0 user05:$apr1$FW9XnAcP$M/at.hOvIiRDjT60Luu040 ","date":"2023-06-19","objectID":"/posts/12b628/:3:8","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["operator"],"content":"参考资料 使用shell-operator实现Operator shell-operator文档 ","date":"2023-06-19","objectID":"/posts/12b628/:4:0","tags":["shell-operator"],"title":"使用Shell实现Operaror","uri":"/posts/12b628/"},{"categories":["监控体系"],"content":"轻量日志采集Loki快速入门 ","date":"2023-06-15","objectID":"/posts/81200000/:0:0","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"安装 Loki ","date":"2023-06-15","objectID":"/posts/81200000/:1:0","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"添加 Helm 仓库 helm repo add grafana https://grafana.github.io/helm-charts helm repo update ","date":"2023-06-15","objectID":"/posts/81200000/:1:1","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"loki-values.yaml global: # -- configures cluster domain (\"cluster.local\" by default) clusterDomain: \"alongparty.cn\" dnsService: \"kube-dns-coredns\" loki: tenants: - name: mongo password: mongo - name: mysql password: mysql storage: bucketNames: chunks: loki ruler: loki type: s3 s3: region: ap-southeast-1 secretAccessKey: AKIA2C3alongparty.cn accessKeyId: jmg9yxa5sR+UVTJKd+6LVTLES75alongparty.cn server: grpc_server_max_recv_msg_size: 8388608 limits_config: ingestion_rate_mb: 8 ingestion_burst_size_mb: 16 test: enabled: false monitoring: lokiCanary: enabled: false dashboards: enabled: false rules: enabled: false serviceMonitor: enabled: false selfMonitoring: enabled: false write: autoscaling: enabled: false persistence: storageClass: local-path enableStatefulSetAutoDeletePVC: false read: autoscaling: enabled: false persistence: storageClass: local-path enableStatefulSetAutoDeletePVC: false backend: autoscaling: enabled: false persistence: storageClass: local-path enableStatefulSetAutoDeletePVC: false gateway: base_auth: enabled: true # username: mongo # password: mongo minio: enabled: false clusterDomain: alongparty.cn image: repository: registry.cn-hangzhou.aliyuncs.com/seam/minio mcImage: repository: registry.cn-hangzhou.aliyuncs.com/seam/minio-mc persistence: size: 50Gi stroageClass: local-path ","date":"2023-06-15","objectID":"/posts/81200000/:1:2","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"查看版本 # helm search repo grafana/loki -l | head -5 NAME CHART VERSION APP VERSION DESCRIPTION grafana/loki 5.6.4 2.8.2 Helm chart for Grafana Loki in simple, scalable... grafana/loki 5.6.3 2.8.2 Helm chart for Grafana Loki in simple, scalable... grafana/loki 5.6.2 2.8.2 Helm chart for Grafana Loki in simple, scalable... grafana/loki 5.6.1 2.8.2 Helm chart for Grafana Loki in simple, scalable... ","date":"2023-06-15","objectID":"/posts/81200000/:1:3","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"部署 loki helm install loki grafana/loki -n loki --version 5.6.4 --create-namespace -f loki-values.yaml ","date":"2023-06-15","objectID":"/posts/81200000/:1:4","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"更新 helm upgrade --install loki grafana/loki -n loki --version 5.6.4 --create-namespace -f loki-values.yaml ","date":"2023-06-15","objectID":"/posts/81200000/:1:5","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"安装 Promtail 日志收集器 ","date":"2023-06-15","objectID":"/posts/81200000/:2:0","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"promtail-values.yaml config: clients: - basic_auth: password: mongo username: mongo url: http://loki-gateway/loki/api/v1/push snippets: extraScrapeConfigs: | # 通过 kubernetes_sd_configs:pod 配置 pod 日志，参考 https://grafana.com/docs/loki/latest/clients/promtail/configuration/#kubernetes_sd_config - job_name: kubernetes-pods-app pipeline_stages: {{- toYaml .Values.config.snippets.podPipelineStages | nindent 4 }} kubernetes_sd_configs: - role: pod relabel_configs: # 存在 app.kubernetes.io/name: \"percona-server-mongodb\" 标签的容器日志采集 # app.kubernetes.io/name: \"percona-server-mongodb\" - action: keep regex: percona-server-mongodb source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name] # 把 pod 所有的标签暴露出来 - action: labelmap regex: __meta_kubernetes_pod_label_(.+) replacement: $1 target_label: $1 - action: replace source_labels: - __meta_kubernetes_pod_ip target_label: pod_ip - action: replace source_labels: - __meta_kubernetes_pod_label_app target_label: app - action: replace source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_component target_label: component - action: replace source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_replset target_label: replset - action: replace source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_instance target_label: app - action: replace source_labels: - __meta_kubernetes_pod_label_topology_kubernetes_io_zone target_label: zone - action: replace source_labels: - __meta_kubernetes_pod_node_name target_label: node_name # 删除标签 - action: labeldrop regex: (app|version|topology|statefulset|controller)_(.+) {{- if .Values.config.snippets.addScrapeJobLabel }} - action: replace replacement: kubernetes-pods-app target_label: scrape_job {{- end }} {{- toYaml .Values.config.snippets.common | nindent 4 }} podPipelineStages: # Container 是 mongos或者mongod的日志,且包含 `Slow query` 的日志采集 - match: selector: '{container=~\"mongos|mongod\"} |= \"Slow query\"' action: keep stages: - docker: {} # Container 是 mongos或者mongod的日志,但不包含 `Slow query` 的日志丢弃 - match: selector: '{container=~\"mongos|mongod\"} != \"Slow query\"' action: drop drop_counter_reason: promtail_not_slow_query # Container 不是 mongos或者mongod的日志丢弃 - match: selector: '{container!~\"mongos|mongod\"}' action: drop drop_counter_reason: promtail_not_other_pod - drop: older_than: 24h drop_counter_reason: \"line_too_old\" scrapeConfigs: \"\" defaultVolumeMounts: - mountPath: /run/promtail name: run - mountPath: /data/k8s/k8s_docker/data/containers name: containers readOnly: true - mountPath: /var/log/pods name: pods readOnly: true defaultVolumes: - hostPath: path: /run/promtail name: run - hostPath: path: /data/k8s/k8s_docker/data/containers name: containers - hostPath: path: /var/log/pods name: pods extraArgs: - -client.external-labels=cluster=dev extraVolumeMounts: - mountPath: /etc/localtime name: host-time extraVolumes: - hostPath: path: /etc/localtime name: host-time readinessProbe: httpGet: path: '{{ printf `%s/metrics` .Values.httpPathPrefix }}' port: http-metrics resources: limits: cpu: 512m memory: 512Mi ","date":"2023-06-15","objectID":"/posts/81200000/:2:1","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"部署 promtail # helm search repo grafana/promtail -l | head -5 NAME CHART VERSION APP VERSION DESCRIPTION grafana/promtail 6.11.3 2.8.2 Promtail is an agent which ships the contents o... grafana/promtail 6.11.2 2.8.2 Promtail is an agent which ships the contents o... grafana/promtail 6.11.1 2.7.4 Promtail is an agent which ships the contents o... grafana/promtail 6.11.0 2.7.4 Promtail is an agent which ships the contents o... helm install promtail grafana/promtail -n loki --version 6.11.3 -f promtail-values.yaml ","date":"2023-06-15","objectID":"/posts/81200000/:2:2","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"安装 Grafana ","date":"2023-06-15","objectID":"/posts/81200000/:3:0","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"grafana-values.yaml service: type: NodePort ","date":"2023-06-15","objectID":"/posts/81200000/:3:1","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"部署 grafana # helm search repo grafana/grafana -l | head -5 NAME CHART VERSION APP VERSION DESCRIPTION grafana/grafana 6.57.1 9.5.3 The leading tool for querying and visualizing t... grafana/grafana 6.57.0 9.5.2 The leading tool for querying and visualizing t... grafana/grafana 6.56.6 9.5.2 The leading tool for querying and visualizing t... grafana/grafana 6.56.5 9.5.2 The leading tool for querying and visualizing t... # helm install grafana grafana/grafana -n loki -f grafana-values.yaml Grafana 添加 loki 数据源 查看日志 ","date":"2023-06-15","objectID":"/posts/81200000/:3:2","tags":["loki","grafana"],"title":"Loki快速使用","uri":"/posts/81200000/"},{"categories":["监控体系"],"content":"Loki 系统架构 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:0:0","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"Loki 组件 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:1:0","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"Distributor (分发器) Distributor 服务负责处理客户端写入的日志，它本质上是日志数据写入路径中的入口，一旦 Distributor 收到日志数据，会将其拆分为多个批次，然后并行发送给多个 Ingester 。 Distributor 通过 gRPC 与 Ingester 通信，它们都是无状态的，所以可以根据需要扩大或缩小规模。 Validation (校验) Distributor 第一步是确保所有传入数据均符合规范。这包括检查标签是否是有效的 Prometheus 标签，以及确保时间戳不会太旧或太新，或者日志行不会太长。 Preprocessing (预处理) 规范化标签,对标签进行排序,从而能够准确的命中 cache 和 hash。 Rate limiting (限速) Distributor 可以根据每个租户的最大比特率对传入日志进行速率限制。 它通过检查每个租户的限制并将其除以当前的 Distributor 数量。 这允许在集群级别为每个租户指定速率限制，并使我们能够向上或向下扩展 Distributor，并相应地调整每个 Distributor 的限制 Forwarding (转发) Distributor 验证通过后，它就会将数据转发到最终负责确认写入的 Ingester 组件。 Replication factor (副本) 为了减少在任何单个 Ingester 上丢失数据的机会，Distributor 会将写入转发到它们的 replication_factor 。默认 3 副本。副本允许 Ingester 重新启动和推出而不会导致写入失败，并在某些情况下增加额外的数据丢失保护。 floor(replication_factor / 2) + 1 Hashing Distributor 将一致性Hash和可配置的复制因子结合使用，以确定 Ingester 服务的哪些实例应该接收指定的数据流。 流是一组与租户和唯一标签集关联的日志，使用租户 ID 和标签集对流进行 hash 处理，然后使用哈希查询要发送流的 Ingester Quorum consistency ","date":"2023-06-15","objectID":"/posts/c7c7b8/:1:1","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"Ingester Ingester 服务负责将日志数据写入存储后端（DynamoDB、S3、Cassandra ）。此外 Ingester 会验证摄取的日志行是否按照时间戳递增的顺序接收的（即每条日志的时间戳都比前面的日志晚一些），当 Ingester 收到不符合这个顺序的日志时，该日志行会被拒绝并返回一个错误。 如果传入的行与之前收到的行完全匹配（与之前的时间戳和日志文本都匹配），传入的行将被视为完全重复并被忽略。 如果传入的行与前一行的时间戳相同，但内容不同，则接受该日志行，表示同一时间戳有两个不同的日志行是可能的。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:1:2","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"Querier (查询器) Querier 接收日志数据查询、聚合统计请求，使用 LogQL 查询语言处理查询，从 ingester 和长期存储中获取日志。 查询器查询所有 ingester 的内存数据，然后再到后端存储运行相同的查询。由于复制因子，查询器有可能会收到重复的数据。为了解决这个问题，查询器在内部对具有相同纳秒时间戳、标签集和日志信息的数据进行重复数据删除。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:1:3","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"Query frontend Query Frontend 查询前端是一个可选的服务，可以用来加速读取路径。当查询前端就位时，将传入的查询请求定向到查询前端，而不是 querier, 为了执行实际的查询，群集中仍需要 querier 服务。 查询前端在内部执行一些查询调整，并在内部队列中保存查询。querier 作为 workers 从队列中提取作业，执行它们，并将它们返回到查询前端进行汇总。querier 需要配置查询前端地址，以便允许它们连接到查询前端。 查询前端是无状态的，然而，由于内部队列的工作方式，建议运行几个查询前台的副本，以获得公平调度的好处，在大多数情况下，两个副本应该足够了。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:1:4","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"数据处理流程 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:2:0","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"数据读取 查询器收到一个 HTTP/1 数据请求。 查询器将查询传递给内存中数据的所有 Ingester 。 Ingester 接收读取请求并返回与查询匹配的数据（如果有）。 如果没有 Ingester 返回数据，查询器会从后备存储中延迟加载数据并对其运行查询。 查询器遍历所有接收到的数据并删除重复数据，通过 HTTP/1 连接返回一组最终数据。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:2:1","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"数据写入 Distributor收到 HTTP/1 请求以存储流数据。 每个流都使用哈希环进行哈希处理。 Distributor将每个流发送到适当的Ingester及其副本（基于配置的复制因子）。 每个Ingester将为流的数据创建一个块或附加到现有块。每个租户和每个标签集的块都是唯一的。 分发者通过 HTTP/1 连接以成功代码响应。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:2:2","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"部署模式 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:3:0","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"单体模式 单体模式适用于快速开始体验 Loki 进行实验以及每天最多约 100GB 的小读/写量场景。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:3:1","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"读写分离模式 当日志量每天超过几百 GB，或者希望分离读写，Loki 提供了简单的读写分离可扩展部署模式。这种部署模式可以扩展到每天数 TB 的日志。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:3:2","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"微服务模式 每个组件独立部署，适用于超大型日志收集。 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:3:3","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["监控体系"],"content":"参考资料 Loki生产环境集群方案 Loki官方文档 Loki查询语法 ","date":"2023-06-15","objectID":"/posts/c7c7b8/:4:0","tags":["loki","Grafana"],"title":"Loki系统架构","uri":"/posts/c7c7b8/"},{"categories":["kubernetes"],"content":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理 基于 kubernetes v1.27.0 版本进行 hpa 源码分析. Kubernetes 的 HorizontalPodAutoscaler (hpa) 组件可以根据目标的资源使用率 (cpu, mem 等等), 动态的资源对象进行的合理扩缩容. hpa 通常是对 deployment 和 replicaset 资源进行自动伸缩. 比如, 当 deployment 关联的 pods 负载超过阈值时, hpa 对其进行动态扩容, 但扩容的副本数不能超过 maxReplicas. 反之, 如果 pods 负载减少, 则进行动态缩容. ","date":"2023-04-18","objectID":"/posts/32dd44/:1:0","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"hpa 的配置例子 分析 hpa 源码之前, 需要先理解 hpa 的配置选项, 不然不好理解后面的代码分析. 下面的 hpa 的配置中定义了最大和最小的副本数, 无论怎么超过定义的资源负载阈值, 不会超过 hpa 中定义的最大的副本数 10. 一样的逻辑, 不管 pods 再怎么空闲, 副本数也不能低于 5 个. 尽量确保 pods 的平均 cpu 使用率不超过 70%, 超过使用率时则需要立即执行扩容. 而进行 scale 缩容时, 则需要等待 300s 后再判断是否进行, 该等待操作是避免业务抖动引发的频繁的 scale pods 自动伸缩. 另外 behavior 的 policies 用来控制动态伸缩的速度, 避免一次性操作太多 pods 实例造成服务抖动, 通常 behavior 配置的原则是快速扩容, 低速缩容回收. kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2 metadata: name: nginx-xiaorui-cc spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx-xiaorui-cc minReplicas: 5 maxReplicas: 10 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 100 periodSeconds: 15 scaleUp: stabilizationWindowSeconds: 0 policies: - type: Percent value: 50 periodSeconds: 15 - type: Pods value: 1 periodSeconds: 15 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 ","date":"2023-04-18","objectID":"/posts/32dd44/:1:1","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"实例化 hpa 控制器 实例化 HorizontalController 控制器, 内部使用 hpaInformer 注册自定义的 eventHandler 事件处理方法. 另外 HorizontalController 还定义了两个 lister 实例. hpaLister 主要用来判断 hpa 是否重新入队, 如果已经不存在, 则无需重新入队. podLister 在计算预期副本数时, 需要考虑到有些 pod 未就绪或者正被清理掉. func NewHorizontalController( ... ) *HorizontalController { hpaController := \u0026HorizontalController{ queue: workqueue.NewNamedRateLimitingQueue(NewDefaultHPARateLimiter(resyncPeriod), \"horizontalpodautoscaler\"), ... } // 为 hpaInformer 增加 eventHandler 事件 hpaInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: hpaController.enqueueHPA, UpdateFunc: hpaController.updateHPA, DeleteFunc: hpaController.deleteHPA, }, resyncPeriod, ) // hpaLister hpaController.hpaLister = hpaInformer.Lister() hpaController.hpaListerSynced = hpaInformer.Informer().HasSynced // podLister hpaController.podLister = podInformer.Lister() hpaController.podListerSynced = podInformer.Informer().HasSynced // 实例化副本计算器 replicaCalc := NewReplicaCalculator( metricsClient, hpaController.podLister, tolerance, ... ) hpaController.replicaCalc = replicaCalc return hpaController } 注册的 ResourceEventHandler 当 podInformer 有事件变更, 如果是触发增加 AddFunc 和更新 UpdateFunc 方法, 直接往 workqueue 里推入 namespace/name 格式的 key 即可, 而当触发删除 DeleteFunc 操作时, 则需要调用 queue.Forget 删除. func (a *HorizontalController) updateHPA(old, cur interface{}) { a.enqueueHPA(cur) } func (a *HorizontalController) enqueueHPA(obj interface{}) { key, err := controller.KeyFunc(obj) if err != nil { return } a.queue.AddRateLimited(key) ... } func (a *HorizontalController) deleteHPA(obj interface{}) { key, err := controller.KeyFunc(obj) if err != nil { return } // 从队列中剔除 a.queue.Forget(key) ... } 默认启动参数 需要注意的是, 启动的 worker 协程数量默认为 5 个, queue 延迟入队时间为 15s. 代码位置: pkg/controller/podautoscaler/config/v1alpha1/defaults.go func RecommendedDefaultHPAControllerConfiguration(obj *kubectrlmgrconfigv1alpha1.HPAControllerConfiguration) { zero := metav1.Duration{} if obj.ConcurrentHorizontalPodAutoscalerSyncs == 0 { obj.ConcurrentHorizontalPodAutoscalerSyncs = 5 } if obj.HorizontalPodAutoscalerSyncPeriod == zero { obj.HorizontalPodAutoscalerSyncPeriod = metav1.Duration{Duration: 15 * time.Second} } ... } ","date":"2023-04-18","objectID":"/posts/32dd44/:1:2","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"启动 hpa 控制器 跟其他 k8s controller 的启动逻辑一样, 调用 Run() 方法启动 HorizontalController 控制器, 内部完成 hpa 和 pod 的 informer 同步完成, 启动多个 worker 协程处理 scale 逻辑, 默认为 5 个. 源码位置: pkg/controller/podautoscaler/horizontal.go func (a *HorizontalController) Run(ctx context.Context, workers int) { klog.Infof(\"Starting HPA controller\") defer klog.Infof(\"Shutting down HPA controller\") // 等待 informer 数据同步完成 if !cache.WaitForNamedCacheSync(\"HPA\", ctx.Done(), a.hpaListerSynced, a.podListerSynced) { return } // 启动 worker for i := 0; i \u003c workers; i++ { go wait.UntilWithContext(ctx, a.worker, time.Second) } \u003c-ctx.Done() } processNextWorkItem 从 workqueue 获取由 informer 的插入的 hpa 对象的 key. 其 key 格式为 namespace/name. 调用核心入口函数 reconcileKey 处理 hpa 逻辑. 当 hpa 资源已不存在时则需要删除, 反之需要再次插入到 workqueue 的 delayQueue 里, 用来实现延迟再入队的逻辑, 默认 15s 后再次入队. hpa 控制器需要不停的周期性对所有 hpa 的资源检测资源使用率, 才可根据阈值条件进行动态扩缩容. 所以 hpa 需要一个定时器的逻辑, 这里的定时器是放在 workqueue 实现的. func (a *HorizontalController) worker(ctx context.Context) { for a.processNextWorkItem(ctx) { } } func (a *HorizontalController) processNextWorkItem(ctx context.Context) bool { // 获取由 informer eventHandler 发到 queue 里的 key, 格式为 `namespace/name` key, quit := a.queue.Get() if quit { return false } defer a.queue.Done(key) // hpa 核心方法, 由该方法来实现扩缩容 deleted, err := a.reconcileKey(ctx, key.(string)) if err != nil { utilruntime.HandleError(err) } // 如果无异常且没有被删除, 则重新入队, 等待一段时间后又可消费此 hpa. if !deleted { a.queue.AddRateLimited(key) } return true } 如果从 workqueue 拿到的 hpa 对象已经被清理掉, 则返回 deleted 标记, 表明该对象无需入队. 反之则调用 reconcileAutoscaler 主逻辑. func (a *HorizontalController) reconcileKey(ctx context.Context, key string) (deleted bool, err error) { namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return true, err } hpa, err := a.hpaLister.HorizontalPodAutoscalers(namespace).Get(name) // 如果从 workqueue 拿到的 hpa 已经被清理掉, 则返回 deleted 标记, 该对象不再入队. if errors.IsNotFound(err) { ... return true, nil } if err != nil { return false, err } // 调用该方法处理 scale 逻辑 return false, a.reconcileAutoscaler(ctx, hpa, key) } 简单看下 workqueue 的实例化类型, 以及 AddRateLimited() 延迟插入的实现原理. 简单说, 其内部把延迟入队对象放到 delay 队列里, 本质 delay queue 数据结构为小顶堆, 使用到期时间进行 heap 排序. 另外内部启动一个协程去监听是否到期, 到期则插入对 queue 里. queue := workqueue.NewNamedRateLimitingQueue(NewDefaultHPARateLimiter(resyncPeriod), \"horizontalpodautoscaler\"), func (q *rateLimitingType) AddRateLimited(item interface{}) { q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item)) } ","date":"2023-04-18","objectID":"/posts/32dd44/:1:3","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"核心方法 reconcileAutoscaler reconcileAutoscaler 是 hpa 控制器的核心处理方法. 流程是这样, 先对一些参数做了调整修正, 而后调用 computeReplicasForMetrics 进行复杂的预期副本计算, 如果副本数跟当前不一致, 说明需要 scale 扩缩容. 接着使用 k8s client 对 hpa 关联对象执行 scale 副本调节请求, 最后更新 hpa 对象的状态. func (a *HorizontalController) reconcileAutoscaler(ctx context.Context, hpaShared *autoscalingv2.HorizontalPodAutoscaler, key string) error { hpa := hpaShared.DeepCopy() // 当前 scale 里副本数 currentReplicas := scale.Spec.Replicas // 预期的副本数 desiredReplicas := int32(0) // 调整预期的最小副本数 var minReplicas int32 if hpa.Spec.MinReplicas != nil { minReplicas = *hpa.Spec.MinReplicas } else { minReplicas = 1 } rescale := true if scale.Spec.Replicas == 0 \u0026\u0026 minReplicas != 0 { // 副本数为0, 不启动自动扩缩容 desiredReplicas = 0 rescale = false } else if currentReplicas \u003e hpa.Spec.MaxReplicas { // 如果当前副本数大于最大期望副本数, 那么设置期望副本数为最大副本数 desiredReplicas = hpa.Spec.MaxReplicas } else if currentReplicas \u003c minReplicas { // 如果当前副本数小于最小期望副本数, 那么设为最小副本数 desiredReplicas = minReplicas } else { var metricTimestamp time.Time // 通过 metrics 指标数据计算预期的的副本数 metricDesiredReplicas, metricName, metricStatuses, metricTimestamp, err = a.computeReplicasForMetrics(ctx, hpa, scale, hpa.Spec.Metrics) if err != nil { // 更新状态 a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa) return fmt.Errorf(\"failed to compute desired number of replicas based on listed metrics for %s: %v\", reference, err) } // 如果指标预期副本比预期副本大, 则使用指标预期副本 if metricDesiredReplicas \u003e desiredReplicas { desiredReplicas = metricDesiredReplicas } // 如果配置中存在 behavior 策略, 那么 if hpa.Spec.Behavior == nil { desiredReplicas = a.normalizeDesiredReplicas(hpa, key, currentReplicas, desiredReplicas, minReplicas) } else { desiredReplicas = a.normalizeDesiredReplicasWithBehaviors(hpa, key, currentReplicas, desiredReplicas, minReplicas) } // 如果预期副本跟当前副本数不一致则进行扩缩容操作 rescale = desiredReplicas != currentReplicas } if rescale { // 配置副本数 scale.Spec.Replicas = desiredReplicas // 进行扩缩容 _, err = a.scaleNamespacer.Scales(hpa.Namespace).Update(ctx, targetGR, scale, metav1.UpdateOptions{}) ... } else { desiredReplicas = currentReplicas } // 配置 hpa.status a.setStatus(hpa, currentReplicas, desiredReplicas, metricStatuses, rescale) // 更新状态 return a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa) } ","date":"2023-04-18","objectID":"/posts/32dd44/:1:4","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"计算预期的副本数 computeReplicasForMetrics 用来根据 metris 指标计算新副本数的方法. 其内部逻辑是这样, 先遍历 hpa.Spec.Metrics 列表依次调用 computeReplicasForMetric() 计算出最后预期副本数. computeReplicasForMetric 方法通过不同的指标类型获取需要扩缩副本的数量. func (a *HorizontalController) computeReplicasForMetrics(hpa *autoscalingv2.HorizontalPodAutoscaler, scale *autoscalingv1.Scale, metricSpecs []autoscalingv2.MetricSpec) (replicas int32, metric string, statuses []autoscalingv2.MetricStatus, timestamp time.Time, err error) { ... // 遍历 `hpa.Spec.Metrics` 列表. for i, metricSpec := range metricSpecs { // 通过不同的指标类型进一步获取需要扩缩副本的数量, 每次调用使用上一波预期的 replicas 进行传参调用 replicaCountProposal, metricNameProposal, timestampProposal, condition, err := a.computeReplicasForMetric(hpa, metricSpec, specReplicas, statusReplicas, selector, \u0026statuses[i]) ... // 重新赋值副本数 if err == nil \u0026\u0026 (replicas == 0 || replicaCountProposal \u003e replicas) { timestamp = timestampProposal replicas = replicaCountProposal metric = metricNameProposal } } ... return replicas, metric, statuses, timestamp, nil } 按照不同的指标类型计算预期出预期的副本数. 拿一个 hpa 的 yaml 举例说明 metrics 指标的使用. apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1 kind: Ingress name: main-route target: type: Value value: 10k - type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 hpa 尝试确保每个 Pod 的 CPU 利用率在 50% 以内, 每秒处理 1000 个数据包请求, 还确保 Ingress 后的 Pod 每秒能够服务的请求总数达到 10000 个. 这些指标不是 and 并且关系, 而是 or 或的关系, 只要超过一个指标也会触发扩缩容操作. hpa 会尝试遍历所有的指标条件, 计算出合理的副本数. hpa 可以根据不同的 metrics 进行 scale 扩缩容操作, 常用的是 resource 指标, 这里拿 resource 为例分析其计算过程. func (a *HorizontalController) computeReplicasForMetric(ctx context.Context, hpa *autoscalingv2.HorizontalPodAutoscaler, spec autoscalingv2.MetricSpec, specReplicas, statusReplicas int32, selector labels.Selector, status *autoscalingv2.MetricStatus) (replicaCountProposal int32, metricNameProposal string, timestampProposal time.Time, condition autoscalingv2.HorizontalPodAutoscalerCondition, err error) { switch spec.Type { case autoscalingv2.ObjectMetricSourceType: // 根据 k8s 内置对象的指标进行计算 case autoscalingv2.PodsMetricSourceType: // 通过 pods 的 metrics 进行计算 case autoscalingv2.ResourceMetricSourceType: // 描述 pod 的 cpu, mem 等资源指标进行计算 replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForResourceMetric(ctx, specReplicas, spec, hpa, selector, status) if err != nil { return 0, \"\", time.Time{}, condition, fmt.Errorf(\"failed to get %s resource metric value: %v\", spec.Resource.Name, err) } case autoscalingv2.ContainerResourceMetricSourceType: // 根据每个 pod 内 container 的 resource 指标进行计算, 比如容器内的 cpu, mem 等. case autoscalingv2.ExternalMetricSourceType: // 根据外部 external 指标进行计算 ... default: condition := a.getUnableComputeReplicaCountCondition(hpa, \"InvalidMetricSourceType\", err) return 0, \"\", time.Time{}, condition, err } return replicaCountProposal, metricNameProposal, timestampProposal, autoscalingv2.HorizontalPodAutoscalerCondition{}, nil } 依赖资源指标计算副本数的逻辑调用链略长, 最后其实是调用 GetResourceReplicas() 计算副本数. func (c *ReplicaCalculator) GetResourceReplicas(ctx context.Context, currentReplicas int32, targetUtilization int32, resource v1.ResourceName, namespace string, selector labels.Selector, container string) (replicaCount int32, utilization int32, rawUtilization int64, timestamp time.Time, err error) { // 获取符合条件 pods 的 metrics 指标 metrics, timestamp, err := c.metricsClient.GetResourceMetric(ctx, resource, namespace, selector, container) if err != nil { return 0, 0, 0, time.Time{}, fmt.Errorf(\"unable to get metrics for resource %s: %v\", resource, err) } // 获取 pods 对象集合 podList, err := c.podLister.Pods(namespace).List(selector) if err != nil { return 0, 0, 0, time.Time{}, fmt.Errorf(\"unable to","date":"2023-04-18","objectID":"/posts/32dd44/:1:5","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"向 apiserver 请求 Scales 操作 client-go 里的 Scales() 实现了对 apiserver 的请求方法. 其本质上是对 deployment/replicaset/… 的资源配置进行更新, 后面依赖各资源的 controller 控制器来实现具体的 scale 操作. 比如 hpa 控制器向 apiserver 发起 deployment 对象的 scale 操作, 最后由 deployment controller 通过 informer 感知 scale 变更, 而后触发 对 replicaset 操作, 后面再由 replicaset controller 完成最后的 scale 操作. 代码位置: vendor/k8s.io/client-go/scale/client.go func (c *scaleClient) Scales(namespace string) ScaleInterface { return \u0026namespacedScaleClient{ client: c, namespace: namespace, } } func (c *namespacedScaleClient) Update(ctx context.Context, resource schema.GroupResource, scale *autoscaling.Scale, opts metav1.UpdateOptions) (*autoscaling.Scale, error) { // 获取 path 和 gvr path, gvr, err := c.client.pathAndVersionFor(resource) // 获取 gvk desiredGVK, err := c.client.scaleKindResolver.ScaleForResource(gvr) if err != nil { return nil, fmt.Errorf(\"could not find proper group-version for scale subresource of %s: %v\", gvr.String(), err) } // 更换版本 scaleUpdate, err := scaleConverter.ConvertToVersion(scale, desiredGVK.GroupVersion()) if err != nil { return nil, fmt.Errorf(\"could not convert scale update to external Scale: %v\", err) } // 向 apiserver 发起请求. // resource 可以是 deployments 等资源类型 // name 为 hpa.Spec.ScaleTargetRef.Name, 通常为 hpa 关联对象的 name // subresource 子路径为 scale result := c.client.clientBase.Put(). AbsPath(path). NamespaceIfScoped(c.namespace, c.namespace != \"\"). Resource(gvr.Resource). Name(scale.Name). SubResource(\"scale\"). SpecificallyVersionedParams(\u0026opts, dynamicParameterCodec, versionV1). Body(scaleUpdateBytes). Do(ctx) if err := result.Error(); err != nil { return nil, err } return convertToScale(\u0026result) } ","date":"2023-04-18","objectID":"/posts/32dd44/:1:6","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"apiserver 对 scale 请求的处理 apiserver 启动时对 deployment, replicaset 和 statefulsets 资源进行 resource/scale 路由和 rest 方法注册. 另外, 看代码的意思 k8s 应该只实现了这三个资源类型的 scale 接口. 代码位置: pkg/registry/apps/rest/storage_apps.go func (p StorageProvider) v1Storage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) (map[string]rest.Storage, error) { storage := map[string]rest.Storage{} // deployments if resource := \"deployments\"; apiResourceConfigSource.ResourceEnabled(appsapiv1.SchemeGroupVersion.WithResource(resource)) { deploymentStorage, err := deploymentstore.NewStorage(restOptionsGetter) storage[resource] = deploymentStorage.Deployment storage[resource+\"/scale\"] = deploymentStorage.Scale } // statefulsets if resource := \"statefulsets\"; apiResourceConfigSource.ResourceEnabled(appsapiv1.SchemeGroupVersion.WithResource(resource)) { statefulSetStorage, err := statefulsetstore.NewStorage(restOptionsGetter) storage[resource] = statefulSetStorage.StatefulSet storage[resource+\"/scale\"] = statefulSetStorage.Scale } // replicasets if resource := \"replicasets\"; apiResourceConfigSource.ResourceEnabled(appsapiv1.SchemeGroupVersion.WithResource(resource)) { replicaSetStorage, err := replicasetstore.NewStorage(restOptionsGetter) storage[resource] = replicaSetStorage.ReplicaSet storage[resource+\"/status\"] = replicaSetStorage.Status storage[resource+\"/scale\"] = replicaSetStorage.Scale } ... return storage, nil } 下面是 apiserver 里 deployment scale 的具体实现. 简单说就是对 deployments 对象调整副本数, 让其配套的控制器去做具体 scale 落地操作. // ScaleREST implements a Scale for Deployment. type ScaleREST struct { store *genericregistry.Store } // Update alters scale subset of Deployment object. func (r *ScaleREST) Update(ctx context.Context, name string, objInfo rest.UpdatedObjectInfo, createValidation rest.ValidateObjectFunc, updateValidation rest.ValidateObjectUpdateFunc, forceAllowCreate bool, options *metav1.UpdateOptions) (runtime.Object, bool, error) { obj, _, err := r.store.Update( ctx, name, \u0026scaleUpdatedObjectInfo{name, objInfo}, toScaleCreateValidation(createValidation), toScaleUpdateValidation(updateValidation), false, options, ) ... deployment := obj.(*apps.Deployment) newScale, err := scaleFromDeployment(deployment) if err != nil { return nil, false, errors.NewBadRequest(fmt.Sprintf(\"%v\", err)) } return newScale, false, nil } 关于 apiserver 就点到为止, 它的实现原理在其他章节有分析, 这里就不再复述了. ","date":"2023-04-18","objectID":"/posts/32dd44/:1:7","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":"总结 hpa 控制器相比其他控制器来说, 其实现原理还是相对好理解的. hpa 控制器启动时监听 hpaInformer, 当有 hpa 资源对象发生变更时, 通过 hpa 控制器注册的 eventHandler 把 hpa 的变动推到 workqueue 里. 而后就是计算该 hpa 的副本预期值, 需要通过设定的 metrics 阈值进行一系列的计算. 每个 metrics 都对应一套计算方法, 最后的预期副本数往往是经过计算后累加的. 最后通过调用 client-go 的 scales 接口, 向 apiserver 请求以实现对 deployment/replicase 的副本数调整. hpa 肯定不是检测一次就完事了, 往往需要不断周期性去检测 hpa 是否满足 scale 条件. 这个周期性的轮询操作是通过 workqueue delayInteface 实现的. hpa 难点在于如何根据 metrics 指标和目标阈值, 计算出合理的预期副本数. 下面是计算副本数的调用关系图. 出处: https://github.com/rfyiamcool/notes#kubernetes ","date":"2023-04-18","objectID":"/posts/32dd44/:1:8","tags":["源码","hpa","转载"],"title":"源码分析 kubernetes hpa controller 水平自动扩缩容的实现原理","uri":"/posts/32dd44/"},{"categories":["kubernetes"],"content":" 基于 kubernetes v1.27.0 源码分析 scheduler 调度器 k8s scheduler 的主要职责是为新创建的 pod 寻找一个最合适的 node 节点, 然后进行 bind node 绑定, 后面 kubelet 才会监听到并创建真正的 pod. 那么问题来了, 如何为 pod 寻找最合适的 node ? 调度器需要经过 predicates 预选和 priority 优选. 预选就是从集群的所有节点中根据调度算法筛选出所有可以运行该 pod 的节点集合 优选则是按照算法对预选出来的节点进行打分，找到分值最高的节点作为调度节点. 选出最优节点后, 对 apiserver 发起 pod 节点 bind 操作, 其实就是对 pod 的 spec.NodeName 赋值最优节点. 源码基本调用关系 ","date":"2023-04-18","objectID":"/posts/a3f5fa/:0:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"k8s scheduler 启动入口 k8s scheduler 在启动时会调用在 cobra 注册的 setup 来初始化 scheduler 调度器对象. 代码位置: cmd/kube-scheduler/app/server.go func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) { // 获取默认配置 if cfg, err := latest.Default(); err != nil { return nil, nil, err } else { opts.ComponentConfig = cfg } // 验证 scheduler 的配置参数 if errs := opts.Validate(); len(errs) \u003e 0 { return nil, nil, utilerrors.NewAggregate(errs) } c, err := opts.Config() if err != nil { return nil, nil, err } // 配置中填充和调整 cc := c.Complete() ... // 构建 scheduler 对象 sched, err := scheduler.New(cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, ctx.Done(), ... ) if err != nil { return nil, nil, err } ... return \u0026cc, sched, nil } 实例化 kubernetes scheduler 对象, 初始化流程直接看下面代码. 代码位置: pkg/scheduler/scheduler.go // New returns a Scheduler func New(client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, stopCh \u003c-chan struct{}, opts ...Option) (*Scheduler, error) { // 构建 registry 对象, 默认集成了一堆的插件 registry := frameworkplugins.NewInTreeRegistry() if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil { return nil, err } // profiles 用来保存不同调度器的 framework 框架, framework 则用来存放 plugin. profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), ... ... ) // 实例化快照 snapshot := internalcache.NewEmptySnapshot() // 实例化 queue, 该 queue 为 PriorityQueue. podQueue := internalqueue.NewSchedulingQueue( profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, ... ) // 实例化 cache 缓存 schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything) // 实例化 scheduler 对象 sched := \u0026Scheduler{ Cache: schedulerCache, client: client, nodeInfoSnapshot: snapshot, NextPod: internalqueue.MakeNextPodFunc(podQueue), StopEverything: stopEverything, SchedulingQueue: podQueue, } sched.applyDefaultHandlers() // 在 informer 里注册自定义的事件处理方法 addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap)) return sched, nil } func (s *Scheduler) applyDefaultHandlers() { s.SchedulePod = s.schedulePod s.FailureHandler = s.handleSchedulingFailure } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:1:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"注册在 scheduler informer 的回调方法 在 informer 里注册 pod 和 node 资源的回调方法，监听 pod 事件对 queue 和 cache 做回调处理. 监听 node 事件对 cache 做处理. func addAllEventHandlers( sched *Scheduler, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, gvkMap map[framework.GVK]framework.ActionType, ) { // 监听 pod 事件，并注册增删改回调方法, 其操作是对 cache 的增删改 informerFactory.Core().V1().Pods().Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { ... }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: sched.addPodToCache, UpdateFunc: sched.updatePodInCache, DeleteFunc: sched.deletePodFromCache, }, }, ) // 监听 pod 事件，并注册增删改方法, 对 queue 插入增删改事件. informerFactory.Core().V1().Pods().Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { ... }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: sched.addPodToSchedulingQueue, UpdateFunc: sched.updatePodInSchedulingQueue, DeleteFunc: sched.deletePodFromSchedulingQueue, }, }, ) // 监听 node 事件，注册回调方法，该方法在 cache 里对 node 的增删改查. informerFactory.Core().V1().Nodes().Informer().AddEventHandler( cache.ResourceEventHandlerFuncs{ AddFunc: sched.addNodeToCache, UpdateFunc: sched.updateNodeInCache, DeleteFunc: sched.deleteNodeFromCache, }, ) } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:2:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"scheduler 的选举实现 kube-scheduler 跟 k8s 的其他主控组件一样, 也会通过选举 leaderelection 机制保证集群只有一个 leader 实例运行调度器, 其他 follower 实例则尝试轮询抢锁直到成功. 源码位置: cmd/kube-scheduler/app/server.go func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error { ... waitingForLeader := make(chan struct{}) isLeader := func() bool { select { case _, ok := \u003c-waitingForLeader: // if channel is closed, we are leading return !ok default: // channel is open, we are waiting for a leader return false } } // 启动 informers, 这里只有 pod 和 node. cc.InformerFactory.Start(ctx.Done()) // 同步 informer 的数据到本地缓存 cc.InformerFactory.WaitForCacheSync(ctx.Done()) // 如果在配置中启动了选举, 创建选举对象, 注册事件方法, 并启用选举. if cc.LeaderElection != nil { cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { // 当选举拿到 leader 时, 启动 scheduler 调度器 close(waitingForLeader) sched.Run(ctx) }, OnStoppedLeading: func() { // 当选举成功但后面又丢失 leader 后, 则退出进程. // 进程退出后, 会被 docker 或 systemd 重新拉起, 尝试拿锁. select { case \u003c-ctx.Done(): // We were asked to terminate. Exit 0. os.Exit(0) default: // We lost the lock. klog.ErrorS(nil, \"Leaderelection lost\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } }, } // 构建 leaderelection 对象 leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection) if err != nil { return fmt.Errorf(\"couldn't create leader elector: %v\", err) } // 启动选举 leaderElector.Run(ctx) return fmt.Errorf(\"lost lease\") } // 如果没有开启选举, 则直接启动 scheduler 调度器. close(waitingForLeader) sched.Run(ctx) return fmt.Errorf(\"finished without leader elect\") } 关于 client-go LeaderElection 选举的实现原理, 请点击下面连接. https://github.com/rfyiamcool/notes/blob/main/kubernetes_leader_election_code.md ","date":"2023-04-18","objectID":"/posts/a3f5fa/:3:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"scheudler 启动入口 Run() 方法是 k8s scheduler 的启动运行入口, 其流程是先启动 queue 队列的 Run 方法, 再异步启动一个协程处理核心调度方法 scheduleOne. schedulingQueue 的 Run() 方法用来监听内部的延迟任务, 把到期的任务放到 activeQ 中. 而 scheduleOne 方法用来从优先级队列里获取由 informer 插入的 pod 对象, 调用 schedulingCycle 为 pod 选择最优的 node 节点. 如果找到了合适的 node 节点, 则调用 bindingCycle 方法来发起 pod 和 node 绑定. 源码位置: pkg/scheduler/scheduler.go func (sched *Scheduler) Run(ctx context.Context) { sched.SchedulingQueue.Run() go wait.UntilWithContext(ctx, sched.scheduleOne, 0) \u003c-ctx.Done() sched.SchedulingQueue.Close() } func (sched *Scheduler) scheduleOne(ctx context.Context) { // 从 activeQ 中获取需要调度的 pod 数据 podInfo := sched.NextPod() pod := podInfo.Pod // 为 pod 选择最优的 node 节点 scheduleResult, assumedPodInfo, err := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if err != nil { // 如何没有找到节点，则执行失败方法. sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, err, scheduleResult.reason, scheduleResult.nominatingInfo, start) return } go func() { // 像 apiserver 发起 pod -\u003e node 绑定 status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) if !status.IsSuccess() { sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status) } }() } NextPod 底层引用了 MakeNextPodFunc 方法, 其内部从 PriorityQueue 队列中获取 pod 对象. func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo { return func() *framework.QueuedPodInfo { podInfo, err := queue.Pop() if err == nil { return podInfo } return nil } } func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) { p.lock.Lock() defer p.lock.Unlock() for p.activeQ.Len() == 0 { // 没有数据则陷入条件变量的等待接口 p.cond.Wait() } obj, err := p.activeQ.Pop() if err != nil { return nil, err } pInfo := obj.(*framework.QueuedPodInfo) pInfo.Attempts++ // 每次 pop 后都增加下 attempts 次数. return pInfo, nil } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:4:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"单实例单协程模型的 schduler 调度器 需要关注的是整个 kubernetes scheduler 调度器只有一个协程处理主调度循环 scheduleOne, 虽然 kubernetes scheduler 可以启动多个实例, 但启动时需要 leaderelection 选举, 只有 leader 才可以处理调度, 其他节点作为 follower 等待 leader 失效. 也就是说整个 k8s 集群调度核心的并发度为 1 个. 云原生社区中有人使用 kubemark 模拟 2000 个节点的规模来压测 kube-scheduler 处理性能及时延, 测试结果是 30s 内完成 15000 个 pod 调度任务. 虽然 kube-scheduler 是单并发模型, 但由于预选和优选都属于计算型任务非阻塞IO, 又有 percentageOfNodesToScore 参数优化, 最重要的是创建 pod 的操作通常不会太高并发. 这几点下来单并发模型的 scheduler 也还可以接受的. ","date":"2023-04-18","objectID":"/posts/a3f5fa/:5:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"为什么 scheduler 不支持并发 ? 按照当前 scheudler 调度器的设计原理, 使用预选和优选算法选出最合适的节点, 并发场景下无法保证安全, 比如, 选出的最优节点在并发下会被多个 pod 绑定. ","date":"2023-04-18","objectID":"/posts/a3f5fa/:5:1","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"使用自定义调度器进行并发调度 ? k8s 默认的调度器为 default-scheduler, 而使用相同调度器只能单并发处理调度. 但是可以使用自定义实现调度器的方案, 在创建 pod 时指定不同的调度器算法 pod.Spec.schedulerName = xiaorui.cc, 这样可以使不同调度器的 kube-schedueler 并行调度起来, 各自按照调度算法来调度, 运行互不影响. 当然大多数公司没这个必要. ","date":"2023-04-18","objectID":"/posts/a3f5fa/:5:2","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"schedulingCycle 核心调度周期的实现 schedulingCycle() 该方法主要为 pod 选出最优的 node 节点. 先通过预选过程过滤出符合 pod 要求的节点集合, 再通过插件对这些节点进行打分, 使用分值最高的 node 为 pod 调度节点. scheduler 内置各个阶段的各种插件, 预选和优选阶段就是遍历回调插件求出结果. 调度周期 schedulingCycle 内关键方法是 schedulePod, 其简化流程如下. 先调用 findNodesThatFitPod 过滤出符合要求的预选节点. 调用 prioritizeNodes 为预选出来的节点进行打分 score. 最后调用 selectHost 选择最合适的 node 节点. func (sched *Scheduler) schedulingCycle( ... ) (ScheduleResult, *framework.QueuedPodInfo, error) { pod := podInfo.Pod // 选择节点 scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) ... // 在缓存 cache 中更新状态 err = sched.assume(assumedPod, scheduleResult.SuggestedHost) ... } func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { // 更新快照 if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil { return result, err } // 进行预选筛选 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil { return result, err } // 预选下来，无节点可以用, 返回错误 if len(feasibleNodes) == 0 { return result, \u0026framework.FitError{ Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, } } // 经过预选就只有一个节点，那么直接返回 if len(feasibleNodes) == 1 { return ScheduleResult{ SuggestedHost: feasibleNodes[0].Name, EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap), FeasibleNodes: 1, }, nil } // 进行优选过程, 对预选出来的节点进行打分 priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil { return result, err } // 从优选中选出最高分的节点 host, err := selectHost(priorityList) return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap), FeasibleNodes: len(feasibleNodes), }, err } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:6:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"findNodesThatFitPod findNodesThatFitPod 方法用来实现调度器的预选过程, 其内部调用插件的 PreFilter 和 Filter 方法来筛选出符合 pod 要求的 node 节点集合. func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) { diagnosis := framework.Diagnosis{ NodeToStatusMap: make(framework.NodeToStatusMap), UnschedulablePlugins: sets.NewString(), } // 获取所有的 nodes allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List() if err != nil { return nil, diagnosis, err } // 调用 framework 的 PreFilter 集合里的插件 preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod) if !s.IsSuccess() { // 如果在 prefilter 有异常, 则直接跳出. if !s.IsUnschedulable() { return nil, diagnosis, s.AsError() } msg := s.Message() diagnosis.PreFilterMsg = msg return nil, diagnosis, nil } ... // 根据 prefilter 拿到的 node names 获取 node info 对象. nodes := allNodes if !preRes.AllNodes() { nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames)) for n := range preRes.NodeNames { nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n) if err != nil { return nil, diagnosis, err } nodes = append(nodes, nInfo) } } // 运行 framework 的 filter 插件判断 node 是否可以运行新 pod. feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes) ... // 调用额外的 extender 调度器来进行预选 feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap) if err != nil { return nil, diagnosis, err } return feasibleNodes, diagnosis, nil } findNodesThatPassFilters 并发执行 Filter 插件方法 findNodesThatPassFilters 方法用来遍历执行 framework 里 Filter 插件集合的 Filter 方法. 为了加快执行效率, 减少预选阶段的时延, framework 内部有个 Parallelizer 并发控制器, 启用 16 个协程并发调用插件的 Filter 方法. 在大集群下 nodes 节点会很多, 为了避免遍历全量的 nodes 执行 Filter 和后续的插件逻辑, 这里通过 numFeasibleNodesToFind 方法来减少扫描计算的 nodes 数量. 当成功执行 filter 插件方法的数量超过 numNodesToFind 时, 则执行 context cancel(). 这样 framework 并发协程池监听到 ctx 被关闭后, 则不会继续执行后续的任务. func (sched *Scheduler) findNodesThatPassFilters( ctx context.Context, fwk framework.Framework, pod *v1.Pod, nodes []*framework.NodeInfo) ([]*v1.Node, error) { numAllNodes := len(nodes) // 计算需要扫描的 nodes 数, 避免了超大集群下 nodes 的计算数量. // 当集群的节点数小于 100 时, 则直接使用集群的节点数作为扫描数据量 // 当大于 100 时, 则使用公式计算 `numAllNodes * (50 - numAllNodes/125) / 100` numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes)) feasibleNodes := make([]*v1.Node, numNodesToFind) // 如果 framework 未注册 Filter 插件, 则退出. if !fwk.HasFilterPlugins() { for i := range feasibleNodes { feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes].Node() } return feasibleNodes, nil } // framework 内置并发控制器, 并发 16 个协程去请求插件的 Filter 方法. errCh := parallelize.NewErrorChannel() var statusesLock sync.Mutex checkNode := func(i int) { // 获取 node info 对象 nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes] // 遍历执行 framework 的 Filter 插件的 Filter 方法. status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo) if status.Code() == framework.Error { // 如果有错误, 直接把错误传到 errCh 管道里. errCh.SendErrorWithCancel(status.AsError(), cancel) return } if status.IsSuccess() { // 如果成功执行 Filter 插件的数量超过 numNodesToFind, 则执行 cancel(). // 当 ctx 被 cancel(), framework 的并发协程池不会继续执行后续的任务. length := atomic.AddInt32(\u0026feasibleNodesLen, 1) if length \u003e numNodesToFind { cancel() atomic.AddInt32(\u0026feasibleNodesLen, -1) } else { feasibleNodes[length-1] = nodeInfo.Node() } } ... } // 并发调用 framework 的 Filter 插件的 Filter 方法. fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, frameworkruntime.Filter) feasibleNodes = feasibleNodes[:feasibleNodesLen] if err := errCh.ReceiveError(); err != nil { // 当有错误时, 直接返回. statusCode = framework.Error return feasibleNodes, err } // 返回可用的 nodes 列表. return feasibleNodes, nil } framework 的并发库也是通过封装 workqueue.ParallelizeUntil 来实现的, 关于 parallelizer 的实现原理这里就不做分析了. k8s.io/client-go/util/workqueue/parallelizer.go numFeasibleNodesToFind 计算多少节点参与预选 🤔 考虑一个问题, 当 k8s 的 node 节点特别多时, 这些节点都要参与预先的调度过程么 ? 比如大集群有 2500 个节点, 注册的插件有 10 个, 那么 筛选 Filter 和 打分 Score 过程需要进行 2500 * 10 *","date":"2023-04-18","objectID":"/posts/a3f5fa/:6:1","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"prioritizeNodes prioritizeNodes 方法为调度器的优选阶段的实现. 其内部会遍历调用 framework 的 PreScore 插件集合里 PeScore 方法, 然后再遍历调用 framework 的 Score 插件集合的 Score 方法. 经过 Score 打分计算后可以拿到各个 node 的分值. func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*v1.Node, ) ([]framework.NodePluginScores, error) { // 如果 extenders 为空和score 插件为空, 则跳出 if len(extenders) == 0 \u0026\u0026 !fwk.HasScorePlugins() { result := make([]framework.NodePluginScores, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodePluginScores{ Name: nodes[i].Name, TotalScore: 1, }) } return result, nil } // 在 framework 的 PreScore 插件集合里, 遍历执行插件的 PreSocre 方法 preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() { // 只有有异常直接退出 return nil, preScoreStatus.AsError() } // 在 framework 的 Score 插件集合里, 遍历执行插件的 Socre 方法 nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() { return nil, scoreStatus.AsError() } klogV := klog.V(10) if klogV.Enabled() { for _, nodeScore := range nodesScores { // 打印插件名字和分值 score for _, pluginScore := range nodeScore.Scores { klogV.InfoS(\"Plugin scored node for pod\", \"pod\", klog.KObj(pod), \"plugin\", pluginScore.Name, \"node\", nodeScore.Name, \"score\", pluginScore.Score) } } } if len(extenders) != 0 \u0026\u0026 nodes != nil { // 当额外 extenders 调度器不为空时, 则需要计算分值. ... ... } return nodesScores, nil } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:6:2","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"selectHost selectHost 是从优选的 nodes 集合里获取分值 socre 最高的 node. 内部还做了一个小优化, 当相近的两个 node 分值相同时, 则通过随机来选择 node, 目的使 k8s node 的负载更趋于均衡. func selectHost(nodeScores []framework.NodePluginScores) (string, error) { // 如果 nodes 为空, 则返回错误 if len(nodeScores) == 0 { return \"\", fmt.Errorf(\"empty priorityList\") } // 直接从头到位遍历 nodeScores 数组, 拿到分值 score 最后的 nodeName. maxScore := nodeScores[0].TotalScore selected := nodeScores[0].Name cntOfMaxScore := 1 for _, ns := range nodeScores[1:] { if ns.TotalScore \u003e maxScore { // 当前的分值更大, 则进行赋值. maxScore = ns.TotalScore selected = ns.Name cntOfMaxScore = 1 } else if ns.TotalScore == maxScore { // 当两个 node 的 分值相同时, // 使用随机算法来选择当前和上一个 node. cntOfMaxScore++ if rand.Intn(cntOfMaxScore) == 0 { selected = ns.Name } } } // 返回分值最高的 node return selected, nil } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:6:3","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"PriorityQueue 的实现 PriorityQueue 用来实现优先级队列, informer 会条件 pod 到 priorityQueue 队列中, scheduleOne 会从该队列中 pop 对象. 在创建延迟队列时传入一个 less 比较方法, 时间最小的 podInfo 放在 heap 的最顶端. flushBackoffQCompleted 会不断的检查 backoff heap 堆顶的元素是否满足条件, 当满足条件把 pod 对象扔到 activeQ 队里, 并激活条件变量. 这里没有采用监听等待堆顶到期时间的方法，而是每隔一秒去检查堆顶的 podInfo 是否已到期 isPodBackingoff. backoff 时长是依赖 podInfo.Attempts 重试次数的，默认情况下重试 5次 是 5s, 最大不能超过 10s. 主调度方法 scheduleOne 每次从队列获取 podInfo 时, 它的 Attempts 字段都会加一. 代码位置: pkg/scheduler/internal/queue/scheduling_queue.go const ( DefaultPodMaxBackoffDuration time.Duration = 10 * time.Second ) // 实例化优先级队列，该队列中含有 activeQ, podBackoffQ, unschedulablePods集合. func NewPriorityQueue() { pq.podBackoffQ = heap.NewWithRecorder( podInfoKeyFunc, pq.podsCompareBackoffCompleted, ... ) } // 添加 pod 对象 func (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() pInfo := p.newQueuedPodInfo(pod) gated := pInfo.Gated // 把对象加到 activeQ 队列里. if added, err := p.addToActiveQ(pInfo); !added { return err } // 如果该对象在不可调度集合中存在, 则需要在里面删除. if p.unschedulablePods.get(pod) != nil { p.unschedulablePods.delete(pod, gated) } // 从 backoffQ 删除 pod 对象 if err := p.podBackoffQ.Delete(pInfo); err == nil { klog.ErrorS(nil, \"Error: pod is already in the podBackoff queue\", \"pod\", klog.KObj(pod)) } // 条件变量通知 p.cond.Broadcast() return nil } // 获取 pod info 对象 func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) { p.lock.Lock() defer p.lock.Unlock() // 如果 activeQ 为空, 陷入等待 for p.activeQ.Len() == 0 { if p.closed { return nil, fmt.Errorf(queueClosed) } p.cond.Wait() } // 从 activeQ 堆顶 pop 对象 obj, err := p.activeQ.Pop() if err != nil { return nil, err } pInfo := obj.(*framework.QueuedPodInfo) // 加一 pInfo.Attempts++ p.schedulingCycle++ return pInfo, nil } // heap 的比较方法，确保 deadline 最低的在 heap 顶部. func (p *PriorityQueue) podsCompareBackoffCompleted(podInfo1, podInfo2 interface{}) bool { bo1 := p.getBackoffTime(pInfo1) bo2 := p.getBackoffTime(pInfo2) return bo1.Before(bo2) } func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time { duration := p.calculateBackoffDuration(podInfo) backoffTime := podInfo.Timestamp.Add(duration) return backoffTime } // backoff duration 随着重试次数不断叠加，但最大不能超过 maxBackoffDuration. func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration { duration := p.podInitialBackoffDuration // 1s for i := 1; i \u003c podInfo.Attempts; i++ { if duration \u003e p.podMaxBackoffDuration-duration { return p.podMaxBackoffDuration // 10s } duration += duration } return duration } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:6:4","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"如何处理调度失败的 pod 前面有说 kubernetes scheduler 的 scheduleOne 作为主循环处理函数，当没有为 pod 找到合适 node 时，会调用 FailureHandler 方法. FailureHandler() 是由 handleSchedulingFailure() 方法实现. 该逻辑的实现简单说就是把失败的 pod 扔到 podBackoffQ 队列或者 unschedulablePods 集合里. func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, err error, reason string, nominatingInfo *framework.NominatingInfo, start time.Time) { podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister() cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name) if e != nil { } else { if len(cachedPod.Spec.NodeName) != 0 { klog.InfoS(\"Pod has been assigned to node. Abort adding it back to queue.\", \"pod\", klog.KObj(pod), \"node\", cachedPod.Spec.NodeName) } else { podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy()) // 重新入队列 if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil { klog.ErrorS(err, \"Error occurred\") } } } ... } func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error { pod := pInfo.Pod // 去重判断 if _, exists, _ := p.activeQ.Get(pInfo); exists { return fmt.Errorf(\"Pod %v is already present in the active queue\", klog.KObj(pod)) } // 去重判断 if _, exists, _ := p.podBackoffQ.Get(pInfo); exists { return fmt.Errorf(\"Pod %v is already present in the backoff queue\", klog.KObj(pod)) } // 把没有调度成功的 podInfo 扔到 backoffQ 队列或者 unschedulablePods 集合中. if p.moveRequestCycle \u003e= podSchedulingCycle { if err := p.podBackoffQ.Add(pInfo); err != nil { return fmt.Errorf(\"error adding pod %v to the backoff queue: %v\", klog.KObj(pod), err) } } else { p.unschedulablePods.addOrUpdate(pInfo) } return nil } scheduler queue 在启动时会开启两个常驻的协程. 一个协程来管理 flushBackoffQCompleted()，每隔一秒来调用一次. 另一个协程来管理 flushUnschedulablePodsLeftover, 每隔三十秒来调用一次. // Run starts the goroutine to pump from podBackoffQ to activeQ func (p *PriorityQueue) Run() { go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop) go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop) } flushBackoffQCompleted 从 podBackoffQ 获取 podInfo, 然后扔到 activeQ 里，等待 scheduleOne 来调度处理. func (p *PriorityQueue) flushBackoffQCompleted() { activated := false for { rawPodInfo := p.podBackoffQ.Peek() pInfo := rawPodInfo.(*framework.QueuedPodInfo) pod := pInfo.Pod // 从 podBackoffQ 中获取上次调度失败的 podInfo. _, err := p.podBackoffQ.Pop() if err != nil { klog.ErrorS(err, \"Unable to pop pod from backoff queue despite backoff completion\", \"pod\", klog.KObj(pod)) break } // 然后把这 podInfo 再扔到 activeQ 里，让 scheduleOne 主循环来处理. if added, _ := p.addToActiveQ(pInfo); added { klog.V(5).InfoS(\"Pod moved to an internal scheduling queue\", \"pod\", klog.KObj(pod), \"event\", BackoffComplete, \"queue\", activeQName) activated = true } } // 如果有添加成功的，则激活条件变量. if activated { p.cond.Broadcast() } } flushUnschedulablePodsLeftover 加锁遍历 PodInfoMap, 如果某个 pod 的距离上次的调度时间大于60s, 则扔到两个队列中的一个，否则等待下个 30s 再来处理. func (p *PriorityQueue) flushUnschedulablePodsLeftover() { p.lock.Lock() defer p.lock.Unlock() var podsToMove []*framework.QueuedPodInfo for _, pInfo := range p.unschedulablePods.podInfoMap { lastScheduleTime := pInfo.Timestamp if currentTime.Sub(lastScheduleTime) \u003e p.podMaxInUnschedulablePodsDuration { podsToMove = append(podsToMove, pInfo) } } if len(podsToMove) \u003e 0 { // 把 podInfo 扔到 activeQ 和 backoffQ 队列中. p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout) } } func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event framework.ClusterEvent) { activated := false for _, pInfo := range podInfoList { pod := pInfo.Pod if p.isPodBackingoff(pInfo) { // 如果还需要 backoff 退避, 则扔到 podBackoffQ 队列中进行延迟, 后面由 flushBackoffQCompleted 处理. if err := p.podBackoffQ.Add(pInfo); err != nil { } else { p.unschedulablePods.delete(pod, pInfo.Gated) } } else { // 把 podInfo 扔到 activeQ 里, 等待 scheduleOne 调度处理. if added, _ := p.addToActiveQ(pInfo); added { activated = true ","date":"2023-04-18","objectID":"/posts/a3f5fa/:6:5","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"bindingCycle 实现 pod 和 node 绑定 bindingCycle 用来实现 pod 和 node 绑定, 其过程为 prebind -\u003e bind -\u003e postbind. func (sched *Scheduler) bindingCycle( ... { assumedPod := assumedPodInfo.Pod // 执行插件的 prebind 逻辑 if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() { return status } // 执行 bind 插件逻辑 if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() { return status } // 在 bind 绑定后执行收尾操作 fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) return nil } func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) { defer func() { sched.finishBinding(fwk, assumed, targetNode, status) }() bound, err := sched.extendersBinding(assumed, targetNode) if bound { return framework.AsStatus(err) } return fwk.RunBindPlugins(ctx, state, assumed, targetNode) } func (sched *Scheduler) extendersBinding(pod *v1.Pod, node string) (bool, error) { for _, extender := range sched.Extenders { if !extender.IsBinder() || !extender.IsInterested(pod) { continue } return true, extender.Bind(\u0026v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: pod.Namespace, Name: pod.Name, UID: pod.UID}, Target: v1.ObjectReference{Kind: \"Node\", Name: node}, }) } return false, nil } extender 默认就只有 DefaultBinder 插件, 该插件的 bind 逻辑是通过 clientset 对 pod 进行 node 绑定.. 代码位置: pkg/scheduler/framework/plugins/defaultbinder/default_binder.go func (b DefaultBinder) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status { binding := \u0026v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: p.Namespace, Name: p.Name, UID: p.UID}, Target: v1.ObjectReference{Kind: \"Node\", Name: nodeName}, } err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{}) if err != nil { return framework.AsStatus(err) } return nil } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:7:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"framework 的实现原理 k8s scheduler 设计了一套 framework 作为调度器的插件系统. 另外在 k8s 中内置了一波插件, 插件是可以在多个扩展点处进行注册. 当调度器在启动时会加载注册插件到 registry, 当 pod 需要创建时, scheduler 通过插件系统过滤出适合的节点集合, 然后在通过实现 socre 打分的插件选出分值最高的节点. ","date":"2023-04-18","objectID":"/posts/a3f5fa/:8:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"framework 关键挂载点分析 PreFilter 在调用 Filter 方法前, 执行 PreFilter 预筛选逻辑. 插件会检测集群和 pod 是否满足定义的条件. 当不满足时返回错误, 则停止调度周期. Filter 这些插件用于过滤出不能运行该 Pod 的节点. PostFilter 这些插件在 Filter 阶段后调用, 但仅在该 Pod 没有可行的节点时调用. PreScore 插件在执行 Score 打分前, 可以先进行 PreScore 前置评分工作. Score 对通过预选阶段的节点集合进行打分. ","date":"2023-04-18","objectID":"/posts/a3f5fa/:8:1","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"scheduler 内置插件的实现原理 k8s scheduler 内置了不少插件, 下面是内置的插件. func getDefaultPlugins() *v1beta3.Plugins { plugins := \u0026v1beta3.Plugins{ MultiPoint: v1beta3.PluginSet{ Enabled: []v1beta3.Plugin{ {Name: names.PrioritySort}, {Name: names.NodeUnschedulable}, {Name: names.NodeName}, {Name: names.TaintToleration, Weight: pointer.Int32(3)}, {Name: names.NodeAffinity, Weight: pointer.Int32(2)}, {Name: names.NodePorts}, {Name: names.NodeResourcesFit, Weight: pointer.Int32(1)}, {Name: names.VolumeRestrictions}, {Name: names.EBSLimits}, {Name: names.GCEPDLimits}, {Name: names.NodeVolumeLimits}, {Name: names.AzureDiskLimits}, {Name: names.VolumeBinding}, {Name: names.VolumeZone}, {Name: names.PodTopologySpread, Weight: pointer.Int32(2)}, {Name: names.InterPodAffinity, Weight: pointer.Int32(2)}, {Name: names.DefaultPreemption}, {Name: names.NodeResourcesBalancedAllocation, Weight: pointer.Int32(1)}, {Name: names.ImageLocality, Weight: pointer.Int32(1)}, {Name: names.DefaultBinder}, }, }, } ... return plugins } 这里选择 NodeName, ImageLocality, NodeResourcesFit 和 NodeAffinity 插件来分析的插件实现原理. ","date":"2023-04-18","objectID":"/posts/a3f5fa/:9:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"NodeName 插件的实现原理 NodeName 插件是一个预选插件, 插件实现了 Filter 方法, 筛选出跟 pod 相同 nodeName 的 node 节点对象. 源码位置: pkg/scheduler/framework/plugins/nodename/node_name.go type NodeName struct{} var _ framework.FilterPlugin = \u0026NodeName{} const ( Name = names.NodeName ErrReason = \"node(s) didn't match the requested node name\" ) func (pl *NodeName) Name() string { return Name } func (pl *NodeName) Filter(ctx context.Context, _ *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { if nodeInfo.Node() == nil { return framework.NewStatus(framework.Error, \"node not found\") } if !Fits(pod, nodeInfo) { return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReason) } return nil } func Fits(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool { // 过滤节点, 当返回 true, 该节点通过了初选. // 当 pod 没有指定 nodeName 或者 pod 指定的 nodeName 跟传入的 node 名字一致则返回 true. return len(pod.Spec.NodeName) == 0 || pod.Spec.NodeName == nodeInfo.Node().Name } ... ","date":"2023-04-18","objectID":"/posts/a3f5fa/:9:1","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"ImageLocality 插件的实现原理 ImageLocality 插件实现了 Score 分支计算接口, 该插件会计算 pod 关联的容器的镜像在 node 上的状态, 经过各种校验和公式计算后得出一个 score 分值. 至于 ImageLocality 插件如何计算 score 分值代码里写清楚了, 但至于为什么要这么计算就看不明白了. 源码位置: pkg/scheduler/framework/plugins/imagelocality/image_locality.go type ImageLocality struct { handle framework.Handle } var _ framework.ScorePlugin = \u0026ImageLocality{} const Name = names.ImageLocality ... func (pl *ImageLocality) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { // 通过 nodeName 获取 nodeInfo 对象 nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName) if err != nil { return 0, framework.AsStatus(fmt.Errorf(\"getting node %q from Snapshot: %w\", nodeName, err)) } // 获取主机列表 nodeInfos, err := pl.handle.SnapshotSharedLister().NodeInfos().List() if err != nil { return 0, framework.AsStatus(err) } // 当前有多少 node totalNumNodes := len(nodeInfos) // 经过一堆表达式计算出 node 对应的 score 分支 score := calculatePriority(sumImageScores(nodeInfo, pod.Spec.Containers, totalNumNodes), len(pod.Spec.Containers)) return score, nil } // 根据 node 当前镜像的状态计算分值. func calculatePriority(sumScores int64, numContainers int) int64 { maxThreshold := maxContainerThreshold * int64(numContainers) if sumScores \u003c minThreshold { sumScores = minThreshold } else if sumScores \u003e maxThreshold { sumScores = maxThreshold } return int64(framework.MaxNodeScore) * (sumScores - minThreshold) / (maxThreshold - minThreshold) } // 遍历 pod 里的所有容器, 当容器对应的镜像在 node 中, 则累加计算分值 score. func sumImageScores(nodeInfo *framework.NodeInfo, containers []v1.Container, totalNumNodes int) int64 { var sum int64 for _, container := range containers { // 获取容器的景象在 node 的状态 if state, ok := nodeInfo.ImageStates[normalizedImageName(container.Image)]; ok { sum += scaledImageScore(state, totalNumNodes) } } return sum } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:9:2","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"NodeResourcesFit 插件的实现原理 NodeResourcesFit 插件实现了三个 framework 插件接口, 分别是 PreFilterPlugin, FilterPlugin, ScorePlugin 接口. PreFilterPlugin 只是在 state 缓存里记录 pod 要求的 request 资源. FilterPlugin 主要用来过滤掉不符合资源要求的 node 节点, 如果当前 node 可申请的 cpu 和 mem 不满足 pod 需求, 则 node 节点会被过滤掉. ScorePlugin 主要对符合要求通过预选的 node 集合进行打分, 用以得到最优的 node 节点, 其实就是 cpu/mem 资源最充足最空闲的 node 节点. 代码位置: pkg/scheduler/framework/plugins/noderesources/fit.go var _ framework.PreFilterPlugin = \u0026Fit{} var _ framework.FilterPlugin = \u0026Fit{} var _ framework.ScorePlugin = \u0026Fit{} ... // 用来在 cycleState 里记录 pod 所需的 request 资源 func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) { cycleState.Write(preFilterStateKey, computePodResourceRequest(pod)) return nil, nil } // 过滤掉不符合 resource request 资源的 node. func (f *Fit) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { // 获取在 preFilter 阶段写入的 preFilterState s, err := getPreFilterState(cycleState) if err != nil { return framework.AsStatus(err) } // 判断当前的 node 是否满足 pod 的资源请求需求 insufficientResources := fitsRequest(s, nodeInfo, f.ignoredResources, f.ignoredResourceGroups) // 如果不为空则有异常, 把所有的异常合并在一起, 构建 framework status 对象再返回. if len(insufficientResources) != 0 { // We will keep all failure reasons. failureReasons := make([]string, 0, len(insufficientResources)) for i := range insufficientResources { failureReasons = append(failureReasons, insufficientResources[i].Reason) } return framework.NewStatus(framework.Unschedulable, failureReasons...) } // 返回 nil, 说明该节点符合资源要求 return nil } // 判断当前 node 是否满足 pod 的资源要求 func fitsRequest(podRequest *preFilterState, nodeInfo *framework.NodeInfo, ignoredExtendedResources, ignoredResourceGroups sets.String) []InsufficientResource { insufficientResources := make([]InsufficientResource, 0, 4) allowedPodNumber := nodeInfo.Allocatable.AllowedPodNumber // 如果当前 node pods 数超过了最大 pods 数, 则 append. if len(nodeInfo.Pods)+1 \u003e allowedPodNumber { insufficientResources = append(insufficientResources, InsufficientResource{ ResourceName: v1.ResourcePods, Reason: \"Too many pods\", Requested: 1, Used: int64(len(nodeInfo.Pods)), Capacity: int64(allowedPodNumber), }) } // 如果没有 pod 没有 resource request 配置, 可直接返回 if podRequest.MilliCPU == 0 \u0026\u0026 podRequest.Memory == 0 \u0026\u0026 podRequest.EphemeralStorage == 0 \u0026\u0026 len(podRequest.ScalarResources) == 0 { return insufficientResources } // 如果当前 node 空闲 cpu 资源不足以运行 pod, 则 append 异常. // 当前的 cpu 资源是按照 request 来计算的, 而不是 metrics 实时的. if podRequest.MilliCPU \u003e (nodeInfo.Allocatable.MilliCPU - nodeInfo.Requested.MilliCPU) { insufficientResources = append(insufficientResources, InsufficientResource{ ResourceName: v1.ResourceCPU, Reason: \"Insufficient cpu\", Requested: podRequest.MilliCPU, Used: nodeInfo.Requested.MilliCPU, Capacity: nodeInfo.Allocatable.MilliCPU, }) } // 如果当前 node 空闲的内存不满足 pod 的要求, 则 append 异常 if podRequest.Memory \u003e (nodeInfo.Allocatable.Memory - nodeInfo.Requested.Memory) { insufficientResources = append(insufficientResources, InsufficientResource{ ResourceName: v1.ResourceMemory, Reason: \"Insufficient memory\", Requested: podRequest.Memory, Used: nodeInfo.Requested.Memory, Capacity: nodeInfo.Allocatable.Memory, }) } // 如果当前 node 的存储空间不够 pod 的要求, 则返回错误. if podRequest.EphemeralStorage \u003e (nodeInfo.Allocatable.EphemeralStorage - nodeInfo.Requested.EphemeralStorage) { insufficientResources = append(insufficientResources, InsufficientResource{ ResourceName: v1.ResourceEphemeralStorage, Reason: \"Insufficient ephemeral-storage\", Requested: podRequest.EphemeralStorage, Used: nodeInfo.Requested.EphemeralStorage, Capacity: nodeInfo.Allocatable.EphemeralStorage, }) } // 这个是 pod 对扩展资源的要求, 当节点不满足其要求时, append 追加异常. for rName, rQuant := range podRequest.ScalarResources { ... if v1helper.IsExtendedResourceName(rName) { var rNamePrefix string if ignoredResourceGroups.Len() \u003e 0 { rNamePrefix = strings.Split(string(rName), \"/\")[0] } if ignoredExtendedResources.","date":"2023-04-18","objectID":"/posts/a3f5fa/:9:3","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"NodeAffinity 节点亲和性插件原理 NodeAffinity 是实现节点亲和性的插件, 主要实现了 PreFilter, Filter 和 PreScore, Score 四个方法. PreFilter 和 PreScore 在 NodeAffinity 插件里做 state 传递, 这里重点分析 Filter 和 Score 方法实现. Filter 用来判断传入的 node 是否匹配 pod 的 RequiredDuringSchedulingIgnoredDuringExecution 硬亲和配置, 不适配则返回异常. Score 用来给 node 打分, 如果节点 labels 适配 pod 的 preferredDuringSchedulingIgnoredDuringExecution, 则把 spec.nodeAffinity.weight 累加到 score. 首先看下 pod nodeAffinity 节点亲和性的两个参数. RequiredDuringSchedulingIgnoredDuringExecution 参数表示调度器只会调度到符合 pod 要求的节点上, 没有符合要求的节点则不进行调度. preferredDuringSchedulingIgnoredDuringExecution 参数表示调度器会优先尝试寻找满足亲和性规则的节点. 如果实在找不到匹配亲和性的节点, 调度器会选择一个分值高但不满足 pod 亲和性要求的节点. 下面是包含 pod 的亲和性的 pod 配置文件, 可对照该配置来理解 nodeAffinity 插件的 Filter 和 Score 的实现. apiVersion: v1 kind: Pod metadata: name: test-xiaorui-cc spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 preference: matchExpressions: - key: nodenames operator: In values: - xiaorui.cc containers: - name: with-node-affinity image: registry.k8s.io/pause:2.0 源码位置: pkg/scheduler/framework/plugins/nodeaffinity/node_affinity.go type NodeAffinity struct { handle framework.Handle addedNodeSelector *nodeaffinity.NodeSelector addedPrefSchedTerms *nodeaffinity.PreferredSchedulingTerms } var _ framework.FilterPlugin = \u0026NodeAffinity{} var _ framework.ScorePlugin = \u0026NodeAffinity{} ... // 检查传入的 node 是否匹配该 pod 的 nodeAffinity func (pl *NodeAffinity) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { node := nodeInfo.Node() // node 对象为空则跳出 if node == nil { return framework.NewStatus(framework.Error, \"node not found\") } ... // 获取 prefilter 阶段写入的状态, 其实就是 pod 的 required 配置. s, err := getPreFilterState(state) if err != nil { s = \u0026preFilterState{requiredNodeSelectorAndAffinity: nodeaffinity.GetRequiredNodeAffinity(pod)} } // 判断 pod 的 required 跟 node 是否适配 match, _ := s.requiredNodeSelectorAndAffinity.Match(node) if !match { // 如不适配直接退出 return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonPod) } return nil } // 跟 pod 和 node 亲和情况进行打分 score func (pl *NodeAffinity) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { ... node := nodeInfo.Node() var count int64 if pl.addedPrefSchedTerms != nil { count += pl.addedPrefSchedTerms.Score(node) } // 获取 prescore 阶段写入的 state s, err := getPreScoreState(state) if err != nil { // Fallback to calculate preferredNodeAffinity here when PreScore is disabled. preferredNodeAffinity, err := getPodPreferredNodeAffinity(pod) if err != nil { return 0, framework.AsStatus(err) } s = \u0026preScoreState{ preferredNodeAffinity: preferredNodeAffinity, } } // 根据 pod 的 preferred 和 node labels 的适配情况, 增加 weight 到 score 分值. if s.preferredNodeAffinity != nil { count += s.preferredNodeAffinity.Score(node) } return count, nil } func (t *PreferredSchedulingTerms) Score(node *v1.Node) int64 { var score int64 nodeLabels := labels.Set(node.Labels) nodeFields := extractNodeFields(node) for _, term := range t.terms { // 如果 node 匹配 pod preferred, 则增加定义的权重. if ok, _ := term.match(nodeLabels, nodeFields); ok { score += int64(term.weight) } } return score } ","date":"2023-04-18","objectID":"/posts/a3f5fa/:9:4","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["kubernetes"],"content":"总结 kubernetes scheduler 从 informer 监听新资源变动, 当有新 pod 创建时, scheduler 需要为其分配绑定一个 node 节点. 在调度 Pod 时要经过两个阶段, 即 调度周期 和 绑定周期. 调度周期分为预选阶段和优选阶段. 预选 (Predicates) 是通过插件的 Filter 方法过滤出符合要求的 node 节点 优选 (Priorities) 则对预选出来的 node 节点进行打分排序 绑定周期默认只有一个默认插件, 仅给 apiserver 发起 node 跟 pod 绑定的请求. 出处: https://github.com/rfyiamcool/notes#kubernetes ","date":"2023-04-18","objectID":"/posts/a3f5fa/:10:0","tags":["kubernetes","源码","转载"],"title":"源码分析 kubernetes scheduler 核心调度器的实现原理","uri":"/posts/a3f5fa/"},{"categories":["云原生"],"content":"局部开启 Access 日志 export NAMESPACE=default export WORKLOAD=details cat \u003c\u003c EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: enable-accesslog namespace: ${NAMESPACE} spec: configPatches: - applyTo: NETWORK_FILTER match: context: ANY listener: filterChain: filter: name: envoy.filters.network.http_connection_manager patch: operation: MERGE value: typed_config: '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager access_log: - name: envoy.access_loggers.file typed_config: '@type': type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout workloadSelector: labels: app: ${WORKLOAD} EOF ","date":"2023-03-31","objectID":"/posts/ba614f/:1:0","tags":["faq","istio"],"title":"Istio故障排除常用指令","uri":"/posts/ba614f/"},{"categories":["云原生"],"content":"修改 Envoy 日志级别 ","date":"2023-03-31","objectID":"/posts/ba614f/:2:0","tags":["faq","istio"],"title":"Istio故障排除常用指令","uri":"/posts/ba614f/"},{"categories":["云原生"],"content":"修改所有 logger export POD_NAME=xxx kubectl exec -ti -n ${NAMESPACE} ${POD_NAME} -c istio-proxy -- curl -X POST 127.0.0.1:15000/logging\\?level=info active loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info connection: info conn_handler: info decompression: info dns: info dubbo: info envoy_bug: info ext_authz: info rocketmq: info file: info filter: info forward_proxy: info grpc: info hc: info health_checker: info http: info http2: info hystrix: info init: info io: info jwt: info kafka: info key_value_store: info lua: info main: info matcher: info misc: info mongo: info quic: info quic_stream: info pool: info rbac: info redis: info router: info runtime: info stats: info secret: info tap: info testing: info thrift: info tracing: info upstream: info udp: info wasm: info ","date":"2023-03-31","objectID":"/posts/ba614f/:2:1","tags":["faq","istio"],"title":"Istio故障排除常用指令","uri":"/posts/ba614f/"},{"categories":["云原生"],"content":"修改其中一个 logger 级别 kubectl exec -ti -n ${NAMESPACE} ${POD_NAME} -c istio-proxy -- curl -X POST 127.0.0.1:15000/logging\\?http=trace active loggers: ... ... health_checker: warning http: trace http2: warning hystrix: warning ... ... ","date":"2023-03-31","objectID":"/posts/ba614f/:2:2","tags":["faq","istio"],"title":"Istio故障排除常用指令","uri":"/posts/ba614f/"},{"categories":["云原生"],"content":" pod=`kubectl get pod -n istio-system -l app=istiod -o name` kubectl exec $pod -n istio-system -- curl http://127.0.0.1:15014/debug/$@ kubectl exec $pod -n istio-system -- curl http://127.0.0.1:15014/debug/endpointz kubectl exec $pod -n istio-system -- curl http://127.0.0.1:15014/debug/adsz kubectl exec $pod -n istio-system -- curl http://127.0.0.1:15014/debug/registryz kubectl exec $pod -n istio-system -- curl http://127.0.0.1:15014/debug/configz ","date":"2023-03-31","objectID":"/posts/ba614f/:3:0","tags":["faq","istio"],"title":"Istio故障排除常用指令","uri":"/posts/ba614f/"},{"categories":["istio","envoy"],"content":"Envoy 初始化配置文件 kubectl exec -ti productpage-v1-d4f8dfd97-p8xlb -c istio-proxy -- cat /etc/istio/proxy/envoy-rev.json 配置文件结构 ","date":"2023-03-28","objectID":"/posts/18798d/:0:0","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Node 包含Envoy所在节点相关信息 id: sidecar~10.86.238.167~productpage-v1-d4f8dfd97-p8xlb.default~default.svc.cluster.local cluster: productpage.default locality: {} metadata: ANNOTATIONS: kubectl.kubernetes.io/default-container: productpage kubectl.kubernetes.io/default-logs-container: productpage kubernetes.io/config.seen: 2023-03-24T09:42:07.297289187+08:00 kubernetes.io/config.source: api prometheus.io/path: /stats/prometheus prometheus.io/port: \"15020\" prometheus.io/scrape: \"true\" sidecar.istio.io/status: '{\"initContainers\":[\"istio-init\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"workload-socket\",\"credential-socket\",\"workload-certs\",\"istio-envoy\",\"istio-data\",\"istio-podinfo\",\"istio-token\",\"istiod-ca-cert\"],\"imagePullSecrets\":null,\"revision\":\"default\"}' APP_CONTAINERS: productpage CLUSTER_ID: Kubernetes ENVOY_PROMETHEUS_PORT: 15090 ENVOY_STATUS_PORT: 15021 INSTANCE_IPS: 10.86.238.167 INTERCEPTION_MODE: REDIRECT ISTIO_PROXY_SHA: 6e6b45cd824e414453ac8f0c81be540269ddff3e ISTIO_VERSION: 1.17.1 LABELS: app: productpage security.istio.io/tlsMode: istio service.istio.io/canonical-name: productpage service.istio.io/canonical-revision: v1 version: v1 MESH_ID: cluster.local NAME: productpage-v1-d4f8dfd97-p8xlb NAMESPACE: default NODE_NAME: master OWNER: kubernetes://apis/apps/v1/namespaces/default/deployments/productpage-v1 PILOT_SAN: - istiod.istio-system.svc PLATFORM_METADATA: {} POD_PORTS: '[{\"containerPort\":9080,\"protocol\":\"TCP\"}]' PROXY_CONFIG: binaryPath: /usr/local/bin/envoy concurrency: 2 configPath: ./etc/istio/proxy controlPlaneAuthPolicy: MUTUAL_TLS discoveryAddress: istiod.istio-system.svc:15012 drainDuration: 45s proxyAdminPort: 15000 serviceCluster: istio-proxy statNameLength: 189 statusPort: 15020 terminationDrainDuration: 5s tracing: zipkin: address: zipkin.istio-system:9411 SERVICE_ACCOUNT: bookinfo-productpage WORKLOAD_NAME: productpage-v1 ","date":"2023-03-28","objectID":"/posts/18798d/:0:1","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Admin 配置Envoy的日志路径以及管理端口 access_log: - name: envoy.access_loggers.file typed_config: \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/null profile_path: /var/lib/istio/data/envoy.prof address: socket_address: address: 127.0.0.1 port_value: 15000 ","date":"2023-03-28","objectID":"/posts/18798d/:0:2","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Dynamic_resources 配置动态资源,这里配置了 LDS、CDS 和 ADS 服务配置 lds_config: ads: {} initial_fetch_timeout: 0s resource_api_version: V3 cds_config: ads: {} initial_fetch_timeout: 0s resource_api_version: V3 ads_config: api_type: GRPC set_node_on_first_message_only: true transport_api_version: V3 grpc_services: - envoy_grpc: cluster_name: xds-grpc ","date":"2023-03-28","objectID":"/posts/18798d/:0:3","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Static_resources 配置静态资源，包括了prometheus_stats、agent、sds-grpc、xds-grpc 和 zipkin 五个cluster, 15090和15021两个listener。 prometheus_stats cluster和15090 listener用于对外提供兼容prometheus格式的统计指标。 agent sds-grpc cluster 用于证书发现服务(SecretDiscoveryService) xds-grpc cluster对应前面dynamic_resources中ADS配置，指明了Envoy用于获取动态资源的服务器地址 zipkin cluster则是外部的zipkin调用跟踪服务器地址，Envoy会向该地址上报兼容zipkin格式的调用跟踪信息。 clusters: - name: prometheus_stats type: STATIC connect_timeout: 0.250s lb_policy: ROUND_ROBIN load_assignment: cluster_name: prometheus_stats endpoints: - lb_endpoints: - endpoint: address: socket_address: protocol: TCP address: 127.0.0.1 port_value: 15000 - name: agent type: STATIC connect_timeout: 0.250s lb_policy: ROUND_ROBIN load_assignment: cluster_name: agent endpoints: - lb_endpoints: - endpoint: address: socket_address: protocol: TCP address: 127.0.0.1 port_value: 15020 - name: sds-grpc type: STATIC typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} connect_timeout: 1s lb_policy: ROUND_ROBIN load_assignment: cluster_name: sds-grpc endpoints: - lb_endpoints: - endpoint: address: pipe: path: ./var/run/secrets/workload-spiffe-uds/socket - name: xds-grpc type: STATIC connect_timeout: 1s lb_policy: ROUND_ROBIN load_assignment: cluster_name: xds-grpc endpoints: - lb_endpoints: - endpoint: address: pipe: path: ./etc/istio/proxy/XDS circuit_breakers: thresholds: - priority: DEFAULT max_connections: 100000 max_pending_requests: 100000 max_requests: 100000 - priority: HIGH max_connections: 100000 max_pending_requests: 100000 max_requests: 100000 upstream_connection_options: tcp_keepalive: keepalive_time: 300 max_requests_per_connection: 1 typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} - name: zipkin type: STRICT_DNS respect_dns_ttl: true dns_lookup_family: V4_ONLY dns_refresh_rate: 30s connect_timeout: 1s lb_policy: ROUND_ROBIN load_assignment: cluster_name: zipkin endpoints: - lb_endpoints: - endpoint: address: socket_address: address: zipkin.istio-system port_value: 9411 listeners: - address: socket_address: protocol: TCP address: 0.0.0.0 port_value: 15090 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: stats route_config: virtual_hosts: - name: backend domains: - \"*\" routes: - match: prefix: /stats/prometheus route: cluster: prometheus_stats http_filters: - name: envoy.filters.http.router typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router - address: socket_address: protocol: TCP address: 0.0.0.0 port_value: 15021 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: agent route_config: virtual_hosts: - name: backend domains: - \"*\" routes: - match: prefix: /healthz/ready route: cluster: agent http_filters: - name: envoy.filters.http.router typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router ","date":"2023-03-28","objectID":"/posts/18798d/:0:4","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Tracing 配置分布式链路跟踪，这里配置的后端cluster是前面static_resources里面定义的zipkin cluster。 http: name: envoy.tracers.zipkin typed_config: \"@type\": type.googleapis.com/envoy.config.trace.v3.ZipkinConfig collector_cluster: zipkin collector_endpoint: /api/v2/spans collector_endpoint_version: HTTP_JSON trace_id_128bit: true shared_span_context: false Envoy 配置分析 从Envoy初始化配置文件中，我们可以大致看到Istio通过Envoy来实现服务发现和流量管理的基本原理。即控制面将xDS server信息通过static resource的方式配置到Envoy的初始化配置文件中，Envoy启动后通过xDS server获取到dynamic resource，包括网格中的service信息及路由规则。 Envoy 配置初始化流程: Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot。 Pilot-agent使用envoy-rev.json启动Envoy进程。 Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等d动态配置信息。 Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理 Envoy中实际生效的配置是由初始化配置文件中的静态配置和从Pilot获取的动态配置一起组成的。接下来通过Envoy的管理接口来获取Envoy的完整配置。 # kubectl exec -ti productpage-v1-d4f8dfd97-p8xlb -c istio-proxy -- curl localhost:15000/config_dump\u003e envoy-productpage-v1-d4f8dfd97-p8xlb.json root@master:~# wc -l envoy-productpage-v1-d4f8dfd97-p8xlb.json 11870 envoy-productpage-v1-d4f8dfd97-p8xlb.json 配置文件内容太长上传到github envoy-productpage-v1-d4f8dfd97-p8xlb.yaml 配置结构 ","date":"2023-03-28","objectID":"/posts/18798d/:0:5","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Bootstrap 这部分是Envoy的初始化配置，在上面已经介绍就不在赘述。 ","date":"2023-03-28","objectID":"/posts/18798d/:1:0","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Clusters 在Envoy中，Cluster是一个服务集群，Cluster中包含一个到多个endpoint，每个endpoint都可以提供服务，Envoy根据负载均衡算法将请求发送到这些endpoint中。 在Productpage的clusters配置中包含static_clusters和dynamic_active_clusters两部分，其中static_clusters是来自于envoy-rev.json的初始化配置中的信息。dynamic_active_clusters是通过xDS接口从Istio控制面获取的动态服务信息。 ","date":"2023-03-28","objectID":"/posts/18798d/:2:0","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Dynamic_active_clusters Dynamic Cluster中有以下几类Cluster Outbound Cluster Inbound Cluster BlackHoleCluster PassthroughCluster Outbound Cluster 这部分的Cluster占了绝大多数，该类Cluster对应于Envoy所在节点的外部服务。以reviews为例，对于Productpage来说,reviews是一个外部服务，因此其Cluster名称中包含outbound字样 # ./istioctl pc c -f envoy-productpage-v1-d4f8dfd97-p8xlb.json --direction outbound SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE details.default.svc.cluster.local 9080 - outbound EDS details.default details.default.svc.cluster.local 9080 v1 outbound EDS details.default details.default.svc.cluster.local 9080 v2 outbound EDS details.default istio-egressgateway.istio-system.svc.cluster.local 80 - outbound EDS istio-egressgateway.istio-system.svc.cluster.local 443 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 80 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 443 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 15021 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 15443 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 31400 - outbound EDS istiod.istio-system.svc.cluster.local 443 - outbound EDS istiod.istio-system.svc.cluster.local 15010 - outbound EDS istiod.istio-system.svc.cluster.local 15012 - outbound EDS istiod.istio-system.svc.cluster.local 15014 - outbound EDS kubernetes.default.svc.cluster.local 443 - outbound EDS productpage.default.svc.cluster.local 9080 - outbound EDS productpage.default productpage.default.svc.cluster.local 9080 v1 outbound EDS productpage.default ratings.default.svc.cluster.local 9080 - outbound EDS ratings.default ratings.default.svc.cluster.local 9080 v1 outbound EDS ratings.default ratings.default.svc.cluster.local 9080 v2 outbound EDS ratings.default ratings.default.svc.cluster.local 9080 v2-mysql outbound EDS ratings.default ratings.default.svc.cluster.local 9080 v2-mysql-vm outbound EDS ratings.default reviews.default.svc.cluster.local 9080 - outbound EDS 从reviews 服务对应的cluster配置中可以看到，其类型为EDS，即表示该Cluster的endpoint来自于动态发现，动态发现中eds_config则指向了ads，最终指向static Resource中配置的xds-grpc cluster,即Pilot的地址。 ./istioctl pc c productpage-v1-d4f8dfd97-p8xlb.default --fqdn reviews.default.svc.cluster.local -o yaml - circuitBreakers: thresholds: - maxConnections: 4294967295 maxPendingRequests: 4294967295 maxRequests: 4294967295 maxRetries: 4294967295 trackRemaining: true commonLbConfig: localityWeightedLbConfig: {} connectTimeout: 10s edsClusterConfig: edsConfig: ads: {} initialFetchTimeout: 0s resourceApiVersion: V3 serviceName: outbound|9080||reviews.default.svc.cluster.local filters: - name: istio.metadata_exchange typedConfig: '@type': type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange protocol: istio-peer-exchange lbPolicy: LEAST_REQUEST metadata: filterMetadata: istio: default_original_port: 9080 services: - host: reviews.default.svc.cluster.local name: reviews namespace: default name: outbound|9080||reviews.default.svc.cluster.local transportSocketMatches: - match: tlsMode: istio name: tlsMode-istio transportSocket: name: envoy.transport_sockets.tls typedConfig: '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext commonTlsContext: alpnProtocols: - istio-peer-exchange - istio combinedValidationContext: defaultValidationContext: matchSubjectAltNames: - exact: spiffe://cluster.local/ns/default/sa/bookinfo-reviews validationContextSdsSecretConfig: name: ROOTCA sdsConfig: apiConfigSource: apiType: GRPC grpcServices: - envoyGrpc: clusterName: sds-grpc setNodeOnFirstMessageOnly: true transportApiVersion: V3 initialFetchTimeout: 0s resourceApiVersion: V3 tlsCertificateSdsSecretConfigs: - name: default sdsConfig: apiConfigSource: apiType: GRPC grpcServices: - envoyGrpc: clusterName: sds-grpc setNodeOnFirstMessageOnly: true transportApiVersion: V3 initialFetchTimeout: 0s resourceApiVersion: V3 tlsParams: tlsMaximumProtocolVersion: TLSv1_3 tlsMinimumProtocolVersion: TLSv1_2 sni: outbound_.9080_._.reviews.defaul","date":"2023-03-28","objectID":"/posts/18798d/:2:1","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Listeners Envoy采用listener来接收并处理downstream发过来的请求，listener采用了插件式的架构，可以通过配置不同的filter在Listener中插入不同的处理逻辑。 Listener可以绑定到IP Socket或者Unix Domain Socket上，以接收来自客户端的请求;也可以不绑定，而是接收从其他listener转发来的数据。Istio利用了Envoy listener的这一特点，通过VirtualOutboundListener在一个端口接收所有出向请求，然后再按照请求的端口分别转发给不同的listener分别处理。 ","date":"2023-03-28","objectID":"/posts/18798d/:2:2","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Routers 配置Envoy的路由规则。Istio下发的缺省路由规则中对每个端口设置了一个路由规则，根据host来对请求进行路由分发 ","date":"2023-03-28","objectID":"/posts/18798d/:2:3","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"Secrets ","date":"2023-03-28","objectID":"/posts/18798d/:2:4","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio","envoy"],"content":"参考资料 Envoy中文文档 Istio流量管理实现机制深度解析 Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 ","date":"2023-03-28","objectID":"/posts/18798d/:3:0","tags":["istio","envoy"],"title":"深入Istio系列 - Envoy配置分析","uri":"/posts/18798d/"},{"categories":["istio"],"content":"开启 Access 日志 apiVersion: v1 kind: ConfigMap metadata: name: istio namespace: istio-system data: mesh: | accessLogEncoding: JSON accessLogFile: /dev/stdout accessLogFormat: \"\" ","date":"2023-03-17","objectID":"/posts/5a662f/:0:1","tags":["envoyfilter","access"],"title":"Istio Proxy Access日志","uri":"/posts/5a662f/"},{"categories":["istio"],"content":"格式化 Access 日志 { \"authority\": \"%REQ(:AUTHORITY)%\", \"bytes_received\": \"%BYTES_RECEIVED%\", \"bytes_sent\": \"%BYTES_SENT%\", \"downstream_local_address\": \"%DOWNSTREAM_LOCAL_ADDRESS%\", \"downstream_remote_address\": \"%DOWNSTREAM_REMOTE_ADDRESS%\", \"duration\": \"%DURATION%\", \"istio_policy_status\": \"%DYNAMIC_METADATA(istio.mixer:status)%\", \"method\": \"%REQ(:METHOD)%\", \"path\": \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\", \"protocol\": \"%PROTOCOL%\", \"request_id\": \"%REQ(X-REQUEST-ID)%\", \"requested_server_name\": \"%REQUESTED_SERVER_NAME%\", \"response_code\": \"%RESPONSE_CODE%\", \"response_flags\": \"%RESPONSE_FLAGS%\", \"route_name\": \"%ROUTE_NAME%\", \"start_time\": \"%START_TIME%\", \"upstream_cluster\": \"%UPSTREAM_CLUSTER%\", \"upstream_host\": \"%UPSTREAM_HOST%\", \"upstream_local_address\": \"%UPSTREAM_LOCAL_ADDRESS%\", \"upstream_service_time\": \"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\", \"upstream_transport_failure_reason\": \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\", \"user_agent\": \"%REQ(USER-AGENT)%\", \"x_forwarded_for\": \"%REQ(X-FORWARDED-FOR)%\" } ","date":"2023-03-17","objectID":"/posts/5a662f/:0:2","tags":["envoyfilter","access"],"title":"Istio Proxy Access日志","uri":"/posts/5a662f/"},{"categories":["istio"],"content":"自定义日志文件路径 accessLogFile: /var/log/envoy/envoy.log ","date":"2023-03-17","objectID":"/posts/5a662f/:0:3","tags":["envoyfilter","access"],"title":"Istio Proxy Access日志","uri":"/posts/5a662f/"},{"categories":["istio"],"content":"局部启用 修改 istio-sidecar-injector sidecar 模板,挂载一个临时卷 # kubectl edit cm -n istio-system istio-sidecar-injector ... ... ... #大概在392行 volumeMounts: - name: access-log mountPath: /var/log/istio ... ... ... #大概在447行 volumes: - emptyDir: name: access-log cat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: enable-accesslog spec: workloadSelector: labels: app: details configPatches: - applyTo: NETWORK_FILTER match: context: ANY listener: filterChain: filter: name: envoy.filters.network.http_connection_manager patch: operation: MERGE value: typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\" access_log: - name: envoy.access_loggers.file typed_config: \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: \"/var/log/istio/access.log\" ## 注意此处文件目录是刚刚调整的临时卷目录 log_format: json_format: authority: \"%REQ(:AUTHORITY)%\" bytes_received: \"%BYTES_RECEIVED%\" bytes_sent: \"%BYTES_SENT%\" downstream_local_address: \"%DOWNSTREAM_LOCAL_ADDRESS%\" downstream_remote_address: \"%DOWNSTREAM_REMOTE_ADDRESS%\" duration: \"%DURATION%\" method: \"%REQ(:METHOD)%\" path: \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\" protocol: \"%PROTOCOL%\" request_id: \"%REQ(X-REQUEST-ID)%\" requested_server_name: \"%REQUESTED_SERVER_NAME%\" response_code: \"%RESPONSE_CODE%\" response_flags: \"%RESPONSE_FLAGS%\" route_name: \"%ROUTE_NAME%\" start_time: \"%START_TIME%\" upstream_cluster: \"%UPSTREAM_CLUSTER%\" upstream_host: \"%UPSTREAM_HOST%\" upstream_local_address: \"%UPSTREAM_LOCAL_ADDRESS%\" upstream_service_time: \"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\" upstream_transport_failure_reason: \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\" user_agent: \"%REQ(USER-AGENT)%\" x_forwarded_for: \"%REQ(X-FORWARDED-FOR)%\" EOF 使用logrotate来滚动日志仍存在问题 ","date":"2023-03-17","objectID":"/posts/5a662f/:0:4","tags":["envoyfilter","access"],"title":"Istio Proxy Access日志","uri":"/posts/5a662f/"},{"categories":["istio"],"content":"日志滚动 FROM istio/proxyv2:1.13.4 RUN apt update \u0026\u0026 apt install -y logrotate RUN install -d -m 0755 -o istio-proxy -g istio-proxy /var/log/istio # logrotate_envoy.conf is logrotate config file COPY logrotate_envoy.conf /etc/logrotate.d/envoy /var/log/istio/access.log { missingok hourly compress notifempty nocreate rotate 5 size=100M sharedscripts copytruncate } template: metadata: annotations: sidecar.istio.io/proxyImage: registry.cn-hangzhou.aliyuncs.com/seam/istio-proxyv2:1.13.4-logrotate ","date":"2023-03-17","objectID":"/posts/5a662f/:0:5","tags":["envoyfilter","access"],"title":"Istio Proxy Access日志","uri":"/posts/5a662f/"},{"categories":["istio"],"content":"参考资料 istio 官方文档给出的常见变量 ","date":"2023-03-17","objectID":"/posts/5a662f/:1:0","tags":["envoyfilter","access"],"title":"Istio Proxy Access日志","uri":"/posts/5a662f/"},{"categories":["Kubernetes"],"content":"前言 这篇内容篇幅比较长，如果不想深入探讨或时间有限，这是全文简述： 在默认设置下，扩展 Kubernetes 集群中的 pod 和节点可能需要几分钟时间。 了解如何调整集群节点的大小、配置水平和集群自动缩放器以及过度配置集群以加快扩展速度。 ","date":"2023-03-16","objectID":"/posts/a18401/:1:0","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"目录 当自动伸缩的 Pod 报错 Kubernetes 的 Cluster Autoscaler 是如何工作的 探索 Pod 自动伸缩提前期 为 Kubernetes 节点选择最佳实例大小 在 Kubernetes 集群中过度配置节点 为 Pod 选择正确的内存和CPU资源 关于集群的缩容 为什么不基于内存或CPU进行自动伸缩 在 Kubernetes 中, 自动伸缩功能包括: Pod水平自动伸缩（Horizontal Pod Autoscaler，HPA） Pod垂直自动伸缩（Vertical Pod Autoscaler，VPA） 集群自动伸缩（Cluster Autoscaler，CA） 这些自动伸缩组件属于不同的类别，关注点也不同。 Horizontal Pod Autoscaler 负责增加 Pod 的副本数量。随着你的应用接收到的流量越来越多，你可以让自动伸缩组件调整副本数量来处理更多的请求。 Vertical Pod Autoscaler 的使用场景是，当资源不足无法创建更多的 Pod 副本时，而又仍然需要处理更多的流量。 一个简单的例子，你无法通过简单地添加更多的 Pod 副本来扩容数据库。数据库可能需要进行数据分片或者配置只读节点。 但你可以通过增加内存和CPU资源来让数据库能够处理更多的连接数。 这正是 VPA 的目的，增加 Pod 的资源大小。 最后，我们要说说集群自动伸缩组件了。 当你的集群资源不足时，Cluster Autoscaler 会配置一个新的计算单元并将其添加到集群中。如果空节点过多，会移除它们以降低成本。 虽然这三个组件都 “自动伸缩” 了一些东西，但它们并不造成相互之间的干扰。它们各自都有自己使用场景，定义和工作机制。并且它们是在独立的项目中开发的，独立的使用。 然而，更重要的是，为了最好的 scaling 你的集群，你必须花些心思去设置好这些 Autoscaler，让我们看个例子。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:0","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"当自动伸缩的 Pod 报错 想象一下，有一个应用程序始终需要并使用 1.5GB 内存和 0.25 个 vCPU。 你配置了一个具有 8GB 和 2 个 vCPU 的单个节点的集群 —— 它应该能够完美地容纳四个 pod（并且还有一点额外的空间）。 :size=700 现在，你部署了一个 Pod 并且配置如下： HPA 配置每 10 个请求进来就添加一个 Pod 副本（例如：如果有 40 个并发请求涌入，会扩容到 4 个 Pod 副本）。 CA 配置在资源不足时，创建更多的 Node 节点。 HPA 可以通过在 deployment 文件中使用 Custom Metrics（例如在 Ingress Controller 中的 queries per second（QPS）） 来扩容 Pod 副本数量。 现在，你开始为集群增加 30 个并发请求，并观察一下情况： HPA 开始扩容 Pod。 创建了两个 Pod 副本。 CA 没有触发 - 没有新增集群 Node 节点。 这很好理解，因为现在有足够的内存和 CPU 资源来支持更多的 Pod。 :size=700 你进一步将流量增加到 40 个并发请求，并再次观察： HPA 又创建了一个 Pod。 这个 Pod 是 pending 状态并且无法被部署。 CA 触发创建了一个新的 Node 节点。 新 Node 节点启动 4 分钟后开始工作。之后，pending Pod 也成功被部署了。 :size=700 :size=700 为什么第四个 Pod 没有部署在第一个 Node 节点上呢？ Pod 部署在集群上需要消耗内存，CPU，硬盘空间等资源，在同一个 Node 上，操作系统和 kubelet 组件也需要消耗内存和 CPU 资源。 Kubernetes 中一个 Worker Node 节点的内存和 CPU 等资源使用分布如下： 需要运行操作系统和一些系统级的守护进程，例如 SSH，Systemd 等。 需要运行 Kubernetes Agent 组件，例如 Kubelet，Container Runtime，Node Problem Detector 等。 需要运行 Pod。 需要保留一些资源用来驱逐阀值 之用。 :size=700 你猜的没错，所有这些配额都是可定制的，但你需要好好计算一下。 在一个 8GB 内存和 2vCPU 的单个节点的，可以按如下估算： 操作系统运行大概需要 100MB 内存和 0.1vCPU。 kubelet 运行大概需要 1.8GB 内存和 0.07vCPU。 驱逐阀值大概需要 100MB 内存。 剩余的大约 6GB 内存空间和 1.83vCPU 是提供给 Pod 使用的。 如果你的集群需要运行 DaemonSet 资源，像 kube-proxy，那么你应该进一步减少提供给 Pod 的资源。考虑到 kube-proxy 大概需要 128MB 内存和 0.1vCPU，那么剩余大约 5.9GB 内存空间和 1.73vCPU 是提供给 Pod 使用的。 另外，如果还需要运行 CNI 组件（例如：Flannel）和日志收集组件（Flentd），又会进一步减少提供给 Pod 的资源。 在统计完所有其他的资源占用情况后，集群的剩余空间就只够运行三个 Pod 了。 :size=700 所以第四个会一直保持 “pending” 状态，直到它被调度到其他的 Node 节点上。 既然 Cluster Autoscaler 知道没有空间容纳第四个 Pod，为什么不提前配置一个新节点？ 为什么它要在 Pod 处于 “pending” 状态之后再触发创建新 Node 节点的操作？ ","date":"2023-03-16","objectID":"/posts/a18401/:2:1","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"Kubernetes 的 Cluster Autoscaler 是如何工作的 Cluster Autoscaler 不是通过观察内存或 CPU 的使用情况来触发自动伸缩的。相反地，是通过对事件的响应和每 10s 对不可调度的 Pod 进行检查。 当 Scheduler 无法找到可以容纳它的 Node 节点时，Pod 就会变成不可调度状态。例如，当一个 Pod 需要 1vCPU 资源而集群只有 0.5vCPU 资源可用，Scheduler 就会把该 Pod 标记为不可调度状态。 这时，Cluster Autoscaler 会开始创建新 Node 节点。创建完成后，它会扫描集群中的不可调度状态的 Pod，检查是否可以将这些 Pod 调度到新节点上。 如果你的集群具有多种节点类型（通常也称为节点组或节点池），则 Cluster Autoscaler 将使用以下策略选择其中一种： Random - 随机选择一种节点类型（默认策略）。 Most Pods - 选择将调度最多 Pod 的节点组。 Least waste - 选择扩容后空闲 CPU 最少的节点组。 Price - 选择成本最低的节点组（目前仅适用于 GCP）。 Priority - 选择优先级最高的节点组（优先级可以手动设置）。 一旦确定了节点类型，Cluster Autoscaler 将调用相关 API 来提供新的计算资源。 如果你使用的是 AWS，Cluster Autoscaler 将预置一个新的 EC2 实例。在 Azure 上，它将创建一个新的虚拟机，并在 GCP 上创建一个新的计算引擎。 创建的节点可能需要一些时间才能出现在 Kubernetes 中。计算资源准备就绪后，节点将被初始化并添加到可以部署未被调度 Pod 的集群中。 不幸的是，配置一个新节点通常会很慢。它可能会花费好几分钟来做这件事。 让我们来看看这几分钟到底干了什么。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:2","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"探索 Pod 自动伸缩前置期 在新节点上创建新 Pod 所需的时间由四个主要因素决定： HPA 的反应时间。 CA 的反应时间。 Node 节点的反应时间。 Pod 创建的时间。 默认地，kubelet 每 10 秒抓取一次 Pod 的 CPU 和内存使用情况。每分钟，Metrics Server 都会聚合这些指标并将它们发送给 Kubernetes API 的其他组件。 Horizontal Pod Autoscaler 控制器负责检查指标并决定扩大或缩小副本数量。 默认地，Horizontal Pod Autoscaler 每 15 秒检查一次 Pod 指标。 Cluster Autoscaler 每 10 秒检查一次集群中不可调度的 Pod。 一旦 CA 检测到不可调度的 Pod，它就会运行一个算法来做决策： 需要多少个节点来将所有的不可调度 Pod 部署完成。 需要创建那种类型的节点组。 整个过程的时间花费应该是： 在少于 100 个节点且每个节点最多 30 个 Pod 的集群上不超过 30 秒。 平均延迟应该是大约 5 秒。 在具有 100 到 1000 个节点的集群上不超过 60 秒。 平均延迟应约为 15 秒。 :size=700 然后是节点配置时间，这主要取决于云提供商。在 3-5 分钟内供应新的计算资源是非常标准的。 :size=700 最后，Pod 必须由容器运行时创建。启动一个容器应该不会超过几毫秒，但下载容器镜像可能需要几秒钟。 如果没有缓存容器映像，则从容器注册表下载映像可能需要几秒钟到一分钟的时间，具体取决于层的大小和数量。 :size=700 因此，当集群中没有空间而触发自动伸缩的时间消耗如下： Horizontal Pod Autoscaler 可能需要长达 1min30s 来增加副本数量。 对于少于 100 个节点的集群，Cluster Autoscaler 应该花费不到 30s 的时间，对于超过 100 个节点的集群，应该不到 1min。 云提供商可能需要 3-5min 来创建计算机资源。 容器运行时可能需要长达 30s 才能下载容器映像。 如果你的集群规模不是很大，在最坏的情况下，时间消耗： :size=700 对于超过 100 个节点的集群，总延迟可能高达 7 分钟。在有更多 Pod 来处理突然激增的流量之前，您是否愿意等待这 7 分钟？ 这里提供了几种减少 scaling 时间的方法： 调整 Horizontal Pod Autoscaler 的刷新时间（由 –horizontal-pod-autoscaler-sync-period 参数控制，默认 15s）。 调整抓取 Pod 的 CPU 和内存使用情况的间隔频率（由 metric-resolution 变量控制，默认 60s）。 调整 Cluster Autoscaler 扫描未被调度 Pod 的间隔频率（由 scan-interval 变量控制，默认10s）。 调整 Node 节点上缓存容器镜像的方式（通过诸如 kube-fledged 等工具）。 但即使将这些设置调整为很小的值，你仍然会收到云提供商创建计算资源的时间限制。有什么方式优化这个部分吗？ 这里可以做两件事： 尽可能地避免创建新地 Node 节点。 主动提前创建节点，以便在需要时能直接使用。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:3","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"为 Kubernetes 节点选择最佳实例大小 选择正确的节点实例类型对集群的扩展策略有很大的影响。 考虑一个这样的场景。 你有一个应用需要 1GB 的内存资源和 0.1 vCPU 资源。 你提供的 Node 节点有 4GB 的内存资源和 1 vCPU 资源。 在为操作系统、kubelet 和驱逐阀值保留内存和 CPU 后，将拥有约 2.5GB 的内存资源和 0.7 vCPU 可用于运行 Pod。 所以你的 Node 节点只能承载 2 个 Pod 的运行。 :size=700 每次扩展 Pod 副本时，都可能会产生最多 7 分钟的延迟（触发 HPA，CA 和云提供商配置计算资源的前置时间）。 让我们来看看如果改成提供 64GB 的内存和 16 vCPU 的节点会发生什么。 在为操作系统、kubelet 和驱逐阀值保留内存和 CPU 后，将拥有约 58.32GB 的内存资源和 15.8 vCPU 可用于运行 Pod。 Node 节点可以承载 58 个 Pod 的运行，只有超过 58 个 Pod 副本时，才需要一个新的节点。 :size=700 此外，每次向集群中添加节点时，都可以部署多个 Pod。再次触发 Cluster Autoscaler 的机会更少。 选择大型节点实例类型还有另一个好处。 为 kubelet 预留的资源、操作系统和驱逐阀值与运行 Pod 的可用资源之间的比率更大。 看看这张图，它描绘了 Pod 可用的内存。 :size=700 随着 Node 实例大小的增加，你可以注意到（按比例）可用于 Pod 的资源增加。换句话说，与拥有两个大小一半的实例相比，可以更高效地利用资源。 那应该一直选择最大的实例吗？ 节点上可以拥有的 Pod 数量决定了效率的峰值。 一些云提供商将 Pod 的数量限制为 110 个（比如 GKE）。其他一些限制是由底层网络基于每个实例（即AWS）规定的。 你可以在这里查看大多数云提供商的限制 所以选择更大的实例类型并不总是一个好的选择。 我们还需要考虑： 爆炸半径 - 如果你只有几个节点，那么一个失败节点的影响比你有很多节点的影响更大。 自动伸缩的成本更高，因为下一个增量是（非常）大的节点。 假设你为集群选择了正确的实例类型，你在配置新计算单元时可能仍然会遇到延迟。 如果不是在需要扩展时创建新节点，而是提前创建相同的节点会怎么样？ ","date":"2023-03-16","objectID":"/posts/a18401/:2:4","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"在 Kubernetes 集群中过度配置节点 如果你可以负担得起随时可用的备用节点的话，你可以： 提前创建一个空的 Node 节点。 一旦空的 Node 节点上有 Pod 了，就会创建另一个空的 Node 节点。 换句话说，让 Cluster Autoscaler 总是保持有一个备用的空 Node 节点。 这是一种权衡：你会产生额外的成本，但扩展新节点的速度会提高。 但有坏消息和好消息。 坏消息是 Cluster Autoscaler 没有内置此功能。它不能被显式的配置，并且也没有提供相应的参数。 好消息是你仍然可以通过一些 trick 的方式来达到这个目的。 你可以运行具有足够请求的 Deployment 来保留一个完整的 Node 节点。你可以将这些 Pod 视为占位符 - 它旨在保留空间，而不是使用资源。 一旦创建了真正的 Pod，就可以驱逐占位符并部署真正的 Pod。 请注意，这一次你仍然需要等待 5 分钟才能将节点添加到集群中，但你可以继续使用当前节点。同时，在后台又提供了一个新的节点。 如何做到这一点呢？ 可以使用运行永久休眠的 pod 的部署来配置过度配置。 :size=700 上图中，你需要特别关注内存和 CPU 配置。Scheduler 会使用这些值来决定部署 Pod 的位置。在这种特殊情况下，它们用于保留空间。 你可以配置一个大型 Pod，该 Pod 的请求大致与可用节点资源相匹配。同时要确保你考虑了 kubelet、操作系统、kube-proxy 等消耗的资源。 如果你的节点实例是 2 vCPU 和 8GB 内存，并且 pod 的可用空间是 1.73 vCPU 和 ~5.9GB 内存，则该节点就无法承载这个 Pod，因为实际的 Pod 可用资源是要小于所需资源的。 :size=700 为了确保在创建真正的 Pod 时能快速的驱逐占位Pod，可以使用优先级和抢占。 Pod Priority 表示一个 Pod 相对于其他 Pod 的重要性。 当一个 Pod 无法被调度时，Scheduler 会尝试抢占（驱逐）较低优先级的 Pod 以调度 “pending” 的 Pod。 可以使用 PodPriorityClass 在集群中配置 Pod 优先级： :size=700 由于 Pod 的默认优先级为 0，而过度配置的 PriorityClass 值为 -1，因此当集群空间不足时，这些 Pod 将首先被逐出。 PriorityClass 还有两个可选字段：globalDefault 和 description。 description 字段是提供给人阅读的关于 PriorityClass 的描述信息。 globalDefault 字段表示这个 PriorityClass 的值应该用于没有 priorityClassName 的 Pod。系统中只能存在一个 global Default 设置为 true 的 PriorityClass。 你可以使用下面的命令为你的 Pod 指定优先级： :size=700 设置完成！ 当集群中没有足够的资源时，Pause Pod 会被抢占，并由新的 Pod 取而代之。 由于 Pause pod 变得不可调度，它会强制 Cluster Autoscaler 向集群添加更多节点。 现在，你已准备好过度配置集群，该是时候考虑优化应用程序以进行扩展了。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:5","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"为 Pod 选择正确的内存和CPU资源 Cluster Autoscaler 会根据 pending Pod 的出现来做出 scaling 决策。 Kubernetes Scheduler 根据 Node 节点的内存和 CPU 负载情况决定将 Pod 分配（或不分配）给节点。 因此，必须为你的工作负载设置正确的资源使用请求，否则您可能会过晚（或过早）触发自动伸缩机制。 让我们看一个例子。 您决定要测试一个应用程序，并发现： 在平均负载下，应用程序消耗 512MB 内存和 0.25 vCPU。 在高峰期，应用程序应最多消耗 4GB 内存和 1 vCPU。 :size=700 你的容器的限制应该是 4GB 内存和 1 个 vCPU。但是，请求呢？ Scheduler 在创建 Pod 之前使用 Pod 的内存和 CPU 请求来选择最佳节点。 所以你可以： 将请求设置为低于实际平均使用量。 保守一点，分配更接近限制的请求。 设置请求以匹配实际的限制。 :size=700 :size=700 :size=700 定义低于实际使用的请求是有问题的，因为你的节点经常会被过度使用。 例如，你可以分配 256MB 的内存作为内存请求。Scheduler 可以为每个节点安装两倍的 Pod。然而，Pod 在实践中使用两倍的内存并开始竞争资源 (CPU) 并被驱逐（节点上没有足够的内存）。 :size=700 过度使用节点会导致过多的驱逐、更多的 kubelet 工作和大量的重新调度。 如果将请求设置为与限制相同的值会发生什么？ 在 Kubernetes 中，这通常被称为 Guaranteed Quality of Service 类，指的是 pod 不太可能被终止和驱逐。Scheduler 将为分配的节点上的 Pod 保留整个 CPU 和内存。该类 Pod 运行稳定，但同时该节点的使用效率就会比较低。 如果你的应用平均使用 512MB 的内存，但为它预留了 4GB，那么大部分时间有 3.5GB 未使用。 :size=700 这值得么？ 如果你想要更多的稳定性，是值得的。 如果你想要效率，你可能希望降低请求并在这些请求与限制之间找到平衡。 这通常被称为 Burstable Quality of Service 类，指的是 Pod 消耗稳定但偶尔会突然使用更多内存和 CPU。 当你的请求与应用的实际使用相匹配时，Scheduler 将高效地将你的 Pod 打包到你的节点中。 有时，应用程序可能需要更多内存或 CPU。 如果 Node 中有资源，应用程序将会在达到最低消耗之前使用它们。 如果 Node 中资源不足，Pod 将竞争资源（CPU），kubelet 可能会尝试驱逐 Pod（内存）。 此时，你应该使用 Guaranteed Quality of Service 还是 Burstable Quality of Service？ 这取决于如下两点： 当你希望最小化 Pod 的重新调度和驱逐时，请使用 Guaranteed Quality of Service（请求等于限制）。 一个很好的例子是用于数据库的 Pod。 当你想要优化集群并明智地使用资源时，请使用 Burstable Quality of Service（请求匹配实际平均使用情况）。 如果您有 Web 应用程序或 REST API，您可能希望使用 Burstable Quality of Service。 那如何选择正确的请求和限制值？ 你应该分析应用程序并测量空闲、负载和峰值时的内存和 CPU 消耗。更直接的策略包括部署 Vertical Pod Autoscaler 并等待它建议正确的值。 Vertical Pod Autoscaler 从 Pod 收集数据并应用回归模型来推断请求和限制。 您可以在本文中了解有关如何执行此操作的更多信息。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:6","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"关于集群的缩容 每 10 秒，只有当请求利用率低于 50% 时，Cluster Autoscaler 才会决定删除节点。 换句话说，对于同一节点上的所有 Pod，它会汇总 CPU 和内存请求。 如果它们低于节点容量的一半，Cluster Autoscaler 将考虑当前节点进行缩减。 值得注意的是，Cluster Autoscaler 不考虑实际的 CPU 和内存使用或限制，而只查看资源请求。 在移除节点之前，Cluster Autoscaler 执行： Pod 检查以确保 Pod 可以移动到其他节点。 Node 节点检查以防止节点过早被破坏。 如果检查通过，Cluster Autoscaler 将从集群中删除节点。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:7","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"为什么不基于内存或CPU进行自动伸缩 在扩缩容时，基于 CPU 或内存的 Cluster Autoscaler 不关心 pod。 想象一下，有一个只有一个节点的集群，并设置 Autoscaler 来添加一个新节点当 CPU 使用率达到总容量的 80%。 然后你决定创建一个具有 3 个副本的 Deployment。三个 Pod 的总资源使用率达到了 CPU 的 85%。 一个新的 Node 节点被提供。如果你不需要更多 Pod 怎么办？你有一个完整节点处于空闲的状态——这不是很好。这种使用 Autoscaler 的方式是不鼓励的。 ","date":"2023-03-16","objectID":"/posts/a18401/:2:8","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["Kubernetes"],"content":"总结 在 Kubernetes 中定义和实施成功的扩缩容策略需要您掌握几个主题： 熟悉 Kubernetes 节点中的可分配资源。 微调 Metrics Server、Horizontal Pod Autoscaler 和 Cluster Autoscalers 的刷新间隔。 规划集群和节点实例大小。 做好容器镜像的缓存。 做好应用程序基准测试和分析。 但是上面这些还不够，你还需要使用适当的监控工具，反复测试您的扩缩容策略并调整集群的节点创建速度和成本。 https://cloudnative.to/blog/kubernetes-autoscaling-strategy/ ","date":"2023-03-16","objectID":"/posts/a18401/:3:0","tags":["自动伸缩"],"title":"如何选择最佳的Kubernetes集群自动伸缩策略","uri":"/posts/a18401/"},{"categories":["sre"],"content":"1，SLA SLA是Service-Level Agreement的缩写，意思是服务等级协议，一般是协议双方做的彼此承诺，放在运维的领域，很重要的一个结果指标就是系统的SLA，这个是技术向业务做的一个承诺。 系统SLA的定制方法一般有两种，一种是通过时间维度进行测算，另外一种是通过用户请求状态进行测算。 时间维度测算 公式： PS：如果年度SLA，则n=365 这种计算方式比较常规，通用，但真正较真起来，还是比较麻烦的，麻烦的地方主要有以下几点： 1），业务中断怎么判断，须知一个业务完全中断的场景并不多见，往往是出现部分业务受到影响。 2），复杂组织场景下，如何做责任划分，比如A部门引发的问题，但B部门的容错性做的也不好，这种情况A，B的各自SLA是多少？ 3），时间分片并不是完全等价的，业务高峰时的一个小时要比业务低谷值钱的多，如果按照同样的时间去计算，其实是有失公允的。 鉴于以上种种原因，在公司SLA实际计算中，计算公式会变得非常复杂，比较常见的一种就是根据业务进行时间换算，公式为： PS：如果年度SLA，则n=365 举例： 如果一天的业务量是一万单，业务时出现故障高峰，持续一个小时，影响1000单，那么时间业务影响时间换算为：1000 / 10000 * 24 = 2.4个小时，当天的SLA为 90%，而非95.8% 这种算法的优点是： 直观，计算简单，业务部门容易理解 缺点是： 这是个结果指标，改进指向不明确。 用户请求状态测算 公式： 举例： 如果一个系统，用户一天请求量为10000，其中5XX的请求为1000，那么当天的SLA为90% 优点： 可以有针对性的改进，只要增加访问成功率即可 缺点： 业务不容易理解，在什么事请求成功上容易产生分歧 2，支撑SLA的运维指标 SLA一般我们定义为结果指标，也就是到最后一刻才知道是否正常，所以一般需要有一些过程指标进行跟踪，这里着重介绍一下运维侧指标，开发侧比较简单，不做详细介绍 一级指标 一级指标直接承载SLA，指标好坏，会对SLA有直接影响 1），故障次数，这个比较理解，就是有业务影响的异常次数 2），故障的平均恢复时间，为了避免某几个故障处理时间过长，导致指标不能反映真实情况，一般会采用P90，P95的故障平均恢复时间 3），N分钟内的异常恢复比例，N的取值和公司的技术能力和实际情况定，以故障为例，一般是30分钟能恢复就已经很不错了 二级指标 二级指标间接承载SLA，指标好坏会对一级指标有直接影响 1），用户报障比，有多少故障是用户发现的，而非监控系统发现的 2），自动化变更占比，数字证明，自动化的变更质量要更好一些 3），问题及时解决率，问题单尤其是故障产生的问题单解决效率 4），事件及时解决率，事件单及时处理效率 5），告警及时处理率，这个是把故障控制在萌芽中的很有效手段 6），监控覆盖率，生产重要的应用和组件的监控覆盖程度 这些指标计算公式比较简单，这里不赘述。 ","date":"2023-03-08","objectID":"/posts/3e3c43/:0:0","tags":["sla"],"title":"SLA和运维指标","uri":"/posts/3e3c43/"},{"categories":["openebs"],"content":"CStor 存储策略 ","date":"2023-03-03","objectID":"/posts/4df4f1/:1:0","tags":["openebs","cstor"],"title":"OpenEBS-CStor使用指南","uri":"/posts/4df4f1/"},{"categories":["openebs"],"content":"目标节点 nodeSelector cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cstor.openebs.io/v1 kind: CStorVolumePolicy metadata: name: csi-volume-policy namespace: openebs spec: target: nodeSelector: biz.type: test EOF ","date":"2023-03-03","objectID":"/posts/4df4f1/:1:1","tags":["openebs","cstor"],"title":"OpenEBS-CStor使用指南","uri":"/posts/4df4f1/"},{"categories":["openebs"],"content":"目标节点亲和行 affinity apiVersion: cstor.openebs.io/v1 kind: CStorVolumePolicy metadata: name: csi-volume-policy namespace: openebs spec: target: affinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: openebs.io/target-affinity operator: In values: - fio-cstor // application-unique-label topologyKey: kubernetes.io/hostname namespaces: [\"default\"] ","date":"2023-03-03","objectID":"/posts/4df4f1/:1:2","tags":["openebs","cstor"],"title":"OpenEBS-CStor使用指南","uri":"/posts/4df4f1/"},{"categories":["openebs"],"content":"目标节点资源限制 resources apiVersion: cstor.openebs.io/v1 kind: CStorVolumePolicy metadata: name: csi-volume-policy namespace: openebs spec: target: resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" auxResources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" ","date":"2023-03-03","objectID":"/posts/4df4f1/:1:3","tags":["openebs","cstor"],"title":"OpenEBS-CStor使用指南","uri":"/posts/4df4f1/"},{"categories":["openebs"],"content":"目标节点污点 tolerations apiVersion: cstor.openebs.io/v1 kind: CStorVolumePolicy metadata: name: csi-volume-policy namespace: openebs spec: replica: {} target: tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" ","date":"2023-03-03","objectID":"/posts/4df4f1/:1:4","tags":["openebs","cstor"],"title":"OpenEBS-CStor使用指南","uri":"/posts/4df4f1/"},{"categories":["云原生"],"content":"本文不讲源码，来说说 istio sidecar 配置，从而灵活的控制 sidecar 注入、资源修改等场景。 kubectl get cm -n istio-system istio-sidecar-injector -o yaml 先来预览一下 istio-sidecar-injector，主要包含 config 和 values ","date":"2023-02-24","objectID":"/posts/aaad19/:0:0","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["云原生"],"content":"config 可以看到配置项有 默认模板 defaultTemplates、注入策略 policy、注入选择器 alwaysInjectSelector、 永不注入选择器 neverInjectSelector、injectedAnnotations 和 模板内容 templates 等 关于注入策略(policy)、注入选择器(alwaysInjectSelector、neverInjectSelector)和注入注解(injectedAnnotations)可以查看 Sidecar 自动注入 本文主要来了解 Sidecar 注入的模板，方便后续需要针对Sidecar 的部署调整 ","date":"2023-02-24","objectID":"/posts/aaad19/:1:0","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["云原生"],"content":"templates - sidecar istio 默认提供了4种不同的注入模板 sidecar、gateway、grpc-simple 和 grpc-agent,基本上都是大同小异。接下来主要看看sidecar这个模板配置了哪些。 ","date":"2023-02-24","objectID":"/posts/aaad19/:2:0","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["云原生"],"content":"通过 Annotations 灵活控制不同部署的资源限制 template: metadata: annotations: ... sidecar.istio.io/proxyCPU: 50m ## proxy CPU request sidecar.istio.io/proxyCPULimit: 2000m ## proxy CPU limit sidecar.istio.io/proxyMemory: 500Mi ## proxy memory request sidecar.istio.io/proxyMemoryLimit: 2Gi ## proxy memory limit ... ","date":"2023-02-24","objectID":"/posts/aaad19/:2:1","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["云原生"],"content":"修改部署的proxy镜像 template: metadata: annotations: ... sidecar.istio.io/proxyImage: aeraki/meta-protocol-proxy:1.1.2 ... ","date":"2023-02-24","objectID":"/posts/aaad19/:2:2","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["云原生"],"content":"修改部署的proxy 初始化配置 template: metadata: annotations: ... sidecar.istio.io/bootstrapOverride: aeraki-bootstrap-config ... 以下操作全局生效 ","date":"2023-02-24","objectID":"/posts/aaad19/:2:3","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["云原生"],"content":"修改环境变量 kubectl edit cm -n istio-system istio-sidecar-injector env: {{- if eq (env \"PILOT_ENABLE_INBOUND_PASSTHROUGH\" \"true\") \"false\" }} - name: REWRITE_PROBE_LEGACY_LOCALHOST_DESTINATION value: \"true\" {{- end }} - name: JWT_POLICY value: {{ .Values.global.jwtPolicy }} - name: PILOT_CERT_PROVIDER value: {{ .Values.global.pilotCertProvider }} - name: CA_ADDR {{- if .Values.global.caAddress }} value: {{ .Values.global.caAddress }} {{- else }} value: istiod{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}.{{ .Values.global.istioNamespace }}.svc:15012 {{- end }} - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name ","date":"2023-02-24","objectID":"/posts/aaad19/:2:4","tags":["istio","sidecars"],"title":"深入Istio系列-Sidecar配置模板","uri":"/posts/aaad19/"},{"categories":["sentry"],"content":"系列 1 分钟快速使用 Docker 上手最新版 Sentry-CLI - 创建版本 快速使用 Docker 上手 Sentry-CLI - 30 秒上手 Source Maps Sentry For React 完整接入详解 Sentry For Vue 完整接入详解 Sentry-CLI 使用详解 Sentry Web 性能监控 - Web Vitals Sentry Web 性能监控 - Metrics Sentry Web 性能监控 - Trends Sentry Web 前端监控 - 最佳实践(官方教程) Sentry 后端监控 - 最佳实践(官方教程) Sentry 监控 - Discover 大数据查询分析引擎 Sentry 监控 - Dashboards 数据可视化大屏 Sentry 监控 - Environments 区分不同部署环境的事件数据 Sentry 监控 - Security Policy 安全策略报告 Sentry 监控 - Search 搜索查询实战 Sentry 监控 - Alerts 告警 Sentry 监控 - Distributed Tracing 分布式跟踪 Sentry 监控 - 面向全栈开发人员的分布式跟踪 101 系列教程(一) Sentry 监控 - Snuba 数据中台架构简介(Kafka+Clickhouse) Sentry 监控 - Snuba 数据中台架构(Data Model 简介) Sentry 监控 - Snuba 数据中台架构(Query Processing 简介) Sentry 官方 JavaScript SDK 简介与调试指南 Sentry 监控 - Snuba 数据中台架构(编写和测试 Snuba 查询) Sentry 监控 - Snuba 数据中台架构(SnQL 查询语言简介) ","date":"2023-01-03","objectID":"/posts/ac8605/:1:0","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"克隆仓库 分别克隆 getsentry/sentry 与 getsentry/snuba： git clone https://github.com/getsentry/sentry.git git clone https://github.com/getsentry/snuba.git ","date":"2023-01-03","objectID":"/posts/ac8605/:2:0","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"安装系统依赖(以 Mac 为例) ","date":"2023-01-03","objectID":"/posts/ac8605/:3:0","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"Xcode CLI tools xcode-select --install ","date":"2023-01-03","objectID":"/posts/ac8605/:3:1","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"Brewfile 进入 sentry 文件夹，你会看到一个 Brewfile 文件： cd sentry Brewfile # required to run devservices cask 'docker' brew 'pyenv' # required for pyenv's python-build brew 'openssl' brew 'readline' # required for yarn test -u brew 'watchman' # required to build some of sentry's dependencies brew 'pkgconfig' brew 'libxslt' brew 'libxmlsec1' brew 'geoip' # Currently needed because on Big Sur there's no wheel for it brew 'librdkafka' # direnv isn't defined here, because we have it configured to check for a bootstrapped environment. # If it's installed in the early steps of the setup process, it just leads to confusion. # brew 'direnv' tap 'homebrew/cask' # required for acceptance testing cask 'chromedriver' 如果你本地已经安装了 Docker Desktop 并且已经启动，可以把 cask 'docker' 注释掉。 接下来，运行： brew bundle --verbose 如果你之前本地没有 Docker Desktop，则还需要手动启动一下它： open -g -a Docker.app ","date":"2023-01-03","objectID":"/posts/ac8605/:3:2","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"构建工具链 Sentry 依赖于 Python Wheels(包含二进制扩展模块的包)，官方为以下平台分发： Linux 兼容 PEP-513 (manylinux1) macOS 10.15 或更高版本 如果您的开发机器没有运行上述系统之一，则需要安装 Rust 工具链。 按照 https://www.rust-lang.org/tools/install 上的说明安装编译器和相关工具。安装后，Sentry 安装程序将自动使用 Rust 构建所有二进制模块，无需额外配置。 官方通常会跟踪最新的稳定 Rust 版本，该版本每六周更新一次。 因此，请确保通过偶尔运行来使您的 Rust 工具链保持最新： rustup update stable ","date":"2023-01-03","objectID":"/posts/ac8605/:4:0","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"Python Sentry 使用 pyenv 来安装和管理 Python 版本。 它是在您运行 brew bundle 时安装的。 要安装所需版本的 Python，您需要运行以下命令。 这将需要一段时间，因为您的计算机实际上正在编译 Python！ make setup-pyenv 这里假设你是 Zsh 用户。 如果您键入 which python，您应该看到类似 $HOME/.pyenv/shims/python 而不是 /usr/bin/python 的内容。这是因为以下内容已添加到您的启动脚本中： cat ~/.zprofile，你会看到如下内容： # MacPorts Installer addition on 2021-10-20_at_11:48:22: adding an appropriate PATH variable for use with MacPorts. export PATH=\"/opt/local/bin:/opt/local/sbin:$PATH\" # Finished adapting your PATH environment variable for use with MacPorts. # It is assumed that pyenv is installed via Brew, so this is all we need to do. eval \"$(pyenv init --path)\" ","date":"2023-01-03","objectID":"/posts/ac8605/:5:0","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"虚拟环境 您现在已准备好创建 Python 虚拟环境。运行： python -m venv .venv 并激活虚拟环境： source .venv/bin/activate 如果一切正常，运行 which python 现在应该会导致类似 /Users/you/sentry/.venv/bin/python 的结果。 ","date":"2023-01-03","objectID":"/posts/ac8605/:5:1","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"Snuba 配置实战 ","date":"2023-01-03","objectID":"/posts/ac8605/:6:0","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"启动 Snuba 相关依赖项容器 cd ../sentry git checkout master git pull source .venv/bin/activate sentry devservices up --exclude=snuba # 11:17:59 [WARNING] sentry.utils.geo: settings.GEOIP_PATH_MMDB not configured. # 11:18:01 [INFO] sentry.plugins.github: apps-not-configured # \u003e Pulling image 'postgres:9.6-alpine' # \u003e Pulling image 'yandex/clickhouse-server:20.3.9.70' # \u003e Not starting container 'sentry_relay' because it should be started on-demand with devserver. # \u003e Creating 'sentry_redis' volume # \u003e Creating 'sentry_zookeeper_6' volume # \u003e Creating 'sentry_kafka_6' volume # \u003e Creating container 'sentry_redis' # \u003e Creating container 'sentry_zookeeper' # \u003e Creating container 'sentry_kafka' # \u003e Starting container 'sentry_redis' (listening: ('127.0.0.1', 6379)) # \u003e Starting container 'sentry_kafka' (listening: ('127.0.0.1', 9092)) # \u003e Starting container 'sentry_zookeeper' # \u003e Creating 'sentry_clickhouse' volume # \u003e Creating container 'sentry_clickhouse' # \u003e Creating 'sentry_postgres' volume # \u003e Creating 'sentry_wal2json' volume # \u003e Starting container 'sentry_clickhouse' (listening: ('127.0.0.1', 9000), ('127.0.0.1', 9009), ('127.0.0.1', 8123)) # \u003e Creating container 'sentry_postgres' # \u003e Starting container 'sentry_postgres' (listening: ('127.0.0.1', 5432)) 这将在 master 上获取最新版本的 Sentry，并调出所有 snuba 的依赖项。 Snuba 主要依赖 clickhouse，zookeeper，kafka，redis 相关容器。 docker ps 查看一下： 1149a6f6ff23 postgres:9.6-alpine \"docker-entrypoint.s…\" 3 minutes ago Up 3 minutes 127.0.0.1:5432-\u003e5432/tcp sentry_postgres a7f3af7d52bb yandex/clickhouse-server:20.3.9.70 \"/entrypoint.sh\" 3 minutes ago Up 3 minutes 127.0.0.1:8123-\u003e8123/tcp, 127.0.0.1:9000-\u003e9000/tcp, 127.0.0.1:9009-\u003e9009/tcp sentry_clickhouse 68913ee15c43 confluentinc/cp-zookeeper:6.2.0 \"/etc/confluent/dock…\" 3 minutes ago Up 3 minutes 2181/tcp, 2888/tcp, 3888/tcp sentry_zookeeper 5a248eb26ed3 confluentinc/cp-kafka:6.2.0 \"/etc/confluent/dock…\" 3 minutes ago Up 3 minutes 127.0.0.1:9092-\u003e9092/tcp sentry_kafka 0573aff7b5af redis:5.0-alpine \"docker-entrypoint.s…\" 3 minutes ago Up 3 minutes 127.0.0.1:6379-\u003e6379/tcp sentry_redis ","date":"2023-01-03","objectID":"/posts/ac8605/:6:1","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"设置 Snuba 虚拟环境 cd snuba make pyenv-setup python -m venv .venv source .venv/bin/activate pip install --upgrade pip==21.1.3 make develop ","date":"2023-01-03","objectID":"/posts/ac8605/:6:2","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"查看迁移列表 snuba migrations list # system # [ ] 0001_migrations # # events # [ ] 0001_events_initial # [ ] 0002_events_onpremise_compatibility # [ ] 0003_errors # [ ] 0004_errors_onpremise_compatibility # [ ] 0005_events_tags_hash_map (blocking) # [ ] 0006_errors_tags_hash_map (blocking) # [ ] 0007_groupedmessages # [ ] 0008_groupassignees # [ ] 0009_errors_add_http_fields # [ ] 0010_groupedmessages_onpremise_compatibility (blocking) # [ ] 0011_rebuild_errors # [ ] 0012_errors_make_level_nullable # [ ] 0013_errors_add_hierarchical_hashes # [ ] 0014_backfill_errors (blocking) # [ ] 0015_truncate_events # # transactions # [ ] 0001_transactions # [ ] 0002_transactions_onpremise_fix_orderby_and_partitionby (blocking) # [ ] 0003_transactions_onpremise_fix_columns (blocking) # [ ] 0004_transactions_add_tags_hash_map (blocking) # [ ] 0005_transactions_add_measurements # [ ] 0006_transactions_add_http_fields # [ ] 0007_transactions_add_discover_cols # [ ] 0008_transactions_add_timestamp_index # [ ] 0009_transactions_fix_title_and_message # [ ] 0010_transactions_nullable_trace_id # [ ] 0011_transactions_add_span_op_breakdowns # [ ] 0012_transactions_add_spans # # discover # [ ] 0001_discover_merge_table # [ ] 0002_discover_add_deleted_tags_hash_map # [ ] 0003_discover_fix_user_column # [ ] 0004_discover_fix_title_and_message # [ ] 0005_discover_fix_transaction_name # [ ] 0006_discover_add_trace_id # [ ] 0007_discover_add_span_id # # outcomes # [ ] 0001_outcomes # [ ] 0002_outcomes_remove_size_and_bytes # [ ] 0003_outcomes_add_category_and_quantity # [ ] 0004_outcomes_matview_additions (blocking) # # metrics # [ ] 0001_metrics_buckets # [ ] 0002_metrics_sets # [ ] 0003_counters_to_buckets # [ ] 0004_metrics_counters # [ ] 0005_metrics_distributions_buckets # [ ] 0006_metrics_distributions # [ ] 0007_metrics_sets_granularity_10 # [ ] 0008_metrics_counters_granularity_10 # [ ] 0009_metrics_distributions_granularity_10 # [ ] 0010_metrics_sets_granularity_1h # [ ] 0011_metrics_counters_granularity_1h # [ ] 0012_metrics_distributions_granularity_1h # [ ] 0013_metrics_sets_granularity_1d # [ ] 0014_metrics_counters_granularity_1d # [ ] 0015_metrics_distributions_granularity_1d # # sessions # [ ] 0001_sessions # [ ] 0002_sessions_aggregates # [ ] 0003_sessions_matview ","date":"2023-01-03","objectID":"/posts/ac8605/:6:3","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"运行迁移 snuba migrations migrate --force # ...... # 2021-12-01 19:45:57,557 Running migration: 0014_metrics_counters_granularity_1d # 2021-12-01 19:45:57,575 Finished: 0014_metrics_counters_granularity_1d # 2021-12-01 19:45:57,589 Running migration: 0015_metrics_distributions_granularity_1d # 2021-12-01 19:45:57,610 Finished: 0015_metrics_distributions_granularity_1d # 2021-12-01 19:45:57,623 Running migration: 0001_sessions # 2021-12-01 19:45:57,656 Finished: 0001_sessions # 2021-12-01 19:45:57,669 Running migration: 0002_sessions_aggregates # 2021-12-01 19:45:57,770 Finished: 0002_sessions_aggregates # 2021-12-01 19:45:57,792 Running migration: 0003_sessions_matview # 2021-12-01 19:45:57,849 Finished: 0003_sessions_matview # Finished running migrations ","date":"2023-01-03","objectID":"/posts/ac8605/:6:4","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"检查迁移 进入 Clickhouse 容器： docker exec -it sentry_clickhouse clickhouse-client # 运行如下 `sql` 语句： select count() from sentry_local # ClickHouse client version 20.3.9.70 (official build). # Connecting to localhost:9000 as user default. # Connected to ClickHouse server version 20.3.9 revision 54433. # a7f3af7d52bb :) select count() from sentry_local # SELECT count() # FROM sentry_local # ┌─count()─┐ # │ 0 │ # └─────────┘ # 1 rows in set. Elapsed: 0.008 sec. # a7f3af7d52bb :) ","date":"2023-01-03","objectID":"/posts/ac8605/:6:5","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"查看相关实体数据集 snuba entities list # Declared Entities: # discover # events # groups # groupassignee # groupedmessage # metrics_sets # metrics_counters # metrics_distributions # outcomes # outcomes_raw # sessions # org_sessions # spans # transactions # discover_transactions # discover_events ","date":"2023-01-03","objectID":"/posts/ac8605/:6:6","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":["sentry"],"content":"启动开发服务器 此命令将启动 api 和所有 Snuba 消费者以从 Kafka 摄取数据： snuba devserver 转到 http://localhost:1218/events/snql，你将会看到一个简易的查询 UI。 sentry开发环境 ","date":"2023-01-03","objectID":"/posts/ac8605/:6:7","tags":["snuba"],"title":"Sentry 监控 - Snuba 数据中台本地开发环境配置实战","uri":"/posts/ac8605/"},{"categories":[],"content":"创建 aws IAM权限 https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/v1.13.0/docs/example-iam-policy.json 获取AK、SK ","date":"2022-12-30","objectID":"/posts/4842bb/:0:1","tags":[],"title":"自建k8s部署aws-ebs-csi-driver","uri":"/posts/4842bb/"},{"categories":[],"content":"部署aws-ebs-csi-driver kubectl create secret generic aws-secret \\ --namespace kube-system \\ --from-literal \"key_id=${AWS_ACCESS_KEY_ID}\" \\ --from-literal \"access_key=${AWS_SECRET_ACCESS_KEY}\" `` ```yaml # helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver # helm upgrade --install aws-ebs-csi-driver --version 2.13.0\\ --namespace kube-system \\ aws-ebs-csi-driver/aws-ebs-csi-driver \\ --set sidecars.provisioner.image.repository=registry.cn-hangzhou.aliyuncs.com/seam/csi-provisioner \\ --set sidecars.attacher.image.repository=registry.cn-hangzhou.aliyuncs.com/seam/csi-attacher \\ --set sidecars.snapshotter.image.repository=registry.cn-hangzhou.aliyuncs.com/seam/csi-snapshotter \\ --set sidecars.livenessProbe.image.repository=registry.cn-hangzhou.aliyuncs.com/seam/livenessprobe \\ --set sidecars.resizer.image.repository=registry.cn-hangzhou.aliyuncs.com/seam/csi-resizer \\ --set sidecars.nodeDriverRegistrar.image.repository=registry.cn-hangzhou.aliyuncs.com/seam/csi-node-driver-registrar \\ --set node.kubeletPath=/data/k8s/kubelet # kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\" NAME READY STATUS RESTARTS AGE ebs-csi-controller-5cbfd45dc-2fq9q 6/6 Running 0 102s ebs-csi-controller-5cbfd45dc-jgpl9 6/6 Running 0 102s ebs-csi-node-2s8lj 3/3 Running 0 101s ebs-csi-node-4jstr 3/3 Running 0 101s ebs-csi-node-72w69 3/3 Running 0 101s ebs-csi-node-759rd 0/3 Pending 0 101s ebs-csi-node-cq86s 3/3 Running 0 101s ebs-csi-node-jnfxk 0/3 Pending 0 101s ebs-csi-node-m48nn 3/3 Running 0 101s ","date":"2022-12-30","objectID":"/posts/4842bb/:0:2","tags":[],"title":"自建k8s部署aws-ebs-csi-driver","uri":"/posts/4842bb/"},{"categories":[],"content":"挂载验证 cat \u003c\u003c EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-sc provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer parameters: csi.storage.k8s.io/fstype: xfs type: gp3 iopsPerGB: \"500\" tagSpecification_1: \"businessid=000000040629041578\" --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ebs-claim spec: accessModes: - ReadWriteOnce storageClassName: ebs-sc resources: requests: storage: 10Gi --- apiVersion: v1 kind: Pod metadata: name: app spec: # nodeSelector: # hostname: 10.86.183.104 containers: - name: app image: centos command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo $(date -u) \u003e\u003e /data/out.txt; sleep 5; done\"] volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: ebs-claim EOF ","date":"2022-12-30","objectID":"/posts/4842bb/:0:3","tags":[],"title":"自建k8s部署aws-ebs-csi-driver","uri":"/posts/4842bb/"},{"categories":[],"content":"设置存储类 cat \u003c\u003c EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-sc parameters: type: gp3 # io1, io2, gp2, gp3, sc1, st1, standard, sbp1, sbg1 csi.storage.k8s.io/fstype: ext4 # xfs, ext2, ext3, ext4 iops: \"3000\" # iopsPerGB: 500 每GB存储的iops,跟iops二选一 throughput: \"125\" tagSpecification_1: \"businessid=000000040629041578\" tagSpecification_2: \"pvcnamespace={{ .PVCNamespace }}\" tagSpecification_3: \"pvcname={{ .PVCName }}\" tagSpecification_4: \"pvname={{ .PVName }}\" provisioner: ebs.csi.aws.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer EOF ","date":"2022-12-30","objectID":"/posts/4842bb/:0:4","tags":[],"title":"自建k8s部署aws-ebs-csi-driver","uri":"/posts/4842bb/"},{"categories":[],"content":"性能测试 基准测试脚本 cat \u003c\u003c EOF \u003e fio-job.yaml # NOTE: For details of params to construct an fio job, refer to this link: # https://fio.readthedocs.io/en/latest/fio_doc.html --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ebs-fio spec: accessModes: - ReadWriteOnce storageClassName: ebs-sc resources: requests: storage: 6Gi --- apiVersion: batch/v1 kind: Job metadata: generateName: dbench- spec: template: spec: containers: - name: dbench image: openebs/perf-test:latest imagePullPolicy: IfNotPresent env: ## storage mount point on which testfiles are created - name: DBENCH_MOUNTPOINT value: /data ########################################################## # I/O PROFILE COVERAGE FOR SPECIFIC PERF CHARACTERISTICS # ########################################################## ## quick: {read, write} iops, {read, write} bw (all random) ## detailed: {quick}, {read, write} latency \u0026 mixed 75r:25w (all random), {read, write} bw (all sequential) ## custom: a single user-defined job run with params specified in env 'CUSTOM' - name: DBENCH_TYPE value: detailed #################################################### # STANDARD TUNABLES FOR DBENCH_TYPE=QUICK/DETAILED # #################################################### ## active data size for the bench test - name: FIO_SIZE value: 2G ## use un-buffered i/o (usually O_DIRECT) - name: FIO_DIRECT value: '1' ## no of independent threads doing the same i/o - name: FIO_NUMJOBS value: '1' ## space b/w starting offsets on a file in case of parallel file i/o - name: FIO_OFFSET_INCREMENT value: 250M ## nature of i/o to file. commonly supported: libaio, sync, - name: FIO_IOENGINE value: libaio ## additional runtime options which will be appended to the above params ## ensure options used are not mutually exclusive w/ above params ## ex: '--group_reporting=1, stonewall, --ramptime=\u003cval\u003e etc.., - name: OPTIONS value: '' #################################################### # CUSTOM JOB SPEC FOR DBENCH_TYPE=CUSTOM # #################################################### ## this will execute a single job run with the params specified ## ex: '--bs=16k --iodepth=64 --ioengine=sync --size=500M --name=custom --readwrite=randrw --rwmixread=80 --random_distribution=pareto' - name: CUSTOM value: '' volumeMounts: - name: dbench-pv mountPath: /data restartPolicy: Never nodeSelector: hostname: shopline-xinjiapo-aws-dev-vm-8c32g-10.90.208.109 volumes: - name: dbench-pv persistentVolumeClaim: claimName: ebs-fio backoffLimit: 4 --- EOF kubectl create -f fio-job.yaml 3000 iops root@k8s-master-7b4d73fd8d:~# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ebs-fio Bound pvc-d603d626-1013-4fef-b1c7-b406d2d93546 6Gi RWO ebs-sc 4s root@k8s-master-7b4d73fd8d:~# kubectl get pod NAME READY STATUS RESTARTS AGE dbench-dgkpv-457kk 1/1 Running 0 110s root@k8s-master-7b4d73fd8d:~# root@k8s-master-7b4d73fd8d:~# root@k8s-master-7b4d73fd8d:~# root@k8s-master-7b4d73fd8d:~# kubectl get pod NAME READY STATUS RESTARTS AGE dbench-dgkpv-457kk 1/1 Running 0 2m59s root@k8s-master-7b4d73fd8d:~# kubectl logs -f dbench-dgkpv-457kk Working dir: /data Testing Read IOPS... read_iops: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64 fio-3.13 Starting 1 process read_iops: Laying out IO file (1 file / 2048MiB) read_iops: (groupid=0, jobs=1): err= 0: pid=18: Tue Dec 27 07:59:57 2022 read: IOPS=2995, BW=11.7MiB/s (12.3MB/s)(176MiB/15029msec) bw ( KiB/s): min=11912, max=12056, per=100.00%, avg=11999.47, stdev=27.31, samples=30 iops : min= 2978, max= 3014, avg=2999.87, stdev= 6.83, samples=30 cpu : usr=0.31%, sys=0.89%, ctx=9728, majf=0, minf=15 IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u003e=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u003e=64=0.0% issued rwts: total=45018,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=64 Ru","date":"2022-12-30","objectID":"/posts/4842bb/:0:5","tags":[],"title":"自建k8s部署aws-ebs-csi-driver","uri":"/posts/4842bb/"},{"categories":[],"content":"存在问题 RBAC角色问题导致节点无法挂载 attacher.Attach failed: volumeattachments.storage.k8s.io is forbidden kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:nodes:volumeattachments rules: - apiGroups: - storage.k8s.io resources: - volumeattachments verbs: - create - watch - delete --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:nodes:volumeattachments roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:nodes:volumeattachments subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodes EOF 操作系统版本问题找不到驱动 源码 ubuntu 16 ubuntu 22 ","date":"2022-12-30","objectID":"/posts/4842bb/:0:6","tags":[],"title":"自建k8s部署aws-ebs-csi-driver","uri":"/posts/4842bb/"},{"categories":[],"content":"1. Fork in the cloud Visit https://github.com/kubernetes/kubernetes Click Fork button (top right) to establish a cloud-based fork. ","date":"2022-12-30","objectID":"/posts/f09cdc/:1:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"2. Clone fork to local storage Per Go’s workspace instructions, place Kubernetes’ code on your GOPATH using the following cloning procedure. In your shell, define a local working directory as working_dir. If your GOPATH has multiple paths, pick just one and use it instead of $GOPATH. You must follow exactly this pattern, neither $GOPATH/src/github.com/${your github profile name}/ nor any other pattern will work. export working_dir=\"$(go env GOPATH)/src/k8s.io\" If you already do Go development on github, the k8s.io directory will be a sibling to your existing github.com directory. Set user to match your github profile name: export user=\u003cyour github profile name\u003e Both $working_dir and $user are mentioned in the figure above. Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/$user/kubernetes.git # or: git clone git@github.com:$user/kubernetes.git cd $working_dir/kubernetes git remote add upstream https://github.com/kubernetes/kubernetes.git # or: git remote add upstream git@github.com:kubernetes/kubernetes.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v ","date":"2022-12-30","objectID":"/posts/f09cdc/:2:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"3. Create a Working Branch Get your local master up to date. Note that depending on which repository you are working from, the default branch may be called “main” instead of “master”. cd $working_dir/kubernetes git fetch upstream git checkout master git rebase upstream/master Create your new branch. git checkout -b myfeature You may now edit files on the myfeature branch. ","date":"2022-12-30","objectID":"/posts/f09cdc/:3:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"Building Kubernetes This workflow is process-specific. For quick-start build instructions for kubernetes/kubernetes, please see here. ","date":"2022-12-30","objectID":"/posts/f09cdc/:3:1","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"4. Keep your branch in sync You will need to periodically fetch changes from the upstream repository to keep your working branch in sync. Note that depending on which repository you are working from, the default branch may be called ‘main’ instead of ‘master’. Make sure your local repository is on your working branch and run the following commands to keep it in sync: git fetch upstream git rebase upstream/master Please don’t use git pull instead of the above fetch and rebase. Since git pull executes a merge, it creates merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful (see below). You might also consider changing your .git/config file via git config branch.autoSetupRebase always to change the behavior of git pull, or another non-merge option such as git pull --rebase. ","date":"2022-12-30","objectID":"/posts/f09cdc/:4:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"5. Commit Your Changes You will probably want to regularly commit your changes. It is likely that you will go back and edit, build, and test multiple times. After a few cycles of this, you might amend your previous commit. git commit ","date":"2022-12-30","objectID":"/posts/f09cdc/:5:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"6. Push to GitHub When your changes are ready for review, push your working branch to your fork on GitHub. git push -f \u003cyour_remote_name\u003e myfeature ","date":"2022-12-30","objectID":"/posts/f09cdc/:6:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"7. Create a Pull Request Visit your fork at https://github.com/\u003cuser\u003e/kubernetes Click the Compare \u0026 Pull Request button next to your myfeature branch. Check out the pull request process for more details and advice. If you have upstream write access, please refrain from using the GitHub UI for creating PRs, because GitHub will create the PR branch inside the main repository rather than inside your fork. ","date":"2022-12-30","objectID":"/posts/f09cdc/:7:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"Get a code review Once your pull request has been opened it will be assigned to one or more reviewers. Those reviewers will do a thorough code review, looking for correctness, bugs, opportunities for improvement, documentation and comments, and style. Commit changes made in response to review comments to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review. ","date":"2022-12-30","objectID":"/posts/f09cdc/:7:1","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"Squash commits After a review, prepare your PR for merging by squashing your commits. All commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process. Before merging a PR, squash the following kinds of commits: Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it’s not a requirement. In particular, merge commits must be removed, as they will not pass tests. To squash your commits, perform an interactive rebase: Check your git branch: git status The output should be similar to this: On branch your-contribution Your branch is up to date with 'origin/your-contribution'. Start an interactive rebase using a specific commit hash, or count backwards from your last commit using HEAD~\u003cn\u003e, where \u003cn\u003e represents the number of commits to include in the rebase. git rebase -i HEAD~3 The output should be similar to this: pick 2ebe926 Original commit pick 31f33e9 Address feedback pick b0315fe Second unit of work # Rebase 7c34fc9..b0315ff onto 7c34fc9 (3 commands) # # Commands: # p, pick \u003ccommit\u003e = use commit # r, reword \u003ccommit\u003e = use commit, but edit the commit message # e, edit \u003ccommit\u003e = use commit, but stop for amending # s, squash \u003ccommit\u003e = use commit, but meld into previous commit # f, fixup \u003ccommit\u003e = like \"squash\", but discard this commit's log message ... Use a command line text editor to change the word pick to squash for the commits you want to squash, then save your changes and continue the rebase: pick 2ebe926 Original commit squash 31f33e9 Address feedback pick b0315fe Second unit of work ... The output after saving changes should look similar to this: [detached HEAD 61fdded] Second unit of work Date: Thu Mar 5 19:01:32 2020 +0100 2 files changed, 15 insertions(+), 1 deletion(-) ... Successfully rebased and updated refs/heads/master. Force push your changes to your remote branch: git push --force For mass automated fixups such as automated doc formatting, use one or more commits for the changes to tooling and a final commit to apply the fixup en masse. This makes reviews easier. ","date":"2022-12-30","objectID":"/posts/f09cdc/:7:2","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"Merging a commit Once you’ve received review and approval, your commits are squashed, your PR is ready for merging. Merging happens automatically after both a Reviewer and Approver have approved the PR. If you haven’t squashed your commits, they may ask you to do so before approving a PR. ","date":"2022-12-30","objectID":"/posts/f09cdc/:8:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":[],"content":"Reverting a commit In case you wish to revert a commit, use the following instructions. If you have upstream write access, please refrain from using the Revert button in the GitHub UI for creating the PR, because GitHub will create the PR branch inside the main repository rather than inside your fork. Create a branch and sync it with upstream. Note that depending on which repository you are working from, the default branch may be called ‘main’ instead of ‘master’. # create a branch git checkout -b myrevert # sync the branch with upstream git fetch upstream git rebase upstream/master If the commit you wish to revert is a merge commit, use this command: # SHA is the hash of the merge commit you wish to revert git revert -m 1 \u003cSHA\u003e If it is a single commit, use this command: # SHA is the hash of the single commit you wish to revert git revert \u003cSHA\u003e This will create a new commit reverting the changes. Push this new commit to your remote. git push \u003cyour_remote_name\u003e myrevert Finally, create a Pull Request using this branch. ","date":"2022-12-30","objectID":"/posts/f09cdc/:9:0","tags":[],"title":"github工作流","uri":"/posts/f09cdc/"},{"categories":["存储"],"content":"依赖说明 ","date":"2022-10-18","objectID":"/posts/f41ffb/:1:0","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"准备裸盘 root@ubuntu:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme2n1 259:2 0 50G 0 disk ### 50G未格式化磁盘 nvme1n1 259:0 0 600G 0 disk └─nvme1n1p1 259:3 0 600G 0 part /data nvme0n1 259:1 0 40G 0 disk └─nvme0n1p1 259:4 0 40G 0 part / ","date":"2022-10-18","objectID":"/posts/f41ffb/:1:1","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"安装iscsi sudo apt-get update sudo apt-get install open-iscsi sudo systemctl enable --now iscsid ","date":"2022-10-18","objectID":"/posts/f41ffb/:1:2","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"OpenEBS安装 helm repo add openebs https://openebs.github.io/charts helm repo update ### 默认安装 helm install openebs --namespace openebs openebs/openebs --create-namespace ","date":"2022-10-18","objectID":"/posts/f41ffb/:2:0","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"启用cstor ### 启用cstor,替换镜像地址:k8s.gcr.io/sig-storage cat \u003c\u003c EOF \u003e\u003e custom-values.yaml USER-SUPPLIED VALUES: cstor: admissionServer: nodeSelector: biz.type: test csiController: attacher: image: registry: registry.cn-hangzhou.aliyuncs.com/ repository: seam/csi-attacher nodeSelector: biz.type: test provisioner: image: registry: registry.cn-hangzhou.aliyuncs.com/ repository: seam/csi-provisioner resizer: image: registry: registry.cn-hangzhou.aliyuncs.com/ repository: seam/csi-resizer snapshotController: image: registry: registry.cn-hangzhou.aliyuncs.com/ repository: seam/snapshot-controller snapshotter: image: registry: registry.cn-hangzhou.aliyuncs.com/ repository: seam/csi-snapshotter csiNode: driverRegistrar: image: registry: registry.cn-hangzhou.aliyuncs.com/ repository: seam/csi-node-driver-registrar kubeletDir: /data/k8s/kubelet/ nodeSelector: biz.type: test cspcOperator: nodeSelector: biz.type: test cvcOperator: nodeSelector: biz.type: test enabled: true localprovisioner: nodeSelector: biz.type: test ndm: nodeSelector: biz.type: test ndmOperator: nodeSelector: biz.type: test provisioner: nodeSelector: biz.type: test snapshotOperator: nodeSelector: biz.type: test webhook: nodeSelector: biz.type: test EOF helm upgrade --install openebs --namespace openebs openebs/openebs --version 3.4.1 -f custom-values.yaml cstor.csiNode.kubeletDir=\"/data/k8s/kubelet/\" 设置为实际集群运行配置 ","date":"2022-10-18","objectID":"/posts/f41ffb/:2:1","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"安装后查看磁盘情况 root@ubuntu:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme2n1 259:2 0 50G 0 disk └─nvme2n1p1 259:6 0 50G 0 part ### 格式化未挂载盘 nvme1n1 259:0 0 600G 0 disk └─nvme1n1p1 259:3 0 600G 0 part /data nvme0n1 259:1 0 40G 0 disk └─nvme0n1p1 259:4 0 40G 0 part / # kubectl get bd -n openebs NAME NODENAME SIZE CLAIMSTATE STATUS AGE blockdevice-5cc77e185a944270fa46b7d233320660 dev-vm-8c32g-600g-10-86-183-104 53686025728 Unclaimed Active 4m56s blockdevice-b3978ac55b8d0fc6ae9e2bc3b5b92fd6 dev-vm-8c64g-600g-10-86-16-225 53686025728 Unclaimed Active 4m56s blockdevice-d93d0a91351a13f4de81fd0a5e6e4540 dev-vm-4c16g-600g-10-86-141-68 53686025728 Unclaimed Active 4m56s ","date":"2022-10-18","objectID":"/posts/f41ffb/:2:2","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"Local PV device 测试 ","date":"2022-10-18","objectID":"/posts/f41ffb/:3:0","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"验证PVC挂载 # cat \u003c\u003c EOF \u003e local-device-pod.yaml --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: local-device-pvc spec: storageClassName: openebs-device accessModes: - ReadWriteOnce resources: requests: storage: 5G --- apiVersion: v1 kind: Pod metadata: name: hello-local-device-pod spec: volumes: - name: local-storage persistentVolumeClaim: claimName: local-device-pvc containers: - name: hello-container image: dockerhub.kubekey.local/library/busybox:latest command: - sh - -c - 'while true; do echo \"`date` [`hostname`] Hello from OpenEBS Local PV.\" \u003e\u003e /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF # kubectl apply -f local-device-pod.yaml ","date":"2022-10-18","objectID":"/posts/f41ffb/:3:1","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"查看PVC挂载情况 # kubectl get pvc local-device-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-device-pvc Bound pvc-b743d6b9-2854-4e9c-aaff-39064605e594 5G RWO openebs-device 2m3s # kubectl get pod hello-local-device-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES hello-local-device-pod 1/1 Running 0 18s 10.86.232.79 dev-vm-4c16g-600g-10-86-141-68 \u003cnone\u003e \u003cnone\u003e # kubectl get bd -n openebs NAME NODENAME SIZE CLAIMSTATE STATUS AGE blockdevice-5cc77e185a944270fa46b7d233320660 dev-vm-8c32g-600g-10-86-183-104 53686025728 Unclaimed Active 11m blockdevice-b3978ac55b8d0fc6ae9e2bc3b5b92fd6 dev-vm-8c64g-600g-10-86-16-225 53686025728 Unclaimed Active 11m blockdevice-d93d0a91351a13f4de81fd0a5e6e4540 dev-vm-4c16g-600g-10-86-141-68 53686025728 Claimed Active 11m Pod调度到节点10-86-141-68, 节点10-86-141-68对应的 blockdevice 状态为 Claimed 登录节点10-86-141-68 验证数据是否写入 # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme2n1 259:2 0 50G 0 disk └─nvme2n1p1 259:5 0 50G 0 part /data/k8s/kubelet/pods/d1f84232-f355-4f9f-850d-129b8fd59864/volumes/kubernetes.io~local-volume/pvc-b743d6b9-2854-4e9c-aaff-39064605e594 nvme1n1 259:0 0 600G 0 disk └─nvme1n1p1 259:3 0 600G 0 part /data nvme0n1 259:1 0 40G 0 disk └─nvme0n1p1 259:4 0 40G 0 part / # ls -l /data/k8s/kubelet/pods/d1f84232-f355-4f9f-850d-129b8fd59864/volumes/kubernetes.io~local-volume/pvc-b743d6b9-2854-4e9c-aaff-39064605e594/ total 20 -rw-r--r-- 1 root root 67 Oct 18 15:33 greet.txt drwx------ 2 root root 16384 Oct 18 15:28 lost+found # ls -l /data/k8s/kubelet/pods/d1f84232-f355-4f9f-850d-129b8fd59864/volumes/kubernetes.io~local-volume/pvc-b743d6b9-2854-4e9c-aaff-39064605e594/greet.txt -rw-r--r-- 1 root root 67 Oct 18 15:33 /data/k8s/kubelet/pods/d1f84232-f355-4f9f-850d-129b8fd59864/volumes/kubernetes.io~local-volume/pvc-b743d6b9-2854-4e9c-aaff-39064605e594/greet.txt # cat /data/k8s/kubelet/pods/d1f84232-f355-4f9f-850d-129b8fd59864/volumes/kubernetes.io~local-volume/pvc-b743d6b9-2854-4e9c-aaff-39064605e594/greet.txt Tue Oct 18 15:32:27 CST 2022 [ubuntu] Hello from OpenEBS Local PV. ","date":"2022-10-18","objectID":"/posts/f41ffb/:3:2","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"Local PV device 性能测试 创建PVC cat \u003e dbench-pvc.yaml \u003c\u003cEOF --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: dbench spec: storageClassName: openebs-device accessModes: - ReadWriteOnce resources: requests: storage: 5G EOF kubectl apply -f dbench-pvc.yaml 创建压测任务 cat \u003c\u003c EOF \u003e fio-job.yaml # NOTE: For details of params to construct an fio job, refer to this link: # https://fio.readthedocs.io/en/latest/fio_doc.html --- apiVersion: batch/v1 kind: Job metadata: generateName: dbench- spec: template: spec: containers: - name: dbench image: openebs/perf-test:latest imagePullPolicy: IfNotPresent env: ## storage mount point on which testfiles are created - name: DBENCH_MOUNTPOINT value: /data ########################################################## # I/O PROFILE COVERAGE FOR SPECIFIC PERF CHARACTERISTICS # ########################################################## ## quick: {read, write} iops, {read, write} bw (all random) ## detailed: {quick}, {read, write} latency \u0026 mixed 75r:25w (all random), {read, write} bw (all sequential) ## custom: a single user-defined job run with params specified in env 'CUSTOM' - name: DBENCH_TYPE value: detailed #################################################### # STANDARD TUNABLES FOR DBENCH_TYPE=QUICK/DETAILED # #################################################### ## active data size for the bench test - name: FIO_SIZE value: 2G ## use un-buffered i/o (usually O_DIRECT) - name: FIO_DIRECT value: '1' ## no of independent threads doing the same i/o - name: FIO_NUMJOBS value: '1' ## space b/w starting offsets on a file in case of parallel file i/o - name: FIO_OFFSET_INCREMENT value: 250M ## nature of i/o to file. commonly supported: libaio, sync, - name: FIO_IOENGINE value: libaio ## additional runtime options which will be appended to the above params ## ensure options used are not mutually exclusive w/ above params ## ex: '--group_reporting=1, stonewall, --ramptime=\u003cval\u003e etc.., - name: OPTIONS value: '' #################################################### # CUSTOM JOB SPEC FOR DBENCH_TYPE=CUSTOM # #################################################### ## this will execute a single job run with the params specified ## ex: '--bs=16k --iodepth=64 --ioengine=sync --size=500M --name=custom --readwrite=randrw --rwmixread=80 --random_distribution=pareto' - name: CUSTOM value: '' volumeMounts: - name: dbench-pv mountPath: /data restartPolicy: Never volumes: - name: dbench-pv persistentVolumeClaim: claimName: dbench backoffLimit: 4 --- EOF kubectl create -f fio-job.yaml IOPS Bandwidth 随机读 2997 125MiB/s 随机写 2997 125MiB/s 顺序读 126MiB/s 顺序写 128MiB/s 混合读写50%:50% 3070/1011 第一次压测日志 # kubectl logs -f dbench-4jwlb-qjhzg Working dir: /data Testing Read IOPS... read_iops: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64 fio-3.13 Starting 1 process read_iops: Laying out IO file (1 file / 2048MiB) read_iops: (groupid=0, jobs=1): err= 0: pid=13: Tue Oct 18 07:55:44 2022 read: IOPS=2997, BW=11.7MiB/s (12.3MB/s)(176MiB/15027msec) bw ( KiB/s): min=11848, max=12136, per=99.92%, avg=11997.87, stdev=44.56, samples=30 iops : min= 2962, max= 3034, avg=2999.47, stdev=11.14, samples=30 cpu : usr=0.24%, sys=1.06%, ctx=13280, majf=0, minf=15 IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u003e=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u003e=64=0.0% issued rwts: total=45049,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=64 Run status group 0 (all jobs): READ: bw=11.7MiB/s (12.3MB/s), 11.7MiB/s-11.7MiB/s (12.3MB/s-12.3MB/s), io=176MiB (185MB), run=15027-15027msec Disk stats (read/write): nvme2n1: ios=53999/2, merge=0/1, ticks=537596/20, in_queue=537796, util=99.46% Testing Write IOPS... write_iops: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64 fio-3.13 Starting 1 process w","date":"2022-10-18","objectID":"/posts/f41ffb/:3:3","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"cStor PV 测试 ","date":"2022-10-18","objectID":"/posts/f41ffb/:4:0","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"创建存储池(cStor storage pools) # kubectl get bd -n openebs NAME NODENAME SIZE CLAIMSTATE STATUS AGE blockdevice-5cc77e185a944270fa46b7d233320660 dev-vm-8c32g-600g-10-86-183-104 53686025728 Unclaimed Active 84m blockdevice-b3978ac55b8d0fc6ae9e2bc3b5b92fd6 dev-vm-8c64g-600g-10-86-16-225 53686025728 Unclaimed Active 84m blockdevice-d93d0a91351a13f4de81fd0a5e6e4540 dev-vm-4c16g-600g-10-86-141-68 53686025728 Unclaimed Active 84m cat \u003c\u003c EOF \u003e cspc.yaml apiVersion: cstor.openebs.io/v1 kind: CStorPoolCluster metadata: name: cstor-disk-pool namespace: openebs spec: pools: - nodeSelector: kubernetes.io/hostname: dev-vm-8c32g-600g-10-86-183-104 dataRaidGroups: - blockDevices: - blockDeviceName: \"blockdevice-5cc77e185a944270fa46b7d233320660\" poolConfig: dataRaidGroupType: \"stripe\" - nodeSelector: kubernetes.io/hostname: dev-vm-8c64g-600g-10-86-16-225 dataRaidGroups: - blockDevices: - blockDeviceName: \"blockdevice-b3978ac55b8d0fc6ae9e2bc3b5b92fd6\" poolConfig: dataRaidGroupType: \"stripe\" - nodeSelector: kubernetes.io/hostname: dev-vm-4c16g-600g-10-86-141-68 dataRaidGroups: - blockDevices: - blockDeviceName: \"blockdevice-d93d0a91351a13f4de81fd0a5e6e4540\" poolConfig: dataRaidGroupType: \"stripe\" EOF kubectl apply -f cspc.yaml blockdevice的CLAIMSTATE状态为Unclaimed才可加入存储池 # kubectl get cspc -n openebs NAME HEALTHYINSTANCES PROVISIONEDINSTANCES DESIREDINSTANCES AGE cstor-disk-pool 3 3 72s # kubectl get cspi -n openebs NAME HOSTNAME FREE CAPACITY READONLY PROVISIONEDREPLICAS HEALTHYREPLICAS STATUS AGE cstor-disk-pool-2v28 dev-vm-8c32g-600g-10-86-183-104 0 0 false 0 0 75s cstor-disk-pool-6mpx dev-vm-8c64g-600g-10-86-16-225 0 0 false 0 0 75s cstor-disk-pool-qmz7 dev-vm-4c16g-600g-10-86-141-68 0 0 false 0 0 74s ","date":"2022-10-18","objectID":"/posts/f41ffb/:4:1","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"创建存储卷声明(StorageClass) cat \u003c\u003c EOF \u003e cstor-csi-disk.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cstor-csi-disk provisioner: cstor.csi.openebs.io allowVolumeExpansion: true parameters: cas-type: cstor # cstorPoolCluster should have the name of the CSPC cstorPoolCluster: cstor-disk-pool # replicaCount should be \u003c= no. of CSPI created in the selected CSPC replicaCount: \"2\" EOF kubectl apply -f cstor-csi-disk.yaml 查看存储卷声明 # kubectl get sc cstor-csi-disk NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cstor-csi-disk cstor.csi.openebs.io Delete Immediate true 8s ","date":"2022-10-18","objectID":"/posts/f41ffb/:4:2","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"创建PVC cat \u003e cstor-pvc.yaml \u003c\u003cEOF --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cstor-pvc spec: storageClassName: cstor-csi-disk accessModes: - ReadWriteOnce resources: requests: storage: 5G EOF kubectl apply -f cstor-pvc.yaml ","date":"2022-10-18","objectID":"/posts/f41ffb/:4:3","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"验证PVC挂载是否正常 cat \u003c\u003cEOF \u003ecstor-pod.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - command: - sh - -c - 'date \u003e\u003e /mnt/openebs-csi/date.txt; hostname \u003e\u003e /mnt/openebs-csi/hostname.txt; sync; sleep 5; sync; tail -f /dev/null;' image: busybox imagePullPolicy: Always name: busybox volumeMounts: - mountPath: /mnt/openebs-csi name: demo-vol volumes: - name: demo-vol persistentVolumeClaim: claimName: cstor-pvc EOF kubectl apply -f cstor-pod.yaml ","date":"2022-10-18","objectID":"/posts/f41ffb/:4:4","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["存储"],"content":"创建压测任务 cat \u003c\u003c EOF \u003e fio-job.yaml # NOTE: For details of params to construct an fio job, refer to this link: # https://fio.readthedocs.io/en/latest/fio_doc.html --- apiVersion: batch/v1 kind: Job metadata: generateName: dbench- spec: template: spec: containers: - name: dbench image: openebs/perf-test:latest imagePullPolicy: IfNotPresent env: ## storage mount point on which testfiles are created - name: DBENCH_MOUNTPOINT value: /data ########################################################## # I/O PROFILE COVERAGE FOR SPECIFIC PERF CHARACTERISTICS # ########################################################## ## quick: {read, write} iops, {read, write} bw (all random) ## detailed: {quick}, {read, write} latency \u0026 mixed 75r:25w (all random), {read, write} bw (all sequential) ## custom: a single user-defined job run with params specified in env 'CUSTOM' - name: DBENCH_TYPE value: detailed #################################################### # STANDARD TUNABLES FOR DBENCH_TYPE=QUICK/DETAILED # #################################################### ## active data size for the bench test - name: FIO_SIZE value: 2G ## use un-buffered i/o (usually O_DIRECT) - name: FIO_DIRECT value: '1' ## no of independent threads doing the same i/o - name: FIO_NUMJOBS value: '1' ## space b/w starting offsets on a file in case of parallel file i/o - name: FIO_OFFSET_INCREMENT value: 250M ## nature of i/o to file. commonly supported: libaio, sync, - name: FIO_IOENGINE value: libaio ## additional runtime options which will be appended to the above params ## ensure options used are not mutually exclusive w/ above params ## ex: '--group_reporting=1, stonewall, --ramptime=\u003cval\u003e etc.., - name: OPTIONS value: '' #################################################### # CUSTOM JOB SPEC FOR DBENCH_TYPE=CUSTOM # #################################################### ## this will execute a single job run with the params specified ## ex: '--bs=16k --iodepth=64 --ioengine=sync --size=500M --name=custom --readwrite=randrw --rwmixread=80 --random_distribution=pareto' - name: CUSTOM value: '' volumeMounts: - name: dbench-pv mountPath: /data restartPolicy: Never volumes: - name: dbench-pv persistentVolumeClaim: claimName: cstor-pvc ## 修改pvc backoffLimit: 4 --- EOF kubectl create -f fio-job.yaml 第一次测试 root@k8s-master-7b4d73fd8d:~# kubectl logs -f dbench-4j6xm-v8xzk Working dir: /data Testing Read IOPS... read_iops: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64 fio-3.13 Starting 1 process read_iops: Laying out IO file (1 file / 2048MiB) read_iops: (groupid=0, jobs=1): err= 0: pid=17: Tue Oct 18 10:44:57 2022 read: IOPS=8859, BW=34.6MiB/s (36.3MB/s)(520MiB/15014msec) bw ( KiB/s): min=33912, max=36200, per=99.98%, avg=35446.50, stdev=496.88, samples=30 iops : min= 8478, max= 9050, avg=8861.60, stdev=124.22, samples=30 cpu : usr=2.42%, sys=9.91%, ctx=127613, majf=0, minf=15 IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u003e=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u003e=64=0.0% issued rwts: total=133015,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=64 Run status group 0 (all jobs): READ: bw=34.6MiB/s (36.3MB/s), 34.6MiB/s-34.6MiB/s (36.3MB/s-36.3MB/s), io=520MiB (545MB), run=15014-15014msec Disk stats (read/write): sda: ios=149875/8, merge=20/18, ticks=1085532/44, in_queue=1086020, util=99.44% Testing Write IOPS... write_iops: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64 fio-3.13 Starting 1 process write_iops: (groupid=0, jobs=1): err= 0: pid=33: Tue Oct 18 10:45:15 2022 write: IOPS=2064, BW=8276KiB/s (8474kB/s)(125MiB/15503msec); 0 zone resets bw ( KiB/s): min= 152, max=13240, per=99.81%, avg=8259.35, stdev=3710.86, samples=31 iops : min= 38, max= 3310, avg=2064.81, stdev=927.72, samples=31 cpu : usr=1.01%, sys=","date":"2022-10-18","objectID":"/posts/f41ffb/:4:5","tags":["openebs"],"title":"OpenEBS性能测试","uri":"/posts/f41ffb/"},{"categories":["性能"],"content":"FIO 介绍 fio 是一个常见的用于测试磁盘 I/O 性能的工具，支持 19 种不同的 I/O 引擎，包括：sync, mmap, libaio, posixaio, SG v3, splice, null, network, syslet, guasi, solarisaio 等等。fio 一直在更新，最新的版本是 v3.19，它的官网是 fio。 常用参数 常用参数 参数说明 参数取值(eg:) name FIO 运行任务的名称 （可选） name=fio-test description FIO 运行时的描述 （可选） description=“this is a test” loops FIO 是否要循环执行该任务 （可选） loops=5 numjobs 每个FIO 任务要开启多少数量的线程 numjobs=8 runtime FIO 任务要执行的时间，单位为s runtime=300 time_based 设置后，即便FIO写完了整个磁盘，也不会退出任务。当任务的时间满足runtime设置的时间后，才会退出任务 time_based startdelay 延迟作业的启动时间。如果指定区间类型的数值，将从区间中选择一个随机值用来启动任务。单位为s startdelay=10 ramp_time FIO 在执行任务时预热的时间。可以使性能测试的结果更加精确可靠。单位为s ramp_time=60 filename 测试文件的名称，通常为硬盘的盘符名称 filename=/dev/sdd pre_read 在FIO下发 I/O 前，文件会被加载到内存中。当 pre_read=true 时会清除掉磁盘中的元数据，默认为false pre_read=false max_open_zones FIO 执行随机写入任务时，允许写入/读取的磁盘的区域的最大数量。 max_open_zones=1000000 direct 是否使用非缓冲I/O。默认为false direct=true readwrite I/O 模式。read、write、randwrite、randread、randrw。混合读写时默认读写比例为50%:50% readwrite=read percentage_random 随机写入时包含多少的随机数据 percentage_random=30% blocksize 读取/写入时块的大小 bs=1M zero_buffers 初始化所有零的缓冲区。默认用随机数填充缓冲区 zero_buffers rwmixread 混合模式中读取所占的百分比 rwmixread=30 rwmixwrite 混合模式中写入所占的百分比 rwmixwrite=70 refill_buffers FIO 在每次提交时重新填充 I/O 缓冲区。使用 zero_buffers 参数后，该参数会失效 refill_buffers buffer_compress_percentage 压缩 I/O 缓冲区的百分比，配合 refill_buffers 可以降低硬盘中相邻块中数据的一致性 buffer_compress_percentage=70 size FIO 执行任务时要读取或写入的数据总和。size为百分比时将按照硬盘容量 * percentage size=300G or size=50% ioengine FIO 任务工具时的引擎，sync, libaio, rdma… ioengine=libaio iodepth 任务执行时的 I/O 队列深度的单元数。任务的线程数 = numjobs * iodepth iodepth=16 命令行使用 # 顺序读 $ fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=1000 -group_reporting -name=mytest # 顺序写 $ fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=1000 -group_reporting -name=mytest # 随机读 $ fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=1000 -group_reporting -name=mytest # 随机写 $ fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=1000 -group_reporting -name=mytest # 混合随机读写 $ fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=100 -group_reporting -name=mytest -ioscheduler=noop 配置文件 # fio.conf [global] ioengine=libaio iodepth=128 direct=0 thread=1 numjobs=16 norandommap=1 randrepeat=0 runtime=60 ramp_time=6 size=1g directory=/your/path [read4k-rand] stonewall group_reporting bs=4k rw=randread [read64k-seq] stonewall group_reporting bs=64k rw=read [write4k-rand] stonewall group_reporting bs=4k rw=randwrite [write64k-seq] stonewall group_reporting bs=64k rw=write ","date":"2022-10-18","objectID":"/posts/4d68ea/:0:1","tags":["fio","存储"],"title":"FIO存储性能工具","uri":"/posts/4d68ea/"},{"categories":["性能"],"content":"测试场景 大文件读写测试 大文件顺序读 fio --name=big-file-sequential-read \\ --directory=/data \\ --rw=read --refill_buffers \\ --bs=256k --size=4G 大文件顺序写 fio --name=big-file-sequential-write \\ --directory=/data \\ --rw=write --refill_buffers \\ --bs=256k --size=4G 大文件并发读 fio --name=big-file-multi-read \\ --directory=/data \\ --rw=read --refill_buffers \\ --bs=256k --size=4G \\ --numjobs={1, 2, 4, 8, 16} 大文件并发写 fio --name=big-file-multi-write \\ --directory=/data \\ --rw=write --refill_buffers \\ --bs=256k --size=4G \\ --numjobs={1, 2, 4, 8, 16} 大文件随机读 fio --name=big-file-rand-read \\ --directory=/data \\ --rw=randread --refill_buffers \\ --size=4G --filename=randread.bin \\ --bs={4k, 16k, 64k, 256k} --pre_read=1 sync \u0026\u0026 echo 3 \u003e /proc/sys/vm/drop_caches fio --name=big-file-rand-read \\ --directory=/data \\ --rw=randread --refill_buffers \\ --size=4G --filename=randread.bin \\ --bs={4k, 16k, 64k, 256k} 大文件随机写 fio --name=big-file-random-write \\ --directory=/data \\ --rw=randwrite --refill_buffers \\ --size=4G --bs={4k, 16k, 64k, 256k} 小文件读写测试 小文件读 fio --name=small-file-seq-read \\ --directory=/data \\ --rw=read --file_service_type=sequential \\ --bs={file_size} --filesize={file_size} --nrfiles=1000 小文件写 fio --name=small-file-seq-write \\ --directory=/data \\ --rw=write --file_service_type=sequential \\ --bs={file_size} --filesize={file_size} --nrfiles=1000 小文件并发读 fio --name=small-file-multi-read \\ --directory=/data \\ --rw=read --file_service_type=sequential \\ --bs=4k --filesize=4k --nrfiles=1000 \\ --numjobs={1, 2, 4, 8, 16} 小文件并发写 fio --name=small-file-multi-write \\ --directory=/data \\ --rw=write --file_service_type=sequential \\ --bs=4k --filesize=4k --nrfiles=1000 \\ --numjobs={1, 2, 4, 8, 16} ","date":"2022-10-18","objectID":"/posts/4d68ea/:0:2","tags":["fio","存储"],"title":"FIO存储性能工具","uri":"/posts/4d68ea/"},{"categories":["性能"],"content":"总结 硬盘性能指标 顺序读写（吞吐量，常用单位为 MB/s）：文件在硬盘上存储位置是连续的。 适用场景：大文件拷贝（比如视频音乐）。速度即使很高，对数据库性能也没有参考价值。 4K 随机读写（IOPS，常用单位为次）：在硬盘上随机位置读写数据，每次 4KB。 适用场景：操作系统运行、软件运行、数据库。 ","date":"2022-10-18","objectID":"/posts/4d68ea/:0:3","tags":["fio","存储"],"title":"FIO存储性能工具","uri":"/posts/4d68ea/"},{"categories":["性能"],"content":"参考资料 官方文档 fio单机性能测试 阿里云测试块存储性能 ","date":"2022-10-18","objectID":"/posts/4d68ea/:0:4","tags":["fio","存储"],"title":"FIO存储性能工具","uri":"/posts/4d68ea/"},{"categories":["云原生","存储"],"content":" 简介 ","date":"2022-10-15","objectID":"/posts/d75507/:0:0","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"OpenEBS 是什么？ OpenEBS 是一种开源云原生存储解决方案，托管于 CNCF 基金会，目前该项目处于沙箱阶段。 OpenEBS 是一组存储引擎，允许您为有状态工作负载 (StatefulSet) 和 Kubernetes 平台类型选择正确的存储解决方案。在高层次上，OpenEBS 支持两大类卷——本地卷和复制卷。 OpenEBS 是 Kubernetes 本地超融合存储解决方案，它管理节点可用的本地存储，并为有状态工作负载提供本地或高可用的分布式持久卷。作为一个完全的 Kubernetes 原生解决方案的另一个优势是，管理员和开发人员可以使用 kubectl、Helm、 Prometheus、Grafana、Weave Scope 等 Kubernetes 可用的所有优秀工具来交互和管理 OpenEBS。 ","date":"2022-10-15","objectID":"/posts/d75507/:0:1","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"OpenEBS 能做什么？ OpenEBS 管理 k8s 节点上存储，并为 k8s 有状态负载（StatefulSet）提供本地存储卷或分布式存储卷。 本地卷（Local Storage） OpenEBS 可以使用宿主机裸块设备或分区，或者使用 Hostpaths 上的子目录，或者使用 LVM、ZFS 来创建持久化卷 本地卷直接挂载到 Stateful Pod 中，而不需要 OpenEBS 在数据路径中增加任何开销 OpenEBS 为本地卷提供了额外的工具，用于监控、备份 / 恢复、灾难恢复、由 ZFS 或 LVM 支持的快照等 对于分布式卷 (即复制卷) OpenEBS 使用其中一个引擎 (Mayastor、cStor 或 Jiva) 为每个分布式持久卷创建微服务 有状态 Pod 将数据写入 OpenEBS 引擎，OpenEBS 引擎将数据同步复制到集群中的多个节点。OpenEBS 引擎本身作为 Pod 部署，并由 Kubernetes 进行协调。当运行 Stateful Pod 的节点失败时，Pod 将被重新调度到集群中的另一个节点，OpenEBS 将使用其他节点上的可用数据副本提供对数据的访问 有状态的 Pods 使用 iSCSI (cStor 和 Jiva) 或 NVMeoF (Mayastor) 连接 OpenEBS 分布式持久卷 OpenEBS cStor 和 Jiva 专注于存储的易用性和持久性。它们分别使用自定义版本的 ZFS 和 Longhorn 技术将数据写入存储。OpenEBS Mayastor 是最新开发的以耐久性和性能为设计目标的引擎，高效地管理计算 (大页面、核心) 和存储 (NVMe Drives)，以提供快速分布式块存储 ❝注意：OpenEBS 分布式块卷被称为复制卷，以避免与传统的分布式块存储混淆，传统的分布式块存储倾向于将数据分布到集群中的许多节点上。复制卷是为云原生有状态工作负载设计的，这些工作负载需要大量的卷，这些卷的容量通常可以从单个节点提供，而不是使用跨集群中的多个节点分片的单个大卷 ","date":"2022-10-15","objectID":"/posts/d75507/:0:2","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"对比传统分布式存储 OpenEBS 与其他传统存储解决方案不同的几个关键方面 : 使用微服务体系结构构建，就像它所服务的应用程序一样。OpenEBS 本身作为一组容器部署在 Kubernetes 工作节点上。使用 Kubernetes 本身来编排和管理 OpenEBS 组件 完全建立在用户空间，使其高度可移植性，以运行在任何操作系统 / 平台。 完全意图驱动，继承了 Kubernetes 易用性的相同原则 OpenEBS 支持一系列存储引擎，因此开发人员可以部署适合于其应用程序设计目标的存储技术。像 Cassandra 这样的分布式应用程序可以使用 LocalPV 引擎进行最低延迟的写操作。像 MySQL 和 PostgreSQL 这样的单片应用程序可以使用使用 NVMe 和 SPDK 构建的 Mayastor 或基于 ZFS 的 cStor 来实现弹性。像 Kafka 这样的流媒体应用程序可以在边缘环境中使用 NVMe 引擎 Mayastor 以获得最佳性能。 驱使用户使用 OpenEBS 的主要原因是 : 在所有的 Kubernetes 发行版上都是可移植的 提高了开发人员和平台 SRE 的生产力 与其他解决方案相比，易于使用 优秀的社区支持 免费开源 ","date":"2022-10-15","objectID":"/posts/d75507/:0:3","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"本地卷类型 本地卷只能从集群中的单个节点访问。必须在提供卷的节点上调度使用 Local Volume 的 Pods。本地卷通常是分布式工作负载的首选，比如 Cassandra、MongoDB、Elastic 等，这些工作负载本质上是分布式的，并且内置了高可用性（分片） 根据附加到 Kubernetes 工作节点上的存储类型，您可以从不同的动态本地 PV 进行选择——Hostpath、Device、LVM、ZFS 或 Rawfile ","date":"2022-10-15","objectID":"/posts/d75507/:0:4","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"可复制卷类型 复制卷顾名思义，是指将数据同步复制到多个节点的卷。卷可以支持节点故障。还可以跨可用性区域设置复制，以帮助应用程序跨可用性区域移动。 复制卷还能够提供像快照、克隆、卷扩展等企业存储特性。复制卷是有状态工作负载 (如 Percona/MySQL、Jira、GitLab 等) 的首选。 根据附加到 Kubernetes 工作节点的存储类型和应用程序性能需求，您可以从 Jiva、cStor 或 Mayastor 中进行选择 ","date":"2022-10-15","objectID":"/posts/d75507/:0:5","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"OpenEBS 存储引擎建议 应用需求 存储类型 OpenEBS 卷类型 低时延、高可用性、同步复制、快照、克隆、精简配置 SSD/ 云存储卷 OpenEBS Mayastor 高可用性、同步复制、快照、克隆、精简配置 机械 /SSD/ 云存储卷 OpenEBS cStor 高可用性、同步复制、精简配置 主机路径或外部挂载存储 OpenEBS Jiva 低时延、本地 PV 主机路径或外部挂载存储 Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile 低时延、本地 PV 本地机械 /SSD/ 云存储卷等块设备 Dynamic Local PV - Device 低延迟，本地 PV，快照，克隆 本地机械 /SSD/ 云存储卷等块设备 OpenEBS Dynamic Local PV - ZFS , OpenEBS Dynamic Local PV - LVM 总结： 多机环境，如果有额外的块设备（非系统盘块设备）作为数据盘，选用 OpenEBS Mayastor、OpenEBS cStor 多机环境，如果没有额外的块设备（非系统盘块设备）作为数据盘，仅单块系统盘块设备，选用 OpenEBS Jiva 单机环境，建议本地路径 Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile，由于单机多用于测试环境，数据可靠性要求较低。 由此看来，OpenEBS 常用场景为以上三个场景 ","date":"2022-10-15","objectID":"/posts/d75507/:0:6","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"OpenEBS 特性 ","date":"2022-10-15","objectID":"/posts/d75507/:0:7","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"容器附加存储 OpenEBS 是一个容器附加存储 (Container Attached Storage, CAS) 的例子。通过 OpenEBS 提供的卷总是被容器化。每个卷都有一个专用的存储控制器，用于提高有状态应用程序的持久性存储操作的敏捷性和粒度。 ","date":"2022-10-15","objectID":"/posts/d75507/:0:8","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"同步复制 同步复制是 OpenEBS 的一个可选的流行特性。当与 Jiva、cStor 和 Mayastor 存储引擎一起使用时，OpenEBS 可以同步复制数据卷以实现高可用性。跨 Kubernetes 区域进行复制，从而为跨 AZ 设置提供高可用性。这个特性对于使用 GKE、EKS 和 AKS 等云提供商服务上的本地磁盘构建高可用状态应用程序特别有用 ","date":"2022-10-15","objectID":"/posts/d75507/:0:9","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"快照和克隆 写时拷贝快照是 OpenEBS 另一个可选的流行特性。使用 cStor 引擎时，快照是瞬时创建的，并且不受快照个数的限制。增量快照功能增强了跨 Kubernetes 集群和跨不同云提供商或数据中心的数据迁移和可移植性。对快照和克隆的操作完全以 Kubernetes 原生方法执行，使用标准 kubectl 命令。常见的用例包括用于备份的高效复制和用于故障排除或针对数据的只读副本进行开发的克隆 ","date":"2022-10-15","objectID":"/posts/d75507/:0:10","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"备份和恢复 OpenEBS 卷的备份和恢复可以通过开源的 OpenEBS Velero 插件与 Kubernetes 备份和恢复解决方案 (如 Velero(前身为 Heptio Ark)) 协同工作。经常使用 OpenEBS 增量快照功能，将数据备份到 AWS S3、GCP object storage、MinIO 等对象存储目标。这种存储级别的快照和备份只使用增量数据进行备份，节省了大量的带宽和存储空间。 ","date":"2022-10-15","objectID":"/posts/d75507/:0:11","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"真正的 Kubernetes 云原生存储 OpenEBS 是 Kubernetes 上有状态应用程序的云原生存储，云原生意味着遵循松散耦合的体系结构。因此，云原生、松散耦合体系结构的一般好处是适用的。例如，开发人员和 DevOps 架构师可以使用标准的 Kubernetes 技能和实用程序来配置、使用和管理持久存储需求 ","date":"2022-10-15","objectID":"/posts/d75507/:0:12","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"减少存储 TCO 高达 50% 在大多数云上，块存储的收费是基于购买的多少，而不是使用的多少 ; 为了实现更高的性能，并在充分利用容量时消除中断的风险，容量经常被过度配置。OpenEBS 的精简配置能力可以共享本地存储或云存储，然后根据需要增加有状态应用程序的数据量。可以动态添加存储，而不会中断暴露给工作负载或应用程序的卷。某些用户报告说，由于使用了 OpenEBS的精简配置，节省了超过 60% 的资源。 ","date":"2022-10-15","objectID":"/posts/d75507/:0:13","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"高可用性 由于 OpenEBS 遵循 CAS 架构，在节点故障时，Kubernetes 将重新调度 OpenEBS 控制器，而底层数据则通过使用一个或多个副本来保护。更重要的是——因为每个工作负载都可以利用自己的 OpenEBS——不存在因存储丢失而导致系统大范围宕机的风险。例如，卷的元数据不是集中的，它可能会像许多共享存储系统那样受到灾难性的通用中断的影响。相反，元数据保持在卷的本地。丢失任何节点都会导致只存在于该节点上的卷副本的丢失。由于卷数据至少在其他两个节点上进行了同步复制，因此当一个节点出现故障时，这些数据将在相同的性能级别上继续可用 ","date":"2022-10-15","objectID":"/posts/d75507/:0:14","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"CAS 介绍 在 CAS 或容器附加存储 (Container Attached Storage) 体系结构中，存储在容器中运行，并且与存储绑定到的应用程序密切相关。存储作为微服务运行，没有内核模块依赖关系。像 Kubernetes 这样的编排系统编排存储卷，就像任何其他微服务或容器一样。CAS 具有 DAS 和 NAS 的优点 ","date":"2022-10-15","objectID":"/posts/d75507/:0:15","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"非 CAS 系统上的 PV 在非 CAS 模型中，Kubernetes 持久卷仍然与内核模块紧密耦合，使得 Kubernetes 节点上的存储软件本质上是单片的 ","date":"2022-10-15","objectID":"/posts/d75507/:0:16","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"基于 CAS 系统上的 PV 相反，CAS 使您能够利用云原生应用程序的灵活性和可伸缩性。定义 Kubernetes PV (Persistent Volume) 的存储软件是基于微服务架构的。存储软件的控制平面 (存储控制器) 和数据平面 (存储副本) 作为 Kubernetes Pods运行，因此，使您能够将云原生的所有优势应用到 CAS。 ","date":"2022-10-15","objectID":"/posts/d75507/:0:17","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"CAS 优势 敏捷 CAS 中的每个存储卷都有一个容器化的存储控制器和相应的容器化副本。因此，围绕这些组件的资源的维护和调优是真正敏捷的。Kubernetes 滚动升级功能可以实现存储控制器和存储副本的无缝升级。可以使用容器 cGroups 调优 CPU和内存等资源配额。 存储策略粒度化 将存储软件容器化并将存储控制器专用于每个卷可以带来最大的存储策略粒度。在 CAS 体系结构中，可以按卷配置所有存储策略。此外，您可以监视每个卷的存储参数，并动态更新存储策略，以实现每个工作负载的预期结果。随着卷存储策略中这种额外粒度级别的增加，存储吞吐量、IOPS 和延迟的控制也会增加。 云原生 CAS 将存储软件装入容器，并使用 Kubernetes 自定义资源定义 (CRDs) 来声明低级存储资源，如磁盘和存储池。这个模型使存储能够无缝地集成到其他云原生工具中。可以使用 Prometheus、Grafana、Fluentd、Weavescope、Jaeger 等云原生工具来供应、监控和管理存储资源 PV 是 CAS 中的一个微服务 如上图所示，在 CAS 架构中，存储控制器和副本的软件完全是基于微服务的，因此不涉及内核组件。通常，存储控制器 POD 被调度在与持久卷相同的节点上，以提高效率，副本 POD 可以被调度在集群节点上的任何位置。每个副本使用本地磁盘、SAN 磁盘和云磁盘的任意组合完全独立于其他副本进行配置。这为大规模管理工作负载的存储分配提供了巨大的灵活性。 超融合非分布式 CAS 架构没有遵循典型分布式存储架构。通过从存储控制器到存储副本的同步复制，存储变得高度可用。卷副本的元数据不是在节点之间共享的，而是在每个本地节点上独立管理。如果一个节点故障，存储控制器 (在本例中是一个无状态容器) 将在一个节点上轮转，该节点上运行着第二个或第三个副本，数据仍然可用。 与超融合系统类似，CAS 中的卷的存储和性能是可扩展的。由于每个卷都有自己的存储控制器，因此存储可以在一个节点的存储容量允许的范围内进行扩展。在给定的 Kubernetes 集群中，随着容器应用程序数量的增加，会增加更多的节点，从而提高存储容量和性能的整体可用性，从而使存储对新的应用程序容器可用。这一过程与 Nutanix 等成功的超融合系统非常相似。 ","date":"2022-10-15","objectID":"/posts/d75507/:0:18","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"OpenESB 架构介绍 OpenESB 遵循容器附加存储（CAS）模型，每个卷都有一个专用的控制器 POD 和一组副本 POD。CAS 体系结构的优点在**`CNCF` 博客[1]** 上进行了讨论。OpenEBS 操作和使用都很简单，因为它看起来和感觉上就像其他云原生和 Kubernetes 友好的项目。 OpenEBS 有许多组件，可以分为以下类别 : 控制面组件 - Provisioner, API Server, volume exports,volume sidecars 数据面组件 - Jiva、cStor 节点磁盘管理器 - Discover, monitor, 管理连接 k8s 的媒介 与云原生工具的集成 - 已经与 Prometheus,Grafana, Fluentd、Jaeger 集成 ","date":"2022-10-15","objectID":"/posts/d75507/:1:0","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"控制面 OpenEBS集群的控制平面通常被称为Maya。 OpenEBS 控制平面负责提供卷、相关的卷操作，如快照、克隆、创建存储策略、执行存储策略、导出 Prometheus/grafana 使用的卷指标，等等。 OpenEBS 提供了一个动态提供程序，这是 Kubernetes 的标准外部存储插件。OpenEBS PV 提供者的主要任务是向应用程序 PODS 启动卷供应，并实现 PV 的 Kubernetes 规范。 m-apiserver 开放存储的 REST API，并承担大量卷策略处理和管理工作。 控制平面和数据平面之间的连通性使用 Kubernetes sidecar 模式。控制平面需要与数据平面通信的场景如下所示。 对于卷的统计，如 IOPS，吞吐量，延迟等–通过卷暴漏的 sidecar 实现 使用卷控制器 pod 执行卷策略，使用卷副本 pod 进行磁盘 / 池管理-通过卷管理 sidecar 实现 ","date":"2022-10-15","objectID":"/posts/d75507/:1:1","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"OpenEBS PV Provisioner 此组件作为 POD 运行，并做出配置决策 , 它的使用方式是 : 开发人员用所需的卷参数构造一个声明，选择适当的存储类，并在 YAML 规范上调用 kubelet。OpenEBS PV 动态提供程序与 maya-apiserver 交互，在适当的节点上为卷控制器 pod 和卷副本 pod 创建部署规范。可以使用 PVC 规范中的注释来控制卷 Pod(控制器 / 副本) 的调度。 目前，OpenEBS Provisioner 只支持一种绑定类型，即 iSCSI。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:2","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Maya-ApiServer m-apiserver作为POD运行。顾名思义，m-apiserver公开OpenEBS REST api`。 m-apiserver 还负责创建创建卷 pod 所需的部署规范文件。在生成这些规范文件之后，它将调用 kube-apiserver 来相应地调度这些 pods。OpenEBS PV 提供者在卷发放结束时，将创建一个 PV 对象并将其挂载到应用程序 pod 上。PV 由控制器 pod 承载，控制器 pod 由不同节点中的一组副本 pod 支持。控制器 pod 和复制 pod 是数据平面的一部分，在存储引擎部分有更详细的描述。 m-apiserver 的另一个重要任务是卷策略管理。OpenEBS 为表示策略提供了非常细粒度的规范。m-apiserver 解释这些 YAML 规范，将它们转换为可执行的组件，并通过容量管理 sidecar 来执行它们 ","date":"2022-10-15","objectID":"/posts/d75507/:1:3","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Maya Volume Exporter Maya卷导出器是每个存储控制器pod的sidecar`。 这些 sidecar 将控制平面连接到数据平面以获取统计信息。统计信息的粒度在卷级别。一些统计数据示例如下： 卷读取延迟 卷写入延迟 卷每秒读取速度 卷每秒写入速度 读取块大小 写入块大小 容量统计 这些统计信息通常由 Prometheus 客户端来拉取，该客户端在 OpenBS 安装期间安装和配置。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:4","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"卷管理 sidecar Sidecars 还用于将控制器配置参数和卷策略传递给卷控制器 pod(卷控制器 pod 是一个数据平面)， 并将副本配置参数和副本数据保护参数传递给卷副本 pod。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:5","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"数据面 OpenEBS 数据平面负责实际的卷 IO 路径。存储引擎在数据平面实现实际的 IO 路径。目前，OpenEBS 提供了两个可以轻松插入的存储引擎。它们被称为 Jiva 和 cStor。这两个存储引擎都完全运行在 Linux 用户空间中，并基于微服务架构。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:6","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Jiva Jiva 存储引擎基于 Rancher's LongHorn 与 gotgt 开发实现， 使用 go 语言开发，并运行于用户命名空间下。LongHorn 控制器将输入的 IO 同步复制到 LongHorn 副本。该副本将 Linux 稀疏文件视为构建存储特性 (如精简配置、快照、重建等) 的基础。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:7","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"cStor cStor 数据引擎使用 C 语言编写，具有高性能的 iSCSI target 和 Copy-On-Write 块系统，提供数据完整性、数据弹性和时间点的快照和克隆。cStor 有一个池特性，它以条带、镜像或 RAIDZ 模式聚合一个节点上的磁盘，以提供更大的容量和性能单位。cStor 还可以跨区域将数据同步复制到多个节点，从而避免节点丢失或节点重启导致数据不可用。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:8","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"LocalPV 对于那些不需要存储级复制的应用程序，LocalPV 可能是很好的选择，因为它能提供更高的性能。OpenEBS LocalPV与 Kubernetes LocalPV 类似，不同之处在于它是由 OpenEBS 控制平面动态提供的， 就像任何其他常规 PV一样。OpenEBS LocalPV 有两种类型 :hostpath LocalPV 和 device LocalPV。hostpath LocalPV 指的是主机上的子目录，LocalPV 指的是在节点上发现的磁盘 (可以是直接连接的，也可以是网络连接的)。OpenEBS引入了一个 LocalPV 提供者，用于根据 PVC 和存储类规范中的一些标准选择匹配的磁盘或主机路径。 ","date":"2022-10-15","objectID":"/posts/d75507/:1:9","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"节点磁盘管理器 节点磁盘管理器 (NDM) 填补了使用 Kubernetes 管理有状态应用程序的持久存储所需的工具链的空白。容器时代的 DevOps 架构师必须以一种自动化的方式满足应用程序和应用程序开发人员的基础设施需求， 这种方式可以跨环境提供弹性和一致性。这些要求意味着存储堆栈本身必须非常灵活， 以便 Kubernetes 和云原生生态系统中的其他软件可以轻松地使用这个堆栈。NDM 在 Kubernetes 的存储堆栈中起着基础性的作用，它统一了不同的磁盘， 并通过将它们标识为 Kubernetes 对象，提供了将它们汇聚的能力。此外，NDM 发现、提供、监视和管理底层磁盘的方式，可以让 Kubernetes PV 提供者 (如 OpenEBS 和其他存储系统) 和 Prometheus 管理磁盘子系统 ","date":"2022-10-15","objectID":"/posts/d75507/:1:10","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"CAS 引擎 ","date":"2022-10-15","objectID":"/posts/d75507/:2:0","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"存储引擎概述 存储引擎是持久化卷 IO 路径的数据平面组件。在 CAS 架构中，用户可以根据不同的配置策略，为不同的应用工作负载选择不同的数据平面。存储引擎可以通过特性集或性能优化给定的工作负载。 操作员或管理员通常选择具有特定软件版本的存储引擎，并构建优化的卷模板， 这些卷模板根据底层磁盘的类型、弹性、副本数量和参与 Kubernetes 集群的节点集进行微调。用户可以在发放卷时选择最优的卷模板，从而在给定的 Kubernetes 集群上为所有存储卷运行最优的软件和存储组合提供最大的灵活性。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:1","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"存储引擎类型 OpenEBS 提供了三种存储引擎 Jiva Jiva 是 OpenEBS 0.1 版中发布的第一个存储引擎，使用起来最简单。它基于 GoLang 开发，内部使用 LongHorn 和 gotgt 堆栈。 Jiva 完全在用户空间中运行，并提供同步复制等标准块存储功能。 Jiva 通常适用于容量较小的工作负载，不适用于大量快照和克隆特性是主要需求的情况 cStor cStor 是 OpenEBS 0.7 版本中最新发布的存储引擎。cStor 非常健壮，提供数据一致性，很好地支持快照和克隆等企业存储特性。 它还提供了一个健壮的存储池特性，用于在容量和性能方面进行全面的存储管理。 cStor 与 NDM (Node Disk Manager) 一起，为 Kubernetes 上的有状态应用程序提供完整的持久化存储特性 OpenEBS Local PV OpenEBS Local PV 是一个新的存储引擎，它可以从本地磁盘或工作节点上的主机路径创建持久卷或 PV。 CAS引擎可以从 OpenEBS 的 1.0.0 版本中获得。使用 OpenEBS Local PV，性能将等同于创建卷的本地磁盘或文件系统 (主机路径)。 许多云原生应用程序可能不需要复制、快照或克隆等高级存储特性，因为它们本身就提供了这些特性。这类应用程序需要以持久卷的形式访问管理的磁盘 SP 存储池，表示 Jiva 自定义存储资源 CV cStor 卷，表示 cStor 卷自定义资源 CVR cStor 卷副本 SPC 存储池声明，表示 cStor 池聚合的自定义资源 CSP cStor 存储池，表示 cStor Pool 每个节点上的自定义资源 一个 SPC 对应多个 CSP，相应的一个 CV 对应多个 CVR ","date":"2022-10-15","objectID":"/posts/d75507/:2:2","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"存储引擎声明 通过指定注释 openebs 来选择存储引擎。StorageClass 规范中的 io/cas-type。StorageClass 定义了提供程序的细节。为每个 CAS 引擎指定单独的供应程序。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:3","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"cStor 存储类规范文件内容 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cStor-storageclass annotations: openebs.io/cas-type: cstor cas.openebs.io/config: | - name: StoragePoolClaim value: \"cStorPool-SSD\" provisioner: openebs.io/provisioner-iscsi --- ","date":"2022-10-15","objectID":"/posts/d75507/:2:4","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Jiva 存储类规范文件内容 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: jiva-storageclass annotations: openebs.io/cas-type: jiva cas.openebs.io/config: | - name: StoragePool value: default provisioner: openebs.io/provisioner-iscsi --- 当 cas 类型为 Jiva 时，StoragePool 的 default 值具有特殊含义。当 pool 为默认值时，Jiva 引擎将从容器 (副本 pod) 本身的存储空间中为副本 pod 开辟数据存储空间。当所需的卷大小很小 (比如 5G 到 10G) 时，StoragePool default 工作得很好，因为它可以容纳在容器本身内。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:5","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Local PV 存储类规范文件内容-主机路径 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localpv-hostpath-sc annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: BasePath value: \"/var/openebs/local\" - name: StorageType value: \"hostpath\" provisioner: openebs.io/local --- ","date":"2022-10-15","objectID":"/posts/d75507/:2:6","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Local PV 存储类规范文件内容-主机设备 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localpv-device-sc annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: StorageType value: \"device\" - name: FSType value: ext4 provisioner: openebs.io/local --- cStor、Jiva、LocalPV 特性比较： 特性 Jiva cStor Local PV 轻量级运行于用户空间 Yes Yes Yes 同步复制 Yes Yes No 适合低容量工作负载 Yes Yes Yes 支持快照，克隆 Basic Advanced No 数据一致性 Yes Yes NA 使用 Velero 恢复备份 Yes Yes Yes 适合高容量工作负载 No Yes Yes 自动精简配置 Yes No 磁盘池或聚合支持 Yes No 动态扩容 Yes Yes 数据弹性 (RAID 支持) Yes No 接近原生磁盘性能 No No Yes 大多数场景推荐 cStor，因其提供了强大的功能，包括快照 / 克隆、存储池功能（如精简资源调配、按需扩容等）。 Jiva 适用于低容量需求的工作负载场景，例如 5 到 50G。尽管使用 Jiva 没有空间限制，但建议将其用于低容量工作负载。Jiva 非常易于使用，并提供企业级容器本地存储，而不需要专用硬盘。有快照和克隆功能的需求的场景，优先考虑使用 cStor 而不是 Jiva。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:7","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"CAS 引擎使用场景 如上表所示，每个存储引擎都有自己的优势。选择引擎完全取决于应用程序的工作负载以及它当前和未来的容量和 / 或性能增长。下面的指导原则在定义存储类时为选择特定的引擎提供一些帮助。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:8","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"选择 cStor 的理想条件 当需要同步复制数据并在节点上有多个磁盘时 当您从每个节点上的本地磁盘或网络磁盘池管理多个应用程序的存储时。通过精简配置、存储池和卷的按需扩容、存储池的性能按需扩容等特性，实现对存储层的管理。cStor 用于在本地运行的 Kubernetes 集群上构建 Kubernetes 本地存储服务，类似于 AWS EBS 或谷歌 PD。 当需要存储级快照和克隆能力时 当您需要企业级存储保护特性，如数据一致性、弹性 (RAID 保护)。 如果您的应用程序不需要存储级复制，那么使用 OpenEBS 主机路径 LocalPV 或 OpenEBS 设备 LocalPV 可能是更好的选择。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:9","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"选择 Jiva 的理想条件 : 当您想要数据的同步复制，并且拥有单个本地磁盘或单个管理磁盘 (如云磁盘 (EBS、GPD))，并且不需要快照或克隆特性时 Jiva 是最容易管理的，因为磁盘管理或池管理不在这个引擎的范围内。Jiva 池是本地磁盘、网络磁盘、虚拟磁盘或云磁盘的挂载路径。 以下场景 Jiva 更优于 cStor : 当程序不需要存储级的快照、克隆特性 当节点上没有空闲磁盘时。Jiva 可以在主机目录上使用，并且仍然可以实现复制。 当不需要动态扩展本地磁盘上的存储时。将更多磁盘添加到 Jiva 池是不可能的，因此 Jiva 池的大小是固定的，如果它在物理磁盘上。但是，如果底层磁盘是虚拟磁盘、网络磁盘或云磁盘，则可以动态地更改 Jiva 池的大小 容量需求较小。大容量应用通常需要动态增加容量，cStor 更适合这种需求 ","date":"2022-10-15","objectID":"/posts/d75507/:2:10","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"选择 OpenEBS 主机路径 LocalPV 的理想条件 : 当应用程序本身具备管理复制能力（例如：es）时，不需要在存储层进行复制。在大多数这样的情况下，应用程序是作为 statefulset 部署的 高于 Jiva 与 cStor 的读写性能需求 当特定应用程序没有专用的本地磁盘或特定应用程序不需要专用的存储时，建议使用 Hostpath。如果您想跨多个应用程序共享一个本地磁盘，主机路径 LocalPV 是正确的方法 ","date":"2022-10-15","objectID":"/posts/d75507/:2:11","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"选择 OpenEBS 主机设备 LocalPV 的理想条件 : 当应用程序管理复制本身，不需要在存储层进行复制时。在大多数这种情况下，应用程序被部署为有状态集 高于 Jiva 与 cStor 的读写性能需求 高于 OpenEBS 主机路径 LocalPV 的读写性能需求 当需要接近磁盘性能时。该卷专用于写入单个 SSD 或 NVMe 接口以获得最高性能 ","date":"2022-10-15","objectID":"/posts/d75507/:2:12","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"总结 如果应用程序处于生产中，并且不需要存储级复制，那么首选 LocalPV 如果您的应用程序处于生产状态，并且需要存储级复制，那么首选 cStor 如果应用程序较小，需要存储级复制，但不需要快照或克隆，则首选 Jiva ","date":"2022-10-15","objectID":"/posts/d75507/:2:13","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"节点磁盘管理器（NDM） 节点磁盘管理器 (NDM) 是 OpenEBS 体系结构中的一个重要组件。NDM 将块设备视为需要监视和管理的资源，就像 CPU、内存和网络等其他资源一样。它是一个在每个节点上运行的守护进程，基于过滤器检测附加的块设备，并将它们作为块设备自定义资源加载到 Kubernetes 中。这些定制资源旨在通过提供类似于 : 轻松访问 Kubernetes 集群中可用的块设备清单 预测磁盘的故障，以帮助采取预防措施 允许动态地将磁盘挂载 / 卸载到存储 Pod 中，而无需重新启动在磁盘挂载 / 卸载的节点上运行的相应 NDM Pod 尽管做了上述所有工作，NDM 还是有助于提供持久卷的总体简化。 NDM 是在 OpenEBS 安装期间作为守护进程部署的。NDM daemonset 发现每个节点上的磁盘，并创建一个名为 Block Device 或 BD 的自定义资源。 ","date":"2022-10-15","objectID":"/posts/d75507/:2:14","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"访问权限说明 NDM 守护进程运行在容器中，必须访问底层存储设备并以特权模式运行。NDM 需要特权模式，因为它需要访问 /dev、/proc 和 /sys 目录来监视附加设备，还需要使用各种探测器获取附加设备的详细信息。NDM 负责发现块设备并过滤掉不应该被 OpenEBS 使用的设备 ; 例如，检测有 OS 文件系统的磁盘。NDM pod 默认情况下在容器中挂载主机的 /proc 目录，然后加载 /proc/1/mounts，以找到操作系统使用的磁盘 ","date":"2022-10-15","objectID":"/posts/d75507/:2:15","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"NDM 守护程序功能 发现 Kubernetes 节点上的块设备 在启动时发现块设备-创建和 / 或更新状态。 维护集群范围内磁盘的唯一 id: 对 WWN / PartitionUUID / FileSystemUUID / DeviceMapperUUID 进行 Hash计算 检测节点中添加 / 移除块设备，并更新块设备状态 添加块设备作为 Kubernetes 自定义资源，具有以下属性： Active : 节点上存在块设备 Inactive : 给定节点上不存在块设备 Unknown : NDM 在块设备最后被检测到的节点上停止 / 无法确定状态 主机名称 (kubernetes.io/hostname) 块设备类型 (ndm.io/blockdevice-type) Managed (ndm.io/managed) 设备路径 设备链接 供应商和型号信息 WWN 和序列号 容量 扇区和区块大小 spec: 如果可用，将更新以下内容 labels: status: 状态可以有以下值 ","date":"2022-10-15","objectID":"/posts/d75507/:2:16","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"过滤器 为要创建块设备 CR 的块设备类型配置过滤器。过滤器可以通过供应商类型、设备路径模式或挂载点进行配置 过滤器可以是包含过滤器，也可以是排除过滤器。它们被配置为 configmap。管理员用户可以在 OpenEBS 安装时通过更改 OpenEBS 操作员 yaml 文件或 helm 值中的 NDM configmap 来配置这些过滤器。yaml 文件。如果这些过滤器需要在安装后更新，那么可以遵循以下方法之一 : 使用 operator 方式安装 OpenEBS。在 Yaml 文件中，更新 configmap 中的过滤器并应用 operator.yaml 如果 OpenEBS 是使用 helm 安装的，更新 values.yaml 中的 configmap 并使用 helm 进行升级 或者，使用 kubectl 编辑 NDM configmap，更新过滤器 ","date":"2022-10-15","objectID":"/posts/d75507/:2:17","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"落地实践 OpenEBS 的 cStor 与 Jiva 引擎由于组件过多，配置相较其他存储方案繁琐，生产环境不建议使用，这里我们仅论述 Local PV 引擎。 Local PV 引擎不具备存储级复制能力，适用于 k8s 有状态服务的后端存储（如 : es、redis 等） ","date":"2022-10-15","objectID":"/posts/d75507/:3:0","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Local PV Hostpath 实践 对比 Kubernetes Hostpath 卷相比，OpenEBS 本地 PV Hostpath 卷具有以下优势 : OpenEBS 本地 PV Hostpath 允许您的应用程序通过 StorageClass、PVC 和 PV 访问 Hostpath。这为您提供了更改 PV 提供者的灵活性，而无需重新设计应用程序 YAML 使用 Velero 备份和恢复进行数据保护 通过对应用程序 YAML 和 pod 完全屏蔽主机路径来防范主机路径安全漏洞 环境依赖 : k8s 1.12 以上 OpenEBS 1.0 以上 实践环境 : docker 19.03.8 k8s 1.18.6 CentOS7 $ kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 8m8s v1.18.6 node2 Ready master,worker 7m15s v1.18.6 node3 Ready master,worker 7m15s v1.18.6 ","date":"2022-10-15","objectID":"/posts/d75507/:3:1","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建数据目录 在将要创建 Local PV Hostpaths 的节点上设置目录。这个目录将被称为 BasePath。默认位置是 /var/openebs/local 节点 node1、node2、node3 创建 /data/openebs/local 目录 （/data 可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统 ‘/’ 挂载点存储空间） $ mkdir -p /data/openebs/local ","date":"2022-10-15","objectID":"/posts/d75507/:3:2","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"下载应用描述文件 ","date":"2022-10-15","objectID":"/posts/d75507/:3:3","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"发布 openebs 应用 根据上述配置文件，保证 k8s 集群可访问到如下镜像（建议导入本地私有镜像库，如 : harbor） openebs/node-disk-manager:1.5.0 openebs/node-disk-operator:1.5.0 openebs/provisioner-localpv:2.10.0 更新 openebs-operator.yaml 中镜像 tag 为实际 tag image: openebs/node-disk-manager:1.5.0 image: openebs/node-disk-operator:1.5.0 image: openebs/provisioner-localpv:2.10.0 发布 $ kubectl apply -f openebs-operator.yaml 查看发布状态 $ kubectl get pod -n openebs -w NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-6d6d9cfc99-4sltp 1/1 Running 0 10s openebs-ndm-85rng 1/1 Running 0 10s openebs-ndm-operator-7df6668998-ptnlq 0/1 Running 0 10s openebs-ndm-qgqm9 1/1 Running 0 10s openebs-ndm-zz7ps 1/1 Running 0 10s ","date":"2022-10-15","objectID":"/posts/d75507/:3:4","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建存储类 更改配置文件中的内容 value: \"/var/openebs/local/\" 发布创建存储类 $ cat \u003e openebs-hostpath-sc.yaml \u003c\u003cEOF apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-hostpath annotations: openebs.io/cas-type: local cas.openebs.io/config: | #hostpath type will create a PV by # creating a sub-directory under the # BASEPATH provided below. - name: StorageType value: \"hostpath\" #Specify the location (directory) where # where PV(volume) data will be saved. # A sub-directory with pv-name will be # created. When the volume is deleted, # the PV sub-directory will be deleted. #Default value is /var/openebs/local - name: BasePath value: \"/data/openebs/local/\" provisioner: openebs.io/local volumeBindingMode: WaitForFirstConsumer reclaimPolicy: Delete EOF $ kubectl apply -f openebs-hostpath-sc.yaml ","date":"2022-10-15","objectID":"/posts/d75507/:3:5","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建 PVC 验证可用性 $ cat \u003e local-hostpath-pvc.yaml \u003c\u003cEOF kind: PersistentVolumeClaim apiVersion: v1 metadata: name: local-hostpath-pvc spec: storageClassName: openebs-hostpath accessModes: - ReadWriteOnce resources: requests: storage: 5G EOF $ kubectl apply -f local-hostpath-pvc.yaml 查看 pvc 状态 $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Pending openebs-hostpath 2m15s 输出显示 STATUS 为 Pending。这意味着 PVC 还没有被应用程序使用。 ","date":"2022-10-15","objectID":"/posts/d75507/:3:6","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建 Pd $ cat \u003e local-hostpath-pod.yaml \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: hello-local-hostpath-pod spec: volumes: - name: local-storage persistentVolumeClaim: claimName: local-hostpath-pvc containers: - name: hello-container image: busybox command: - sh - -c - 'while true; do echo \"`date` [`hostname`] Hello from OpenEBS Local PV.\" \u003e\u003e /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF 发布创建 $ kubectl apply -f local-hostpath-pod.yaml ","date":"2022-10-15","objectID":"/posts/d75507/:3:7","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"验证数据是否写入卷 $ kubectl exec hello-local-hostpath-pod -- cat /mnt/store/greet.txt Thu Jun 24 15:10:45 CST 2021 [node1] Hello from OpenEBS Local PV. ","date":"2022-10-15","objectID":"/posts/d75507/:3:8","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"验证容器是否使用 Local PV Hostpath 卷 $ kubectl describe pod hello-local-hostpath-pod Name: hello-local-hostpath-pod Namespace: default Priority: 0 ... Volumes: local-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: local-hostpath-pvc ReadOnly: false default-token-98scc: Type: Secret (a volume populated by a Secret) SecretName: default-token-98scc Optional: false ... ","date":"2022-10-15","objectID":"/posts/d75507/:3:9","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"查看 PVC 状态 $ kubectl get pvc local-hostpath-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Bound pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 5G RWO openebs-hostpath 10m ","date":"2022-10-15","objectID":"/posts/d75507/:3:10","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"查看该 PV 卷数据存储目录为 $ kubectl get -o yaml pv pvc-6eac3773-49ef-47af-a475-acb57ed15cf6|grep 'path:' f:path: {} path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 并且 pv 配置了亲和性，制定了调度节点为 node2 spec: accessModes: - ReadWriteOnce capacity: storage: 5G claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: local-hostpath-pvc namespace: default resourceVersion: \"9034\" uid: 6eac3773-49ef-47af-a475-acb57ed15cf6 local: fsType: \"\" path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node2 persistentVolumeReclaimPolicy: Delete storageClassName: openebs-hostpath volumeMode: Filesystem 结果证明数据仅存在于 node2 下。 ","date":"2022-10-15","objectID":"/posts/d75507/:3:11","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"清理 Pod $ kubectl delete pod hello-local-hostpath-pod ","date":"2022-10-15","objectID":"/posts/d75507/:3:12","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"基准测试 下载**基准测试 Job 声明文件** 调整以下内容 image: openebs/perf-test:latest # 调整为内网镜像库tag claimName: dbench # 调整为local-hostpath-pvc 发布运行 $ kubectl create -f fio-deploy.yaml 查看运行状态 $ kubectl get pod NAME READY STATUS RESTARTS AGE dbench-729cw-nqfpt 1/1 Running 0 24s 查看基准测试结果 $ kubectl logs -f dbench-729cw-nqfpt ... All tests complete. ================== = Dbench Summary = ================== Random Read/Write IOPS: 2144/6654. BW: 131MiB/s / 403MiB/s Average Latency (usec) Read/Write: 4254.08/3661.59 Sequential Read/Write: 1294MiB/s / 157MiB/s Mixed Random Read/Write IOPS: 1350/443 ","date":"2022-10-15","objectID":"/posts/d75507/:3:13","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"清理 $ kubectl delete pvc local-hostpath-pvc $ kubectl delete sc openebs-hostpath ","date":"2022-10-15","objectID":"/posts/d75507/:3:14","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"Local PV Device 实践 对比 Kubernetes 本地持久卷，OpenEBS 本地 PV 设备卷有以下优点 : OpenEBS 本地 PV 设备卷 provider 是动态的，Kubernetes 设备卷 provider 是静态的 OpenEBS NDM 更好地管理用于创建本地 pv 的块设备。NDM 提供了发现块设备属性、设置设备筛选器、度量集合以及检测块设备是否已经跨节点移动等功能 环境依赖 : k8s 1.12 以上 OpenEBS 1.0 以上 实践环境 : docker 19.03.8 k8s 1.18.6 CentOS7 $ kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 8m8s v1.18.6 node2 Ready master,worker 7m15s v1.18.6 node3 Ready master,worker 7m15s v1.18.6 三个节点上的 /dev/sdb 作为块设备存储 [root@node1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node3 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom ","date":"2022-10-15","objectID":"/posts/d75507/:3:15","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建数据目录 在将要创建 Local PV Hostpaths 的节点上设置目录。这个目录将被称为 BasePath。默认位置是 /var/openebs/local 节点 node1、node2、node3 创建 /data/openebs/local 目录 （/data 可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统 ‘/’ 挂载点存储空间） $ mkdir -p /data/openebs/local ","date":"2022-10-15","objectID":"/posts/d75507/:3:16","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"下载应用描述文件 ","date":"2022-10-15","objectID":"/posts/d75507/:3:17","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"发布 openebs 应用 根据上述配置文件，保证 k8s 集群可访问到如下镜像（建议导入本地私有镜像库，如 : harbor） openebs/node-disk-manager:1.5.0 openebs/node-disk-operator:1.5.0 openebs/provisioner-localpv:2.10.0 更新 openebs-operator.yaml 中镜像 tag 为实际 tag image: openebs/node-disk-manager:1.5.0 image: openebs/node-disk-operator:1.5.0 image: openebs/provisioner-localpv:2.10.0 发布 $ kubectl apply -f openebs-operator.yaml 查看发布状态 $ kubectl get pod -n openebs -w NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-6d6d9cfc99-4sltp 1/1 Running 0 10s openebs-ndm-85rng 1/1 Running 0 10s openebs-ndm-operator-7df6668998-ptnlq 0/1 Running 0 10s openebs-ndm-qgqm9 1/1 Running 0 10s openebs-ndm-zz7ps 1/1 Running 0 10s ","date":"2022-10-15","objectID":"/posts/d75507/:3:18","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建存储类 $ cat \u003e local-device-sc.yaml \u003c\u003cEOF apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-device annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: StorageType value: device provisioner: openebs.io/local reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer EOF $ kubectl apply -f local-device-sc.yaml ","date":"2022-10-15","objectID":"/posts/d75507/:3:19","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"创建 Pod 及 PVC $ cat \u003e local-device-pod.yaml \u003c\u003cEOF --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: local-device-pvc spec: storageClassName: local-device accessModes: - ReadWriteOnce resources: requests: storage: 5G --- apiVersion: v1 kind: Pod metadata: name: hello-local-device-pod spec: volumes: - name: local-storage persistentVolumeClaim: claimName: local-device-pvc containers: - name: hello-container image: busybox command: - sh - -c - 'while true; do echo \"`date` [`hostname`] Hello from OpenEBS Local PV.\" \u003e\u003e /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF 发布 $ kubectl apply -f local-device-pod.yaml 查看 pod 状态 $ kubectl get pod hello-local-device-pod -w NAME READY STATUS RESTARTS AGE hello-local-device-pod 1/1 Running 0 9s 确认 pod 关联 pvc 是否为 local-device-pvc $ kubectl describe pod hello-local-device-pod Name: hello-local-device-pod Namespace: default Node: node2/192.168.1.112 ... Volumes: local-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: local-device-pvc ReadOnly: false ... 观察到调度的节点为 node2，确认 node2 节点 /dev/sdb 是否被使用 [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk /var/lib/kubelet/pods/266b7b14-5eb7-40ec-bccb-3ac189acf939/volumes/kubernetes.io~local-volume/pvc-9bd89019-13dc-4 sr0 11:0 1 4.4G 0 rom 确实被使用，OpenEBS 强大之处则在于此，极致的简洁。如上文我们讨论的那样，NDM 负责发现块设备并过滤掉不应该被 OpenEBS 使用的设备，例如，检测有 OS 文件系统的磁盘。 ","date":"2022-10-15","objectID":"/posts/d75507/:3:20","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"基准测试 创建基准测试 pvc $ cat \u003e dbench-pvc.yaml \u003c\u003cEOF --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: dbench spec: storageClassName: local-device accessModes: - ReadWriteOnce resources: requests: storage: 5G EOF 下载**基准测试 Job 声明文件** 调整以下内容 image: openebs/perf-test:latest # 调整为内网镜像库tag 发布运行 $ kubectl create -f dbench-pvc.yaml $ kubectl create -f fio-deploy.yaml 查看运行状态 $ kubectl get pod NAME READY STATUS RESTARTS AGE dbench-vqk68-f9877 1/1 Running 0 24s 查看基准测试结果 $ kubectl logs -f dbench-vqk68-f9877 ... All tests complete. ================== = Dbench Summary = ================== Random Read/Write IOPS: 3482/6450. BW: 336MiB/s / 1017MiB/s Average Latency (usec) Read/Write: 2305.77/1508.63 Sequential Read/Write: 6683MiB/s / 2312MiB/s Mixed Random Read/Write IOPS: 3496/1171 从结果来看，相较 Local PV HostPath 模式性能翻倍 ","date":"2022-10-15","objectID":"/posts/d75507/:3:21","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"总结 在整个测试验证过程，OpenEBS 给我的感觉是：极简的操作，尤其 Local PV 引擎的部署使用。 但 OpenEBS 现阶段也存在一些不足： cStor 与 Jiva 数据面组件较多，配置较为繁琐（第一感觉概念性的组件过多，） cStor 与 Jiva 部分组件创建依赖内部定义的镜像 tag，在离线环境下无法通过调整为私有库 tag 导致组件无法成功运行 存储类型单一，多个引擎仅支持块存储类型，不支持原生多节点读写（需结合 NFS 实现），对比 ceph 等稍显逊色 建议以下场景使用 OpenEBS 作为后端存储： 单机测试环境 多机实验 / 演示环境 ","date":"2022-10-15","objectID":"/posts/d75507/:3:22","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["云原生","存储"],"content":"参考资料 全网最全的云原生存储 OpenEBS 使用指南 ","date":"2022-10-15","objectID":"/posts/d75507/:3:23","tags":["openebs","存储"],"title":"全网最全的云原生存储 OpenEBS 使用指南","uri":"/posts/d75507/"},{"categories":["golang","每日一库"],"content":"在用 Go 和 gin 框架开发网站时，gin 缺乏实时重载的功能是令人遗憾的,修改完代码之后经常需要 ctrl + c 结束服务，重新运行 go run,影响开发效率。 Air 是为 Go 应用开发设计的另外一个热重载的命令行工具。只需在你的项目根目录下输入 air，然后把它放在一边，专注于你的代码即可 ","date":"2022-09-19","objectID":"/posts/f7f93a/:0:0","tags":["golang","air"],"title":"golang热加载工具","uri":"/posts/f7f93a/"},{"categories":["golang","每日一库"],"content":"安装 # binary 文件会是在 $(go env GOPATH)/bin/air curl -sSfL https://raw.githubusercontent.com/cosmtrek/air/master/install.sh | sh -s -- -b $(go env GOPATH)/bin # 或者把它安装在 ./bin/ 路径下 curl -sSfL https://raw.githubusercontent.com/cosmtrek/air/master/install.sh | sh -s air -v ","date":"2022-09-19","objectID":"/posts/f7f93a/:1:0","tags":["golang","air"],"title":"golang热加载工具","uri":"/posts/f7f93a/"},{"categories":["golang","每日一库"],"content":"初始化 cd /path/project/app air init # 当前目录下生成配置文件 `.air.toml` ","date":"2022-09-19","objectID":"/posts/f7f93a/:2:0","tags":["golang","air"],"title":"golang热加载工具","uri":"/posts/f7f93a/"},{"categories":["golang","每日一库"],"content":"配置 root = \".\" testdata_dir = \"testdata\" tmp_dir = \"tmp\" [build] args_bin = [] bin = \"./tmp/main\" # 编译输出的二进制文件 cmd = \"go build -o ./tmp/main .\" # 编译命令,如果有其他子命令或者参数可以通过 `air server --port 8081` 传入 delay = 1000 exclude_dir = [\"assets\", \"tmp\", \"vendor\", \"testdata\"] exclude_file = [] exclude_regex = [\"_test.go\"] exclude_unchanged = false follow_symlink = false full_bin = \"\" include_dir = [] include_ext = [\"go\", \"tpl\", \"tmpl\", \"html\"] kill_delay = \"0s\" log = \"build-errors.log\" send_interrupt = false stop_on_error = true [color] app = \"\" build = \"yellow\" main = \"magenta\" runner = \"green\" watcher = \"cyan\" [log] time = false [misc] clean_on_exit = false [screen] clear_on_rebuild = false ","date":"2022-09-19","objectID":"/posts/f7f93a/:3:0","tags":["golang","air"],"title":"golang热加载工具","uri":"/posts/f7f93a/"},{"categories":["golang","每日一库"],"content":"使用 # 进入项目根目录执行 `air` 命令即可 ","date":"2022-09-19","objectID":"/posts/f7f93a/:4:0","tags":["golang","air"],"title":"golang热加载工具","uri":"/posts/f7f93a/"},{"categories":["云原生","Istio"],"content":"查看监听端口和进程 istio-proxy@istio-ingressgateway-799bbc9474-hf7kv:/$ ss -ntlp State Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 128 127.0.0.1:15000 0.0.0.0:* users:((\"envoy\",pid=16,fd=18)) LISTEN 0 128 127.0.0.1:15004 0.0.0.0:* users:((\"pilot-agent\",pid=1,fd=15)) LISTEN 0 128 0.0.0.0:15021 0.0.0.0:* users:((\"envoy\",pid=16,fd=24)) LISTEN 0 128 0.0.0.0:15021 0.0.0.0:* users:((\"envoy\",pid=16,fd=23)) LISTEN 0 128 0.0.0.0:8080 0.0.0.0:* users:((\"envoy\",pid=16,fd=40)) LISTEN 0 128 0.0.0.0:8080 0.0.0.0:* users:((\"envoy\",pid=16,fd=39)) LISTEN 0 128 0.0.0.0:15090 0.0.0.0:* users:((\"envoy\",pid=16,fd=22)) LISTEN 0 128 0.0.0.0:15090 0.0.0.0:* users:((\"envoy\",pid=16,fd=21)) LISTEN 0 128 *:15020 *:* users:((\"pilot-agent\",pid=1,fd=12)) istio-proxy@istio-ingressgateway-799bbc9474-hf7kv:/$ ps -ef |grep pilot-agent istio-p+ 1 0 0 Sep14 ? 00:01:27 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info istio-p+ 74 30 0 09:37 pts/0 00:00:00 grep --color=auto pilot-agent ","date":"2022-09-16","objectID":"/posts/3d18d6/:0:1","tags":["istio","pilot"],"title":"深入Istio系列-Pilot agent","uri":"/posts/3d18d6/"},{"categories":["云原生","Istio"],"content":"pilot-agent proxy router 启动参数 istio-proxy@istio-ingressgateway-799bbc9474-hf7kv:/$ /usr/local/bin/pilot-agent proxy router --help XDS proxy agent Usage: pilot-agent proxy [flags] Flags: --concurrency int number of worker threads to run --domain string DNS domain suffix. If not provided uses ${POD_NAMESPACE}.svc.cluster.local -h, --help help for proxy --meshConfig string File name for Istio mesh configuration. If not specified, a default mesh will be used. This may be overridden by PROXY_CONFIG environment variable or proxy.istio.io/config annotation. (default \"./etc/istio/config/mesh\") --outlierLogPath string The log path for outlier detection --proxyComponentLogLevel string The component log level used to start the Envoy proxy. Deprecated, use proxyLogLevel instead --proxyLogLevel string The log level used to start the Envoy proxy (choose from {trace, debug, info, warning, error, critical, off}).Level may also include one or more scopes, such as 'info,misc:error,upstream:debug' (default \"warning,misc:error\") --serviceCluster string Service cluster (default \"istio-proxy\") --stsPort int HTTP Port on which to serve Security Token Service (STS). If zero, STS service will not be provided. --templateFile string Go template bootstrap config --tokenManagerPlugin string Token provider specific plugin name. (default \"GoogleTokenExchange\") Global Flags: --log_as_json Whether to format output as JSON or in plain console-friendly format --log_caller string Comma-separated list of scopes for which to include caller information, scopes can be any of [ads, all, authn, authorization, ca, cache, citadelclient, controllers, default, delta, dns, gcecred, googleca, googlecas, grpcgen, healthcheck, iptables, klog, mockcred, model, proxyconfig, sds, security, serviceentry, spiffe, stsclient, stsserver, telemetry, token, trustBundle, validation, wasm, wle, xdsproxy] --log_output_level string Comma-separated minimum per-scope logging level of messages to output, in the form of \u003cscope\u003e:\u003clevel\u003e,\u003cscope\u003e:\u003clevel\u003e,... where scope can be one of [ads, all, authn, authorization, ca, cache, citadelclient, controllers, default, delta, dns, gcecred, googleca, googlecas, grpcgen, healthcheck, iptables, klog, mockcred, model, proxyconfig, sds, security, serviceentry, spiffe, stsclient, stsserver, telemetry, token, trustBundle, validation, wasm, wle, xdsproxy] and level can be one of [debug, info, warn, error, fatal, none] (default \"default:info\") --log_rotate string The path for the optional rotating log file --log_rotate_max_age int The maximum age in days of a log file beyond which the file is rotated (0 indicates no limit) (default 30) --log_rotate_max_backups int The maximum number of log file backups to keep before older files are deleted (0 indicates no limit) (default 1000) --log_rotate_max_size int The maximum size in megabytes of a log file beyond which the file is rotated (default 104857600) --log_stacktrace_level string Comma-separated minimum per-scope logging level at which stack traces are captured, in the form of \u003cscope\u003e:\u003clevel\u003e,\u003cscope:level\u003e,... where scope can be one of [ads, all, authn, authorization, ca, cache, citadelclient, controllers, default, delta, dns, gcecred, googleca, googlecas, grpcgen, healthcheck, iptables, klog, mockcred, model, proxyconfig, sds, security, serviceentry, spiffe, stsclient, stsserver, telemetry, token, trustBundle, validation, wasm, wle, xdsproxy] and level can be one of [debug, info, warn, error, fatal, none] (default \"default:none\") --log_target stringArray The set of paths where to output the log. This can be any path as well as the special values stdout and stderr (default [stdout]) --vklog Level number for the log level verbosity. Like -v flag. ex: --vklog=9 istio-proxy@istio-ingressgateway-799bbc9474-hf7kv:/$ 20220916174047 ","date":"2022-09-16","objectID":"/posts/3d18d6/:0:2","tags":["istio","pilot"],"title":"深入Istio系列-Pilot agent","uri":"/posts/3d18d6/"},{"categories":["云原生","Istio"],"content":"查看meshconfig配置 istio-proxy@istio-ingressgateway-799bbc9474-hf7kv:/$ cat ./etc/istio/config/mesh accessLogFile: /dev/stdout defaultConfig: discoveryAddress: istiod.istio-system.svc:15012 proxyMetadata: {} tracing: zipkin: address: zipkin.istio-system:9411 enablePrometheusMerge: true extensionProviders: - envoyOtelAls: port: 4317 service: opentelemetry-collector.istio-system.svc.cluster.local name: otel rootNamespace: istio-system trustDomain: cluster.local istio-proxy@istio-ingressgateway-799bbc9474-hf7kv:/$ 20220916174246 ","date":"2022-09-16","objectID":"/posts/3d18d6/:0:3","tags":["istio","pilot"],"title":"深入Istio系列-Pilot agent","uri":"/posts/3d18d6/"},{"categories":["云原生"],"content":" # ss -ntlp State Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 4096 127.0.0.1:10245 0.0.0.0:* users:((\"nginx-ingress-c\",pid=2671,fd=8)) LISTEN 0 511 127.0.0.1:10246 0.0.0.0:* users:((\"nginx\",pid=2783,fd=17),(\"nginx\",pid=2738,fd=17)) LISTEN 0 511 127.0.0.1:10247 0.0.0.0:* users:((\"nginx\",pid=2783,fd=18),(\"nginx\",pid=2738,fd=18)) LISTEN 0 4096 0.0.0.0:80 0.0.0.0:* users:((\"nginx\",pid=2783,fd=11),(\"nginx\",pid=2738,fd=11)) LISTEN 0 4096 0.0.0.0:8181 0.0.0.0:* users:((\"nginx\",pid=2783,fd=15),(\"nginx\",pid=2738,fd=15)) LISTEN 0 4096 0.0.0.0:443 0.0.0.0:* users:((\"nginx\",pid=2783,fd=13),(\"nginx\",pid=2738,fd=13)) LISTEN 0 4096 *:10254 *:* users:((\"nginx-ingress-c\",pid=2671,fd=33)) LISTEN 0 4096 [::]:80 [::]:* users:((\"nginx\",pid=2783,fd=12),(\"nginx\",pid=2738,fd=12)) LISTEN 0 4096 [::]:8181 [::]:* users:((\"nginx\",pid=2783,fd=16),(\"nginx\",pid=2738,fd=16)) LISTEN 0 4096 *:8443 *:* users:((\"nginx-ingress-c\",pid=2671,fd=34)) LISTEN 0 4096 [::]:443 [::]:* users:((\"nginx\",pid=2783,fd=14),(\"nginx\",pid=2738,fd=14)) ## nginx-ingress-controller进程 # ps aux |grep -w 2671 _rpc 2671 0.4 0.2 743856 42000 ? Ssl Sep15 5:06 /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key root 284760 0.0 0.0 3100 812 pts/1 S+ 04:15 0:00 grep -w 2671 # ## Nginx Master 进程 # ps aux |grep -w 2738 _rpc 2738 0.0 0.2 145128 35448 ? S Sep15 0:00 nginx: master process /usr/local/nginx/sbin/nginx -c /etc/nginx/nginx.conf root 284662 0.0 0.0 3100 844 pts/1 S+ 04:15 0:00 grep -w 2738 # ## Nginx worker 进程 # ps aux |grep -w 2783 _rpc 2783 0.0 0.2 157240 41060 ? Sl Sep15 0:31 nginx: worker process root 285147 0.0 0.0 3100 832 pts/1 S+ 04:17 0:00 grep -w 2783 # 从上面可以看出整个Ingress Nginx Controller Pod 包含两部分 nginx-ingress-controller 和 Nginx nginx-ingress-controller 如其名 控制管理（control）ingress资源的应用。ingress controller会监听集群中ingress资源事件，然后根据对应的事件处理逻辑更新Nginx的配置。 ","date":"2022-09-16","objectID":"/posts/2da521/:0:0","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"1 Controller 控制器 ","date":"2022-09-16","objectID":"/posts/2da521/:1:0","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"编译 Controller build/build.sh ${GO_BUILD_CMD} \\ -trimpath -ldflags=\"-buildid= -w -s \\ -X ${PKG}/version.RELEASE=${TAG} \\ -X ${PKG}/version.COMMIT=${COMMIT_SHA} \\ -X ${PKG}/version.REPO=${REPO_INFO}\" \\ -o \"${TARGETS_DIR}/nginx-ingress-controller\" \"${PKG}/cmd/nginx\" 从构建命令可以知道 nginx-ingress-controller 的入口在 cmd/nginx cmd/nginx/main.go func main() { klog.InitFlags(nil) rand.Seed(time.Now().UnixNano()) fmt.Println(version.String()) // 命令行传入配置信息以及一些默认配置项，并实例化controller.Configuration showVersion, conf, err := ingressflags.ParseFlags() if showVersion { os.Exit(0) } if err != nil { klog.Fatal(err) } err = file.CreateRequiredDirectories() if err != nil { klog.Fatal(err) } kubeClient, err := createApiserverClient(conf.APIServerHost, conf.RootCAFile, conf.KubeConfigFile) if err != nil { handleFatalInitError(err) } if len(conf.DefaultService) \u003e 0 { err := checkService(conf.DefaultService, kubeClient) if err != nil { klog.Fatal(err) } klog.InfoS(\"Valid default backend\", \"service\", conf.DefaultService) } if len(conf.PublishService) \u003e 0 { err := checkService(conf.PublishService, kubeClient) if err != nil { klog.Fatal(err) } } if conf.Namespace != \"\" { _, err = kubeClient.CoreV1().Namespaces().Get(context.TODO(), conf.Namespace, metav1.GetOptions{}) if err != nil { klog.Fatalf(\"No namespace with name %v found: %v\", conf.Namespace, err) } } conf.FakeCertificate = ssl.GetFakeSSLCert() klog.InfoS(\"SSL fake certificate created\", \"file\", conf.FakeCertificate.PemFileName) // 检查版本信息 if !k8s.NetworkingIngressAvailable(kubeClient) { klog.Fatalf(\"ingress-nginx requires Kubernetes v1.19.0 or higher\") } _, err = kubeClient.NetworkingV1().IngressClasses().List(context.TODO(), metav1.ListOptions{}) if err != nil { if !errors.IsNotFound(err) { if errors.IsForbidden(err) { klog.Warningf(\"No permissions to list and get Ingress Classes: %v, IngressClass feature will be disabled\", err) conf.IngressClassConfiguration.IgnoreIngressClass = true } } } conf.Client = kubeClient err = k8s.GetIngressPod(kubeClient) if err != nil { klog.Fatalf(\"Unexpected error obtaining ingress-nginx pod: %v\", err) } // 注册Prometheus reg := prometheus.NewRegistry() reg.MustRegister(collectors.NewGoCollector()) reg.MustRegister(collectors.NewProcessCollector(collectors.ProcessCollectorOpts{ PidFn: func() (int, error) { return os.Getpid(), nil }, ReportErrors: true, })) // metric指标数据采集，mc是一个用于收集指标的collector实例 mc := metric.NewDummyCollector() if conf.EnableMetrics { mc, err = metric.NewCollector(conf.MetricsPerHost, conf.ReportStatusClasses, reg, conf.IngressClassConfiguration.Controller, *conf.MetricsBuckets) if err != nil { klog.Fatalf(\"Error creating prometheus collector: %v\", err) } } // Pass the ValidationWebhook status to determine if we need to start the collector // for the admissionWebhook mc.Start(conf.ValidationWebhook) // 启用性能调试端口 if conf.EnableProfiling { go metrics.RegisterProfiler(\"127.0.0.1\", nginx.ProfilerPort) } // 实例化 ngx controller 控制器 ngx := controller.NewNGINXController(conf, mc) mux := http.NewServeMux() metrics.RegisterHealthz(nginx.HealthPath, mux, ngx) metrics.RegisterMetrics(reg, mux) _, errExists := os.Stat(\"/chroot\") if errExists == nil { conf.IsChroot = true go logger(conf.InternalLoggerAddress) } // 启动健康检查和 metrics API 接口 go metrics.StartHTTPServer(conf.HealthCheckHost, conf.ListenPorts.Health, mux) // 启动 nginx master 进程 go ngx.Start() process.HandleSigterm(ngx, conf.PostShutdownGracePeriod, func(code int) { os.Exit(code) }) } controller 启动进程中传入配置信息以及一些默认配置项, 并根据配置项启用 Metrics、HealthCheck、Profile、Logging 等功能，本文主要介绍 Controller 控制器主流程 ngx := controller.NewNGINXController(conf, mc) ","date":"2022-09-16","objectID":"/posts/2da521/:1:1","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"2 创建NGINXController 控制器 ","date":"2022-09-16","objectID":"/posts/2da521/:2:0","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"2.1 NGINXController type NGINXController struct { // 配置信息 cfg *Configuration // 事件通知器 recorder record.EventRecorder // 同步队列 syncQueue *task.Queue // 同步状态 -- [kubernetes-ingress状态上报机制](https://blog.dianduidian.com/post/kubernetes-ingress%E7%8A%B6%E6%80%81%E4%B8%8A%E6%8A%A5%E6%9C%BA%E5%88%B6/) syncStatus status.Syncer // 同步限流器 syncRateLimiter flowcontrol.RateLimiter // stopLock is used to enforce that only a single call to Stop send at // a given time. We allow stopping through an HTTP endpoint and // allowing concurrent stoppers leads to stack traces. stopLock *sync.Mutex stopCh chan struct{} // 更新环状channel updateCh *channels.RingChannel // 接受nginx 错误信息channel // ngxErrCh is used to detect errors with the NGINX processes ngxErrCh chan error // 当前配置文件,用来对比配置是否有更新 // runningConfig contains the running configuration in the Backend runningConfig *ingress.Configuration // nginx 配置模板渲染器 t ngx_template.Writer // nameserver 列表 // 读取/etc/resolv.conf 中的ns地址，用来生成nginx.conf使用，eg:resolver 127.0.0.1 [::1]:5353; resolver []net.IP // 是否启用ipv6 isIPV6Enabled bool // 是否关闭 isShuttingDown bool // TCP代理，启用SSLPassthrough时使用 Proxy *tcpproxy.TCPProxy // 本地缓存 store store.Storer // metrics 收集器 metricCollector metric.Collector admissionCollector metric.Collector // webhook validationWebhookServer *http.Server // 操作Nginx的接口，用来启动Nginx进程和测试配置文件 command NginxExecTester } ","date":"2022-09-16","objectID":"/posts/2da521/:2:1","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"2.2 Configuration type Configuration struct { APIServerHost string RootCAFile string KubeConfigFile string Client clientset.Interface ResyncPeriod time.Duration ConfigMapName string DefaultService string Namespace string WatchNamespaceSelector labels.Selector // +optional TCPConfigMapName string // +optional UDPConfigMapName string DefaultSSLCertificate string // +optional PublishService string PublishStatusAddress string UpdateStatus bool UseNodeInternalIP bool ElectionID string UpdateStatusOnShutdown bool HealthCheckHost string ListenPorts *ngx_config.ListenPorts DisableServiceExternalName bool EnableSSLPassthrough bool EnableProfiling bool EnableMetrics bool MetricsPerHost bool MetricsBuckets *collectors.HistogramBuckets ReportStatusClasses bool FakeCertificate *ingress.SSLCert SyncRateLimit float32 DisableCatchAll bool IngressClassConfiguration *ingressclass.IngressClassConfiguration ValidationWebhook string ValidationWebhookCertPath string ValidationWebhookKeyPath string DisableFullValidationTest bool GlobalExternalAuth *ngx_config.GlobalExternalAuth MaxmindEditionFiles *[]string MonitorMaxBatchSize int PostShutdownGracePeriod int ShutdownGracePeriod int InternalLoggerAddress string IsChroot bool DeepInspector bool DynamicConfigurationRetries int } 这些配置需要控制器启动时通过命令行传入 ","date":"2022-09-16","objectID":"/posts/2da521/:2:2","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"2.3 实例化 NewNGINXController 控制器 func NewNGINXController(config *Configuration, mc metric.Collector) *NGINXController { // kubectl describe 命令看到的事件日志就是这个库产生的 eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(klog.Infof) eventBroadcaster.StartRecordingToSink(\u0026v1core.EventSinkImpl{ Interface: config.Client.CoreV1().Events(config.Namespace), }) // 读取pod里面的/etc/resolv.conf h, err := dns.GetSystemNameServers() if err != nil { klog.Warningf(\"Error reading system nameservers: %v\", err) } // 实例化 NGINXController n := \u0026NGINXController{ isIPV6Enabled: ing_net.IsIPv6Enabled(), resolver: h, cfg: config, syncRateLimiter: flowcontrol.NewTokenBucketRateLimiter(config.SyncRateLimit, 1), recorder: eventBroadcaster.NewRecorder(scheme.Scheme, apiv1.EventSource{ Component: \"nginx-ingress-controller\", }), stopCh: make(chan struct{}), updateCh: channels.NewRingChannel(1024), ngxErrCh: make(chan error), stopLock: \u0026sync.Mutex{}, // 当前运行的配置文件，刚启动是为空 runningConfig: new(ingress.Configuration), Proxy: \u0026tcpproxy.TCPProxy{}, metricCollector: mc, // 一个可以调用 nginx -c nginx.conf 命令的对象 command: NewNginxCommand(), } // 启动 webhook 服务 if n.cfg.ValidationWebhook != \"\" { n.validationWebhookServer = \u0026http.Server{ Addr: config.ValidationWebhook, //G112 (CWE-400): Potential Slowloris Attack ReadHeaderTimeout: 10 * time.Second, Handler: adm_controller.NewAdmissionControllerServer(\u0026adm_controller.IngressAdmission{Checker: n}), TLSConfig: ssl.NewTLSListener(n.cfg.ValidationWebhookCertPath, n.cfg.ValidationWebhookKeyPath).TLSConfig(), // disable http/2 // https://github.com/kubernetes/kubernetes/issues/80313 // https://github.com/kubernetes/ingress-nginx/issues/6323#issuecomment-737239159 TLSNextProto: make(map[string]func(*http.Server, *tls.Conn, http.Handler)), } } // 实例化 store (本地缓存) // store 对象，很重要，数据缓存与k8s交互的接口都在这个对象 n.store = store.New( config.Namespace, config.WatchNamespaceSelector, config.ConfigMapName, config.TCPConfigMapName, config.UDPConfigMapName, config.DefaultSSLCertificate, config.ResyncPeriod, config.Client, n.updateCh, config.DisableCatchAll, config.DeepInspector, config.IngressClassConfiguration) // 创建工作队列，这里把 syncIngress 注册到这个工作队列 n.syncQueue = task.NewTaskQueue(n.syncIngress) if config.UpdateStatus { n.syncStatus = status.NewStatusSyncer(status.Config{ Client: config.Client, PublishService: config.PublishService, PublishStatusAddress: config.PublishStatusAddress, IngressLister: n.store, UpdateStatusOnShutdown: config.UpdateStatusOnShutdown, UseNodeInternalIP: config.UseNodeInternalIP, }) } else { klog.Warning(\"Update of Ingress status is disabled (flag --update-status)\") } // 监听模板文件更新 onTemplateChange := func() { // 渲染模板 template, err := ngx_template.NewTemplate(nginx.TemplatePath) if err != nil { // this error is different from the rest because it must be clear why nginx is not working klog.ErrorS(err, \"Error loading new template\") return } // 若模板渲染正确，则更新到 nginxcontroller 对象中，并往同步队列发送一个 template-change 事件 n.t = template klog.InfoS(\"New NGINX configuration template loaded\") n.syncQueue.EnqueueTask(task.GetDummyObject(\"template-change\")) } // 首次启动加载配置模板文件 ngxTpl, err := ngx_template.NewTemplate(nginx.TemplatePath) if err != nil { klog.Fatalf(\"Invalid NGINX configuration template: %v\", err) } n.t = ngxTpl // 监听模板文件变化 // 监听 /etc/nginx/template/nginx.tmpl 模板文件是否有变化，有变化则调用 onTemplateChange _, err = file.NewFileWatcher(nginx.TemplatePath, onTemplateChange) if err != nil { klog.Fatalf(\"Error creating file watcher for %v: %v\", nginx.TemplatePath, err) } filesToWatch := []string{} err = filepath.Walk(\"/etc/nginx/geoip/\", func(path string, info os.FileInfo, err error) error { if err != nil { return err } if info.IsDir() { return nil } filesToWatch = append(filesToWatch, path) return nil }) if err != nil { klog.Fatalf(\"Error creating file watchers: %v\", err) } // 配置文件有变化则往同步队列发送一个 file-change 事件 for _, f := range filesToWatch { _, err = file.NewFileWatcher(f, func() { klog.InfoS(\"File changed detected. Reloading NGINX\", \"path\", f) n.syncQueu","date":"2022-09-16","objectID":"/posts/2da521/:2:3","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"2.4 创建store(本地缓存) func New( namespace string, namespaceSelector labels.Selector, configmap, tcp, udp, defaultSSLCertificate string, resyncPeriod time.Duration, client clientset.Interface, updateCh *channels.RingChannel, disableCatchAll bool, deepInspector bool, icConfig *ingressclass.IngressClassConfiguration) Storer { // store的具体实现是k8sStore store := \u0026k8sStore{ informers: \u0026Informer{}, listers: \u0026Lister{}, sslStore: NewSSLCertTracker(), updateCh: updateCh, backendConfig: ngx_config.NewDefault(), syncSecretMu: \u0026sync.Mutex{}, backendConfigMu: \u0026sync.RWMutex{}, secretIngressMap: NewObjectRefMap(), defaultSSLCertificate: defaultSSLCertificate, } // kubectl describe 命令看到的事件日志就是这个库产生的 eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(klog.Infof) eventBroadcaster.StartRecordingToSink(\u0026clientcorev1.EventSinkImpl{ Interface: client.CoreV1().Events(namespace), }) recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{ Component: \"nginx-ingress-controller\", }) // 用于提取注释的对象 // 集中在internal/ingress/annotations目录 // k8sStore fulfills resolver.Resolver interface store.annotations = annotations.NewAnnotationExtractor(store) // 将数据再缓存一份用于本地查询，缓存的对象正如其名IngressWithAnnotation // 会缓存internal/ingress/types.go:Ingress store.listers.IngressWithAnnotation.Store = cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc) // As we currently do not filter out kubernetes objects we list, we can // retrieve a huge amount of data from the API server. // In a cluster using HELM \u003c v3 configmaps are used to store binary data. // If you happen to have a lot of HELM releases in the cluster it will make // the memory consumption of nginx-ingress-controller explode. // In order to avoid that we filter out labels OWNER=TILLER. labelsTweakListOptionsFunc := func(options *metav1.ListOptions) { if len(options.LabelSelector) \u003e 0 { options.LabelSelector += \",OWNER!=TILLER\" } else { options.LabelSelector = \"OWNER!=TILLER\" } } // As of HELM \u003e= v3 helm releases are stored using Secrets instead of ConfigMaps. // In order to avoid listing those secrets we discard type \"helm.sh/release.v1\" secretsTweakListOptionsFunc := func(options *metav1.ListOptions) { helmAntiSelector := fields.OneTermNotEqualSelector(\"type\", \"helm.sh/release.v1\") baseSelector, err := fields.ParseSelector(options.FieldSelector) if err != nil { options.FieldSelector = helmAntiSelector.String() } else { options.FieldSelector = fields.AndSelectors(baseSelector, helmAntiSelector).String() } } // 创建informer工厂函数 // create informers factory, enable and assign required informers infFactory := informers.NewSharedInformerFactoryWithOptions(client, resyncPeriod, informers.WithNamespace(namespace), ) // infFactoryConfigmaps, infFactorySecrets // create informers factory for configmaps infFactoryConfigmaps := informers.NewSharedInformerFactoryWithOptions(client, resyncPeriod, informers.WithNamespace(namespace), informers.WithTweakListOptions(labelsTweakListOptionsFunc), ) // create informers factory for secrets infFactorySecrets := informers.NewSharedInformerFactoryWithOptions(client, resyncPeriod, informers.WithNamespace(namespace), informers.WithTweakListOptions(secretsTweakListOptionsFunc), ) store.informers.Ingress = infFactory.Networking().V1().Ingresses().Informer() store.listers.Ingress.Store = store.informers.Ingress.GetStore() if !icConfig.IgnoreIngressClass { store.informers.IngressClass = infFactory.Networking().V1().IngressClasses().Informer() store.listers.IngressClass.Store = cache.NewStore(cache.MetaNamespaceKeyFunc) } store.informers.Endpoint = infFactory.Core().V1().Endpoints().Informer() store.listers.Endpoint.Store = store.informers.Endpoint.GetStore() store.informers.Secret = infFactorySecrets.Core().V1().Secrets().Informer() store.listers.Secret.Store = store.informers.Secret.GetStore() store.informers.ConfigMap = infFactoryConfigmaps.Core().V1().ConfigMaps().Informer() store.listers.ConfigMap.Store = store.informers.ConfigMap.GetStore() store.informers.Ser","date":"2022-09-16","objectID":"/posts/2da521/:2:4","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"2.5 syncIngress 注意有两个 syncIngress 方法,分别是 NGINXController 和 store 2.5.1 NGINXController 入口 n.syncQueue = task.NewTaskQueue(n.syncIngress) 2.5.2 store 入口 AddFunc: func(obj interface{}) { ... ... store.syncIngress(ing) // ... updateCh.In() \u003c- Event{ Type: CreateEvent, Obj: obj, } } ","date":"2022-09-16","objectID":"/posts/2da521/:2:5","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"3 Nginx 反向代理 ","date":"2022-09-16","objectID":"/posts/2da521/:3:0","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"3.1 nginx.tmpl 文件内容太大，详细请查看源文件 注解内容 ","date":"2022-09-16","objectID":"/posts/2da521/:3:1","tags":["ingress"],"title":"深入Ingress - Ingress Controller源码解析","uri":"/posts/2da521/"},{"categories":["云原生"],"content":"Grafana 实验室的 Mimir 是一个在 AGPLv3 许可下新的时间序列数据库，该工程团队从 Cortex TSDB 中汲取精华，同时降低了复杂性并提高了可扩展性。 根据 Grafana 实验室的测试，Mimir 可以扩展到 10 亿个活跃时间序列和 5000 万个样本/秒的摄取率，该基准测试要求运行一个具有 7000 个 CPU 核心和 30TiB 内存的集群，这已经是我听说的最大、最昂贵的时间序列数据库的公共基准测试了。要重现这样规模的基准测试并不那么容易，幸运的是，在大多数情况下，用户的工作负荷要求要低得多，比较容易模拟。在本文我们将尝试比较 VictoriaMetrics 和 Grafana Mimir 集群在相同硬件上的中等工作负载下运行的性能和资源使用情况。 ","date":"2022-09-14","objectID":"/posts/6650ff/:0:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"方法 在比较两种不同产品的时候，最复杂的事情是保持透明和公正。尤其是当你对一种产品非常熟悉而对另一种产品完全陌生的时候。在这里我们要非常感谢 Mimir 的工程团队，我们在准备这个基准的过程中与他们进行了联系。我们非常感谢他们愿意开放地回答与产品有关的问题，并解释实的相关细节，这确实很有帮助。 VictoriaMetrics 和 Grafana Mimir 都是时间序列数据库，支持大部分相同的协议和 API。但是，它们具有不同的架构和组件，这使得比较起来更加复杂。在基准测试中，我们将使用有限的资源，并根据我的理解将它们分配给两个集群。 然后，我将进行一轮基准测试，以了解两种解决方案如何处理相同的工作负载，以及它们在使用分配的资源方面的效率如何。 基准测试将在 Google Kubernetes Engine 中运行，该引擎由 e2-standard-16 节点（每个节点具有 16vCPU 和 64GiB 的 RAM）和基于 SSD 的标准持久卷组成。 ","date":"2022-09-14","objectID":"/posts/6650ff/:1:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"Prometheus 基准工具 为了生成负载，我将使用 Prometheus-benchmark(https://github.com/VictoriaMetrics/prometheus-benchmark) 这个工具，它在 VictoriaMetrics 内部用于对新版本进行测试和基准测试。由于以下原因，该工具会产生类似生产的工作负载： 作为摄取指标的来源，它使用真正的 node_exporter 目标，这通常是大多数生产环境的情况； 作为读取查询的来源，它使用 node_exporter 推荐的警报规则 列表； 非零指标流失率会产生额外的压力，模拟 Kubernetes 中周期性的 pods 重新部署的场景。 Benchmark 运行两组相同的隔离服务，它们中的每一个都被配置为抓取指标，通过远程写入将它们转发到配置的存储，并定期执行警报规则。基准测试的配置如下： # how frequently to scrape node_exporter targets scrapeInterval: 15s # how often to execute configured rules queryInterval: 15s # defines the number of node_exporter instances to scrape targetsCount: 6000 # percent of node_exporter targets to update # in order to generate series churn rate scrapeConfigUpdatePercent: 1 # specifies how frequently to update targets # for generating time series churn rate scrapeConfigUpdateInterval: 10m 在 https://gist.github.com/hagen1778/a0824cde3903d6506e1b18eff7fd8b40 此次可以查看使用的配置参数的完整列表。 基准测试中的每个 node_exporter 目标产生大约 900 个（取决于 node_exporter 运行的硬件）时间序列，targetCount=6000 和 scrapeInterval=15s 以 360k 样本/秒 的摄取率产生大约 550 万个活跃时间序列到每个配置的远程存储。对于配置的警报列表，queryInterval=15s 通过即时查询生成大约 1.5 个查询/秒的读取负载。scrapeConfigUpdatePercent=1 和 scrapeConfigUpdateInterval=10m 产生每 10 分钟约 60k 新时间序列的 流失率 。 ","date":"2022-09-14","objectID":"/posts/6650ff/:2:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"Mimir 安装 我以前从未使用过 Cortex 或 Mimir，所以我从 官方文档 和一个分布式安装的 Helm Chart 开始探索这个项目。其架构如下图所示。 Mimir 分布式架构 从上图可以看出 Mimir 有 7 个不同的组件，给人的第一印象是一个复杂的系统。值得庆幸的是，helm chart 使事情变得更容易，而且还有根据有效负载提供了资源分配建议。大型工作负载的建议 要求大约 140 个 CPU 和 800GB 内存，用于 1000 万个活跃时间序列。对于一个简单的基准测试，显然这要求太高了，所以我从针对 100 万个活跃时间序列的小型工作负载的推荐配置开始，资源要求约为 30 个 CPU 和 200GB 内存。 通过几次测试运行，我发现推荐的容量规划是比较保守的。为 小型工作负载 所做的设置完全能够比这多出两倍，而且，通过对组件资源的一些手动调整，它还能够处理更多。 helm chart 中有大约 7 个缓存副本和 overrides_exporter，没有设置明确的限制。我假设它们总共消耗约 1 个 CPU。 可以在此处 https://gist.github.com/hagen1778/856fb6e99d7b1dfe284158ca8952a9fd 找到 Helm Chart Values 值的完整覆盖列表。 在测试期间，我不得不突破以下限制： distributor: extraArgs: distributor.ingestion-rate-limit: \"10000000000000\" ingester: extraArgs: ingester.max-global-series-per-user: \"0\" ingester.max-global-series-per-metric: \"0\" querier: extraArgs: querier.max-fetched-chunks-per-query: \"8000000\" mimir: structuredConfig: limits: out_of_order_time_window: 1h 我在这个基准测试中使用了 grafana/mimir:2.2.0 这个发行版。 为了监控已部署的设置，我使用了 https://github.com/grafana/mimir/tree/main/operations/mimir-mixin-compiled 这里列出的仪表盘和记录规则。仪表盘的列表非常丰富和详细，但我发现由于以下原因，它不是很方便。 我没有找到具有全局概览的仪表盘，只是为了显示集群是否一切正常； 仪表盘中的某些面板需要部署 记录规则 ，这是一个额外的步骤，有人可能会错过； 一些面板依赖于带有 cortex_ 前缀和选择器的指标，例如 job=~\"(query-frontend.*|cortex|mimir)\"。Cortex 和 Mimir 前缀指标混合在一起，可能会让大家感到困惑。 不过总的来说 Helm Chart 非常有用且易于理解，Mimir 的团队在这里做得很好，让新手更容易安装。 ","date":"2022-09-14","objectID":"/posts/6650ff/:3:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"VictoriaMetrics 安装 VictoriaMetrics 集群架构如下所示： VictoriaMetrics 集群架构 VictoriaMetrics 有 3 种不同类型的组件，也可以通过 helm chart 进行部署。VictoriaMetrics 集群的组件与 Mimir 的组件不同，其资源配置文件也不同。Mimir 的一些组件需要额外的内存，但 VictoriaMetrics 的组件需要额外的 CPU。因此，在资源分配方面，我将尽量在为 Mimir 分配的边界内。 请注意，我们 建议 运行具有大量小型 vmstorage 节点的集群，而不是运行具有少量大型 vmstorage 节点的集群。 VictoriaMetrics 的资源分配比 CPU 的限制多出约 1 个核心，但是内存少用了大约 80GiB，可以在 https://gist.github.com/hagen1778/bf143173b5512515950f41e3a9bd6005 这里找到完整的 helm chart values 值列表。 VictoriaMetrics 配置了 -replicationFactor=2，这与 Mimir 的默认复制因子 不同，这是 VictoriaMetrics 的推荐值，稍后会有更详细的介绍。 我在这个基准测试中使用的是 VictoriaMetrics/VictoriaMetrics:1.80.0-cluster 这个版本。 VictoriaMetrics 配备了 Grafana 仪表盘 和用于 自我监控的预定义警报规则 。 ","date":"2022-09-14","objectID":"/posts/6650ff/:4:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"基准测试 ","date":"2022-09-14","objectID":"/posts/6650ff/:5:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"快速统计 该基准测试已运行 24 小时； 发送到 VictoriaMetrics 和 Mimir 的样本总数约为 310 亿：360K 样本/秒 * 86400； 基准测试期间生成的新时间序列总数约为 1360 万：550 万初始序列 + 6K 系列/分钟 * 60 分钟 * 24。 在使用的 基准工具 中，摄取负载由 vmagent 生成。它暴露了许多有用的指标，好奇的读者可以在 Grafana 仪表板快照 上研究这些指标。根据这些指标，两个远程存储都可以很好地摄取，没有返回错误，没有丢失数据，每个指标都按预期交付。 如前所述，VictoriaMetrics 和 Mimir 都提供了用于监控的工具和仪表盘。为了客观地比较统计数据，我用 Mimir 的仪表盘所使用的磁盘、内存和 CPU 使用率的相同查询来制作了新的 Grafana 仪表盘。诸如摄取率、活跃时间序列或延迟等面板使用不同的指标，因为它们是由每个解决方案的内部组件导出的，该仪表盘的快照可在 https://snapshots.raintank.io/dashboard/snapshot/1lXGSoVm6xVDtKVZ8LFQvBCG1uDYKChJ 获得。关于每个使用的查询的细节可以在面板的左上角找到。 ","date":"2022-09-14","objectID":"/posts/6650ff/:5:1","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"结果 Mimir 和 VictoriaMetrics 都完全能够处理 360k 样本/秒的摄取率： Mimir 和 VictoriaMetrics 的摄取率和活跃时间序列数 VictoriaMetrics 和 Mimir 之间的活跃时间序列数量略有不同，因为两种解决方案对它们的计算方式不同。由于非零流失率，Mimir 的活跃时间序列数量在不断增长，每 2 小时创建一个新的 TSDB 块时就会重置回来。 Ingester 是 Mimir 负责接收和处理写入的组件，将接收到的数据存储在内存中。这样的方法大大减少了写的放大作用，并有助于减少接触磁盘的频率。但是每 2 小时（可配置）Ingester 需要刷新磁盘上所有缓冲的数据，创建一个新的 TSDB 块并将其上传到对象存储。此操作对磁盘使用指标有影响： Mimir 和 VictoriaMetrics 的磁盘统计信息 虽然大多数时候 Mimir 的磁盘 IO 仍然很低，几乎比 VictoriaMetrics 低 2 倍，但每 2 小时 Mimir 就开始创建一个 TSDB 块，并消耗额外的磁盘资源。 解决方案之间的磁盘空间使用量情况为，VictoriaMetrics 的 49GiB 和 Mimir 的 369GiB。请注意，该面板仅考虑本地文件系统的大小。Mimir 还使用 Google Cloud Storage 进行长期存储，额外占用了 149GiB： gsutil du -sh gs://mimir-bench-tsdb/ 149.7 GiB gs://mimir-bench-tsdb 一个重要的注意事项是，Ingester 在本地文件系统上存储 TSDB 块，默认为 24 小时，可以查看 -blocks-storage.tsdb.retention-period 参数。因此本地文件系统上占用的磁盘大小可以大大减少。但是，在此测试中，仍然只有长期存储比 VictoriaMetrics 本地存储占用的空间多 3 倍。 在测试之后，还发现了其他细节。Compactor 在多个时间间隔内运行合并对象存储的数据块的压缩作业：默认为 2h、12h 和 24h。由于测试只运行了 24 小时，所以不可能发生所有的压缩作业。正确的压缩比较将需要运行多天的基准测试。 此外 VictoriaMetrics 的 CPU 使用率低于 Mimir 的： Mimir 和 VictoriaMetrics 的 CPU 使用率 对于好奇的读者，可以从仪表盘快照 https://snapshots.raintank.io/dashboard/snapshot/1lXGSoVm6xVDtKVZ8LFQvBCG1uDYKChJ?viewPanel=2 中查看有关每个 pod CPU 使用情况的详细信息。这些指标再次证明，这两个解决方案都具有非常不同的架构和组件设计。对于 Mimir 来说，最大的 CPU 用户是 Ingester —— 仅他们就平均消耗了 13 个 CPU 核，高峰时达到 18 个，利用率为其极限的 80%。对于 VictoriaMetrics 来说，vmselects 使用了大部分 CPU：平均 7 个 CPU，峰值高达 12 个 CPU 内核，利用率为其极限的 70%。在这个基准测试中，VictoriaMetrics 平均消耗的 CPU 比 Mimir 少 1.7 倍。 Mimir 和 VictoriaMetrics 的内存使用量也不同： Mimir 和 VictoriaMetrics 的内存使用 在此基准测试中，与 Mimir 相比，VictoriaMetrics 使用的内存少了大约 5 倍。可以在仪表盘快照 https://snapshots.raintank.io/dashboard/snapshot/1lXGSoVm6xVDtKVZ8LFQvBCG1uDYKChJ 中找到组件之间的更详细比较。最有价值的收获是 Mimir 的 Ingester 运行非常接近极限，利用率为 80-90%，这意味着进一步增加负载可能会导致 OOM 异常。 读取查询的延迟具有以下统计信息： Mimir 和 VictoriaMetrics 的读取查询延迟 如前所述，读取负载只包括即时查询，并且由执行 node_exporter 指标的警报规则的外部 ruler 生成。规则列表包含轻量级查询和涉及数小时数据的重度查询。这会影响延迟，使得两个解决方案的第 50 百分位数在 100 到 500ms 之间，但是，Mimir 的第 99 百分位数最大为 47 秒，VictoriaMetrics 的最大为 20 秒。 我没有在这个基准中测试范围查询，这将是未来运行的一个很好的测试场景。 ","date":"2022-09-14","objectID":"/posts/6650ff/:5:2","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"副本 两种解决方案都有不同的复制方法。 在 Mimir，每个系列都由 distributors 复制给 ingesters，如果 Mimir 集群丢失了一个 ingester，则丢失的 ingester 持有的内存中的系列至少在另一个 ingester 中可用。因此，只要至少有一个副本还活着，读取查询就会成功。写入查询需要一定数量的副本才能成功，因此复制因子为 3 时，只有一个副本会丢失。 当 ingester 返回时，它会通过读取 WAL 来恢复其内存状态。恢复的 ingester 在离线时可能会丢失最近的数据，因此查询者需要查询所有 ingester 并合并数据以填补空白（如果有）。每隔 2 小时，每个 ingester 将 TSDB 数据块上传到对象存储，其中 compactor 合并数据块，如果有间隙，则对数据进行去重，所以只有一个样本被长期保存。 Mimir 的复制保护了在计划中的重启或由于硬件问题造成的意外 ingester 崩溃的情况下，不会丢失 ingester 内存中的最新数据。ingester 上的复制并不能保护无法到达的对象存储或对象存储上的数据损坏（由于人为错误或压缩错误）。对象存储的数据安全为存储提供商的责任。 在 VictoriaMetrics 中，每个系列都由 vminserts 复制到存储节点。这意味着，在任何时候，VictoriaMetrics 集群都会在存储节点上保存所有样本的 N 个副本。复制因子为 2 时，只有一个 vmstorage pod 可以丢失，以保持写入和读取成功。vmstorage 节点没有故意使用 WAL，因此它们可以非常快速地启动和运行。恢复的 vmstorage 在离线时可能会丢失最近的数据，因此 vmselects 需要查询所有 vmstorage 并合并数据以填补空白（如果有）。 VictoriaMetrics 的复制可防止丢失存储节点，这意味着，在整个时间范围内，一个或多个存储节点可以丢失、停用、更换或只是重新启动，而不会对整个时间范围内的读者产生影响。 虽然在这两种情况下，复制都是关于数据安全的，但它仍然不能保证数据安全。我们在 Google Cloud 中运行基准测试，使用 SSD 永久性磁盘 (SSD PD) 作为本地文件系统，并将 Cloud Storage 作为 Mimir 的对象存储。SSD PD 具有 5 个 9 的持久性，并在使用 x3 复制。如果我们假设 Cloud Storage 具有足够高的可用性而不进行复制，那么我们也可以对 SSD PD 进行相同的假设，并在数据可用性方面获得同等地位。即使 Cloud Storage 比 SSD PD 具有更高的可用性承诺 - Mimir 和 VictoriaMetrics 都会在 SSD PD 发生故障时面临写入和读取问题，尽管存在复制因素。 复制无法防止因资源不足或工作负载突然增加而导致的内存不足异常 (OOM)。在 VictoriaMetrics 和 Mimir 中，摄取的时间序列在各组件（分别为 vmstorage 和 ingester）之间均匀分片。因此，如果其中一个组件由于过载而开始出现 OOM，则很可能其他组件也会出现这种情况。这种情况可能会由于级联故障而导致数据丢失，组件开始一个接一个地崩溃。只有增加更多的资源或减少工作负荷，才能帮助摆脱这种情况。对于 VictoriaMetrics，我建议将复制因子设置为 2，以防止在维护或磁盘故障期间丢失一个 vmstorage 节点数据。如果需要更高的可用性，我们建议将复制下沉到 SSD PD 等持久存储或为集群分配额外资源。 Mimir 在复制后消除重复数据的能力非常酷。它不仅降低了存储成本，而且还应该提高了读取性能。VictoriaMetrics 从不删除复制数据的重复数据，以确保在某些 vmstorage 节点上的数据丢失时数据仍然可用于查询。 ","date":"2022-09-14","objectID":"/posts/6650ff/:5:3","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"总结 两种解决方案在处理负载方面都做得很好。没有发生故障或中断，系统在 24 小时的持续读写压力下保持稳定。但是，两种解决方案的不同架构都会产生影响。我可以肯定地说，Mimir 比 VictoriaMetrics 更耗费内存——对于相同的负载，它需要 5 倍以上的内存。在进一步扩展工作负载期间，内存是 Mimir 的瓶颈。即使我将 ingester 的复制因子从 3 降低到 2，它仍然需要比 VictoriaMetrics 更多的内存。 虽然没有一个集群达到它们的 CPU 限制，但 VictoriaMetrics 平均消耗的 CPU 比 Mimir 少了约 1.7 倍。 Mimir 的第 50 个百分点的延迟较好，但第 99 个百分点的延迟比 VictoriaMetrics 高一倍。目前尚不清楚是什么原因导致 Mimir 的延迟出现如此高的峰值。然而，阅读 Grafana Labs 团队进行其他测试文档，让我觉得 Mimir 在读取大时间范围内的指标时可以胜过 VictoriaMetrics，因为它需要扫描已经重复的数据。 VictoriaMetrics 的磁盘空间使用率较低。如果我们不考虑 ingesters 的本地文件系统，仅将 Mimir 的长期存储与 VictoriaMetrics 的磁盘使用情况进行比较——后者的磁盘空间使用量仍然是前者的 3 倍。虽然对象存储与 SSD PD 相比要便宜得多，但它也意味着数据访问的额外成本。阅读 Mimir 和 VictoriaMetrics 用户关于存储指标成本的评论会很有趣。 Mimir 有很大的规模潜力，每个组件都可以很容易地扩展。它还具有开箱即用的区域感知复制、对 ingester 和 querier 的可配置限制、高效的数据存储以及许多其他功能，例如查询分片。文档仍然需要一些优化，但总的来说我喜欢这个产品！ 在此基准测试中，与 Mimir 在相同硬件上相比，VictoriaMetrics 表现出更高的资源效率和性能。从操作上讲，VictoriaMetrics 扩展有点复杂，因为数据存储在有状态的 vmstorage 节点上。这使得缩减 vmstorage 节点的数量变得非常重要。我建议在规划集群的架构时，一定要有相当数量的 vmstorage 节点。 在数据方面，基准测试结果如下： 对于相同的工作负载，VictoriaMetrics 使用的 CPU 减少了 x1.7； 对于相同数量的活跃系列，VictoriaMetrics 使用的内存减少了 5 倍； VictoriaMetrics 在基准测试期间收集的 24 小时数据使用的存储空间减少了 3 倍。 ","date":"2022-09-14","objectID":"/posts/6650ff/:5:4","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"探索极限 在基准测试的极限探索回合中，我将只测试 VictoriaMetrics，因为 Mimir 负载的增加开始导致摄取器上的 OOM 异常。自上一个基准测试以来，仅更改了两个参数： # defines the number of node_exporter instances to scrape targetsCount: 8000 # defines how many pods of writers to deploy. # each replica will scrape targetsCount targets and will have # its own extra label `replica` attached to written time series. writeReplicas: 4 该更改将唯一目标的数量从 6000 个增加到 8000 个，并启动了 4 个 vmagent 副本（每个副本都有唯一标签），这总共使负载增加了 8000 * 4 / 6000 =~ 5 倍 。 可以在此处 https://gist.github.com/hagen1778/dec1d6c73fb9cd1ae8f715ec5356a88e 找到 helm chart values 的完整覆盖列表。 ","date":"2022-09-14","objectID":"/posts/6650ff/:6:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"快速统计 基准测试已运行 3 小时； 发送到 VictoriaMetrics 的样本总数约为 195 亿：180 万样本/秒 * 3 * 3600； 基准测试期间生成的新时间序列总数约为 3180 万：2750 万初始序列 + 24K 系列/分钟 * 60 分钟 * 3。 ","date":"2022-09-14","objectID":"/posts/6650ff/:6:1","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"结果 基准测试结果在上一轮基准中使用的仪表盘快照 https://snapshots.raintank.io/dashboard/snapshot/hoDTqThPYDoLQ6YfHeGsRVtTB80pMLG5 上获取。 VictoriaMetrics 的摄取率和活跃时间序列 我还抓取了 VictoriaMetrics 集群仪表板的快照 https://snapshots.raintank.io/dashboard/snapshot/J61xIFLm5oV2Q5MDaQ2kADq3JeG18RUQ ，其中包含每个组件的更详细指标。 VictoriaMetrics 在基准测试期间保持稳定，成功接收约 180 万个样本/秒和 2900 万个活跃时间序列（包括复制在内 5800 万个）。与之前的测试相比，CPU 利用率显着提高： VictoriaMetrics CPU 使用率 现在的平均使用率在 32 个可用核心中达到了约 26 个。如果检查每个 pod 的 CPU 利用率，我们会看到 vmstorage 平均以 80% 的速度运行，峰值高达 99%。这意味着进一步扩展需要更多的 CPU 用于 vmstorage 节点。 相反，VictoriaMetrics 只使用了允许内存的 1/4： VictoriaMetrics 内存使用 存储节点上内存的平均利用率约为 30%。这意味着，如果摄取率保持不变，则活跃时间序列的数量可以翻倍。 查询延迟随着负载的增加而显着降低： VictoriaMetrics 的读取查询延迟 仔细观察会发现 vmselects 之间的均衡性很差。VictoriaMetrics 集群 helm chart 使用标准 Kubernetes 服务进行负载均衡，但没有提供太大的灵活性。基于此基准测试的结果，我们计划 通过 nginx 使用更好的均衡策略 。类似于 Mimir 的 helm chart 中的操作方式。 ","date":"2022-09-14","objectID":"/posts/6650ff/:6:2","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["云原生"],"content":"总结 基准测试文章总是很有趣的。写这些文章的目的是为了证明不同解决方案的优势和劣势，以展示令人印象深刻的数字和结论。但是，我必须警告，没有一个基准测试是客观的，通常与现实的关联性很弱。我鼓励读者针对他们的具体需求、硬件和数据运行自己的基准测试。只有这样，你才能确定经过测试的解决方案是否符合你的需求和期望。 特别感谢 Nikolay Khramchikhin 协助撰写这篇博文，并感谢 Mimir 的工程团队提供咨询！ 原文地址：https://victoriametrics.com/blog/mimir-benchmark/ ","date":"2022-09-14","objectID":"/posts/6650ff/:7:0","tags":["grafana","monitor","转载"],"title":"Grafana Mimir 和 VictoriaMetrics 之间的性能测试","uri":"/posts/6650ff/"},{"categories":["Linux"],"content":"Iptales四表五链 ","date":"2022-09-14","objectID":"/posts/d2d1d6/:1:0","tags":["iptables"],"title":"Iptable的概念","uri":"/posts/d2d1d6/"},{"categories":["Linux"],"content":"四表五链关系概览 raw 表 mangle 表 nat 表 filter 表 PREROUTING 链 ✅ ✅ ✅ ❌ INPUT 链 ❌ ✅ ✅ ✔ FORWARD 链 ❌ ✅ ❌ ✅ OUTPUT 链 ✅ ✅ ✅ ✅ POSTROUTING 链 ❌ ✅ ✅ ❌ Centos6中NAT表不支持INPUT链 ✅ 表示链支持在表操作, ❌ 表示链不支持在表操作, ✔ 表示部分支持 iptables中同一条链四张表的执行优先级 raw -\u003e mangle -\u003e nat -\u003e filter ","date":"2022-09-14","objectID":"/posts/d2d1d6/:1:1","tags":["iptables"],"title":"Iptable的概念","uri":"/posts/d2d1d6/"},{"categories":["Linux"],"content":"数据经过防火墙的流向 ","date":"2022-09-14","objectID":"/posts/d2d1d6/:1:2","tags":["iptables"],"title":"Iptable的概念","uri":"/posts/d2d1d6/"},{"categories":["Linux"],"content":"规则概念 匹配条件 基本匹配条件 源地址 Source IP , 目标地址 Destination IP 扩展匹配条件 源端口 Source Port , 目标端口 Destination Port 处理动作 序号 名称 描述 1 ACCEPT 允许数据包通过 2 DROP 直接丢弃数据包，不给任何回应信息，客户端直到超时才做出反应 3 REJECT 拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息 4 SNAT 源地址转换，解决内网用户用同一个公网地址上网的问题 5 MASQUERADE SNAT的一种特殊形式，适用于动态的临时会变的IP上 6 DNAT 目标地址转换 7 REDIRECT DNAT的特殊形式，重定向至在本机做端口映射 8 LOG 在/var/log/messages文件记录日志信息，然后将数据包传递给下一条规则 ","date":"2022-09-14","objectID":"/posts/d2d1d6/:1:3","tags":["iptables"],"title":"Iptable的概念","uri":"/posts/d2d1d6/"},{"categories":["云原生"],"content":"Prometheus Operator 安装完成后会有很多默认的监控指标，一不注意就大量的报警产生，所以我们非常有必要了解下这些常用的监控指标，有部分指标很有可能对于我们自己的业务可有可无，所以可以适当的进行修改，这里我们就来对常用的几个指标进行简单的说明。 ","date":"2022-09-13","objectID":"/posts/adb863/:0:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1. Kubernetes 资源相关 ","date":"2022-09-13","objectID":"/posts/adb863/:1:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.1 CPUThrottlingHigh 关于 CPU 的 limit 合理性指标。查出最近5分钟，超过25%的 CPU 执行周期受到限制的容器。表达式： sum(increase(container_cpu_cfs_throttled_periods_total{container!=\"\", }[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace) \u003e ( 25 / 100 ) 相关指标： container_cpu_cfs_periods_total：容器生命周期中度过的 cpu 周期总数 container_cpu_cfs_throttled_periods_total：容器生命周期中度过的受限的 cpu 周期总数 ","date":"2022-09-13","objectID":"/posts/adb863/:1:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.2 KubeCPUOvercommit 集群 CPU 过度使用。CPU 已经过度使用无法容忍节点故障，节点资源使用的总量超过节点的 CPU 总量，所以如果有节点故障将影响集群资源运行因为所需资源将无法被分配。 表达式： sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{}) / sum(kube_node_status_allocatable_cpu_cores) \u003e (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores) 相关指标： kube_pod_container_resource_requests_cpu_cores：资源 CPU 使用的 cores 数量 kube_node_status_allocatable_cpu_cores：节点 CPU cores 数量 ","date":"2022-09-13","objectID":"/posts/adb863/:1:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.3 KubeMemoryOvercommit 集群内存过度使用。内存已经过度使用无法容忍节点故障，节点资源使用的总量超过节点的内存总量，所以如果有节点故障将影响集群资源运行因为所需资源将无法被分配。表达式： sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{}) / sum(kube_node_status_allocatable_memory_bytes) \u003e (count(kube_node_status_allocatable_memory_bytes)-1) / count(kube_node_status_allocatable_memory_bytes) 相关指标： kube_pod_container_resource_requests_memory_bytes：资源内存使用的量 kube_node_status_allocatable_memory_bytes：节点内存量 ","date":"2022-09-13","objectID":"/posts/adb863/:1:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.4 KubeCPUQuotaOvercommit 集群CPU是否超分。查看 CPU 资源分配的额度是否超过进群总额度 表达式： sum(kube_pod_container_resource_limits_cpu_cores{job=\"kube-state-metrics\"}) / sum(kube_node_status_allocatable_cpu_cores) \u003e 1.1 相关指标： kube_pod_container_resource_limits_cpu_cores：资源分配的 CPU 资源额度 kube_node_status_allocatable_cpu_cores：节点 CPU 总量 ","date":"2022-09-13","objectID":"/posts/adb863/:1:4","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.5 KubeMemoryQuotaOvercommit 集群超分内存，查看内存资源分配的额度是否超过进群总额度 表达式： sum(kube_pod_container_resource_limits_memory_bytes{job=\"kube-state-metrics\"}) / sum(kube_node_status_allocatable_memory_bytes{job=\"kube-state-metrics\"}) \u003e 1.1 相关指标: kube_pod_container_resource_limits_memory_bytes：资源配额内存量 kube_node_status_allocatable_memory_bytes：节点内存量 ","date":"2022-09-13","objectID":"/posts/adb863/:1:5","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.6 KubeMEMQuotaExceeded 命名空间级内存资源使用的比例，关乎资源配额。当使用 request 和 limit 限制资源时，使用值和最大值还是有一点区别，当有 request 时说明最低分配了这么多资源。需要注意当 request 等于 limit 时那么说明资源已经是100%已经分配使用当监控告警发出的时候需要区分。表达式： sum (kube_pod_container_resource_requests_memory_bytes{job=\"kube-state-metrics\"} ) by (namespace)/ (sum(kube_pod_container_resource_limits_memory_bytes{job=\"kube-state-metrics\"}) by (namespace)) \u003e 0.8 相关指标: kube_pod_container_resource_requests_memory_bytes：内存资源使用量 kube_pod_container_resource_limits_memory_bytes：内存资源最大值 ","date":"2022-09-13","objectID":"/posts/adb863/:1:6","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"1.7 KubeCPUQuotaExceeded 命名空间级 CPU 资源使用的比例，关乎资源配额。当使用 request 和 limit 限制资源时，使用值和最大值还是有一点区别，当有 request 时说明最低分配了这么多资源。需要注意当 request 等于 limit 时那么说明资源已经是100%已经分配使用当监控告警发出的时候需要区分。 表达式： sum (kube_pod_container_resource_requests_cpu_cores{job=\"kube-state-metrics\"} ) by (namespace)/ (sum(kube_pod_container_resource_limits_cpu_cores{job=\"kube-state-metrics\"}) by (namespace)) \u003e 0.8 相关指标: kube_pod_container_resource_requests_cpu_cores：CPU 使用量 kube_pod_container_resource_limits_cpu_cores：CPU 限额最大值 ","date":"2022-09-13","objectID":"/posts/adb863/:1:7","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"2. Kubernetes 存储相关 ","date":"2022-09-13","objectID":"/posts/adb863/:2:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"2.1 KubePersistentVolumeFillingUp PVC 容量监控 表达式： kubelet_volume_stats_available_bytes{job=\"kubelet\", metrics_path=\"/metrics\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\", metrics_path=\"/metrics\"} \u003c 0.3 相关指标： kubelet_volume_stats_available_bytes：剩余空间 kubelet_volume_stats_capacity_bytes：空间总量 ","date":"2022-09-13","objectID":"/posts/adb863/:2:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"2.2 KubePersistentVolumeFillingUp 磁盘空间耗尽预测：通过PVC资源使用6小时变化率预测 接下来4天的磁盘使用率 表达式： (kubelet_volume_stats_available_bytes{job=\"kubelet\", metrics_path=\"/metrics\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\", metrics_path=\"/metrics\"} ) \u003c 0.4 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\", metrics_path=\"/metrics\"}[6h], 4 * 24 * 3600) \u003c 0 相关指标: kubelet_volume_stats_available_bytes：剩余空间 kubelet_volume_stats_capacity_bytes：空间总量 ","date":"2022-09-13","objectID":"/posts/adb863/:2:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"2.3 KubePersistentVolumeErrors PV 使用状态监控。 表达式： kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",job=\"kube-state-metrics\"} 相关指标： kube_persistentvolume_status_phase：PV 使用状态 ","date":"2022-09-13","objectID":"/posts/adb863/:2:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"3. kubernetes system 相关 ","date":"2022-09-13","objectID":"/posts/adb863/:3:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"3.1 KubeVersionMismatch 组件版本与当前集群版本是否有差异。对比组件版本是否有差异，默认为1 。 表达式： count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~\"kube-dns|coredns\"},\"gitVersion\",\"$1\",\"gitVersion\",\"(v[0-9]*.[0-9]*.[0-9]*).*\"))) 相关指标： kubernetes_build_info：获取组件信息 ","date":"2022-09-13","objectID":"/posts/adb863/:3:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"3.2 KubeClientErrors 客户端访问某些接口的错误率。 表达式： (sum(rate(rest_client_requests_total{code=~\"5..\"}[5m])) by (instance, job) / sum(rate(rest_client_requests_total[5m])) by (instance, job)) \u003e 0.01 相关指标： rest_client_requests_total：状态码 ","date":"2022-09-13","objectID":"/posts/adb863/:3:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"4. APIServer 相关 ","date":"2022-09-13","objectID":"/posts/adb863/:4:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"4.1 KubeAPIErrorsHigh APIServer 请求错误率。5分钟内 APIServer 请求错误率。 表达式： sum(rate(apiserver_request_total{job=\"apiserver\",code=~\"5..\"}[5m])) by (resource,subresource,verb) / sum(rate(apiserver_request_total{job=\"apiserver\"}[5m])) by (resource,subresource,verb) \u003e 0.05 相关指标： apiserver_request_total：APIServer 请求数 ","date":"2022-09-13","objectID":"/posts/adb863/:4:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"4.2 KubeClientCertificateExpiration kubelet 客户端证书过期。监测证书状态30天告警和7天告警。 表达式： apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} \u003e 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) \u003c 2592000 apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} \u003e 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) \u003c 604800 相关指标： apiserver_client_certificate_expiration_seconds_count：证书有效剩余时间 ","date":"2022-09-13","objectID":"/posts/adb863/:4:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"4.3 AggregatedAPIErrors 自定义注册的 APIServer 服务可用性监控，当检测到自定义注册的 APIServer 五分钟不用次数达到2次。 表达式： sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count[5m])) \u003e 2 相关指标: aggregator_unavailable_apiservice_count：监测自定义注册的 APIService 不可用次数。 ","date":"2022-09-13","objectID":"/posts/adb863/:4:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"4.4 KubeAPIDown APIserver 失联，监控 APIServer 服务，失联原因可能是服务 down 还可能是网络出现状况。 表达式： absent(up{job=\"apiserver\"} == 1) ","date":"2022-09-13","objectID":"/posts/adb863/:4:4","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"5. kubelet 相关 ","date":"2022-09-13","objectID":"/posts/adb863/:5:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"5.1 KubeNodeNotReady 节点是否处于就绪状态。检测节点是否为就绪状态，或者可能是 kubelet 服务down 了。 表达式： - kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0 相关指标： kube_node_status_condition：节点状态监测 ","date":"2022-09-13","objectID":"/posts/adb863/:5:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"5.2 KubeNodeUnreachable 节点状态为 Unreachable。 表达式： kube_node_spec_unschedulable{job=\"kube-state-metrics\"} == 1 ","date":"2022-09-13","objectID":"/posts/adb863/:5:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"5.3 KubeletTooManyPods 节点运行过多的 Pod，监测节点上运行的 Pods 数量。 表达式： max(max(kubelet_running_pod_count{job=\"kubelet\", metrics_path=\"/metrics\"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\", metrics_path=\"/metrics\"}) by(node) / max(kube_node_status_capacity_pods{job=\"kube-state-metrics\"} != 1) by(node) \u003e 0.95 相关指标： kubelet_running_pod_count：节点运行的 Pods 数量 kubelet_node_name：节点名称 kube_node_status_capacity_pods：节点可运行的最大 Pod 数量 ","date":"2022-09-13","objectID":"/posts/adb863/:5:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"5.4 KubeNodeReadinessFlapping 监测集群状态，查看集群内节点状态改变的频率。 表达式： sum(changes(kube_node_status_condition{status=\"true\",condition=\"Ready\"}[15m])) by (node) \u003e 2 ","date":"2022-09-13","objectID":"/posts/adb863/:5:4","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"5.5 KubeletDown 监控 kubelet 服务，down 或者网络出现问题。 表达式： absent(up{job=\"kubelet\", metrics_path=\"/metrics\"} == 1) ","date":"2022-09-13","objectID":"/posts/adb863/:5:5","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"6. 集群组件 ","date":"2022-09-13","objectID":"/posts/adb863/:6:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"6.1 KubeSchedulerDown KubeScheduler 失联，监测 KubeScheduler 是否正常。 表达式： absent(up{job=\"kube-scheduler\"} == 1) ","date":"2022-09-13","objectID":"/posts/adb863/:6:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"6.2 KubeControllerManagerDown 监测 KubeControllerManager 服务，Down 或者网络不通。 表达式： absent(up{job=\"kube-controller-manager\"} == 1) ","date":"2022-09-13","objectID":"/posts/adb863/:6:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7. 应用相关 ","date":"2022-09-13","objectID":"/posts/adb863/:7:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.1 KubePodCrashLooping Pod 重启时间，重启时间超过3m告警。 表达式： rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"}[5m]) * 60 * 3 \u003e 0 相关指标: kube_pod_container_status_restarts_total：重启状态0为正常 ","date":"2022-09-13","objectID":"/posts/adb863/:7:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.2 KubePodNotReady Pods 没有就绪，检测 Pod 是否就绪。 表达式： sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!=\"Job\"})) \u003e 0 相关指标： kube_pod_status_phase：Pod 状态 ","date":"2022-09-13","objectID":"/posts/adb863/:7:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.3 KubeDeploymentGenerationMismatch Deployment 部署失败，Deployment 生成的资源与定义的资源不匹配。 表达式： kube_deployment_status_observed_generation{job=\"kube-state-metrics\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\"} 相关指标： kube_deployment_status_observed_generation：Deployment 生成资源数 kube_deployment_metadata_generation：Deployment 定义资源数 ","date":"2022-09-13","objectID":"/posts/adb863/:7:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.4 KubeDeploymentReplicasMismatch 查看 Deplyment 副本是否达到预期。 表达式： ( kube_deployment_spec_replicas{job=\"kube-state-metrics\"} != kube_deployment_status_replicas_available{job=\"kube-state-metrics\"} ) and ( changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\"}[3m]) == 0 ) 相关指标： kube_deployment_spec_replicas 资源定义副本数 kube_deployment_status_replicas_available 正在运行副本数 kube_deployment_status_replicas_updated 更新的副本数 ","date":"2022-09-13","objectID":"/posts/adb863/:7:4","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.5 KubeStatefulSetReplicasMismatch 监测 StatefulSet 副本是否达到预期。 表达式： ( kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\"} ) and ( changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\"}[5m]) == 0 ) 相关指标： kube_statefulset_status_replicas_ready：就绪副本数 kube_statefulset_status_replicas：当前副本数 kube_statefulset_status_replicas_updated：更新的副本数 ","date":"2022-09-13","objectID":"/posts/adb863/:7:5","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.6 KubeStatefulSetUpdateNotRolledOut StatefulSet 更新失败且未回滚，对比版本号和副本数。 表达式： max without (revision) ( kube_statefulset_status_current_revision{job=\"kube-state-metrics\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\"} ) * ( kube_statefulset_replicas{job=\"kube-state-metrics\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\"} ) 相关指标： kube_statefulset_status_replicas：每个 StatefulSet 的副本数。 kube_statefulset_status_replicas_current：每个 StatefulSet 的当前副本数。 kube_statefulset_status_replicas_ready：每个StatefulSet 的就绪副本数。 kube_statefulset_status_replicas_updated：每个StatefulSet 的更新副本数。 kube_statefulset_status_observed_generation：StatefulSet 控制器观察到的生成。 kube_statefulset_replicas：StatefulSet 所需的副本数。 kube_statefulset_metadata_generation：表示 StatefulSet 所需状态的特定生成的序列号。 kube_statefulset_created：创建时间戳。 kube_statefulset_labels：Kubernetes 标签转换为 Prometheus 标签。 kube_statefulset_status_current_revision：指示用于按顺序(0，currentReplicas)生成 Pod 的StatefulSet 的版本。 kube_statefulset_status_update_revision：指示用于按顺序 [replicas-updatedReplicas，replicas] 生成 Pod 的 StatefulSet 的版本。 ","date":"2022-09-13","objectID":"/posts/adb863/:7:6","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.7 KubeDaemonSetRolloutStuck 监测 DaemonSet 是否处于就绪状态。 表达式： kube_daemonset_status_number_ready{job=\"kube-state-metrics\"} / kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"} \u003c 1.00 相关指标： kube_daemonset_status_number_ready：就绪的 DaemonSet kube_daemonset_status_desired_number_scheduled：应该调度的 DaemonSet 数量 ","date":"2022-09-13","objectID":"/posts/adb863/:7:7","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.8 KubeDaemonSetMisScheduled DaemonSet 运行在不该运行的节点上面。 表达式： kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"} \u003e 0 相关指标： kube_daemonset_status_number_misscheduled：运行在不该运行的节点状态 ","date":"2022-09-13","objectID":"/posts/adb863/:7:8","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"7.9 KubeContainerWaiting 监测哪些容器是在等待状态的。 表达式： sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\"}) \u003e 0 相关指标： kube_pod_container_status_waiting_reason：容器声明周期过程中的状态，无论是创建成功还是失败都应该是0。 ","date":"2022-09-13","objectID":"/posts/adb863/:7:9","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8. 节点相关 ","date":"2022-09-13","objectID":"/posts/adb863/:8:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.1 NodeClockNotSynchronising 主机与时间服务器失联。 表达式： min_over_time(node_timex_sync_status[5m]) == 0 相关指标： node_timex_sync_status：同步状态。 ","date":"2022-09-13","objectID":"/posts/adb863/:8:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.2 NodeClockSkewDetected 本地时间偏移量。 表达式： (node_timex_offset_seconds \u003e 0.05 and deriv(node_timex_offset_seconds[5m]) \u003e= 0 ) or ( node_timex_offset_seconds \u003c -0.05 and deriv(node_timex_offset_seconds[5m]) \u003c= 0) 相关指标： node_timex_offset_seconds：误差 ","date":"2022-09-13","objectID":"/posts/adb863/:8:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.3 NodeHighNumberConntrackEntriesUsed 链接状态跟踪。 表达式： (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) \u003e 0.75 相关指标： node_nf_conntrack_entries：链接状态跟踪表分配的数量 node_nf_conntrack_entries_limit：表总量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.4 NodeNetworkReceiveErrs 网卡接收错误量。 表达式： increase(node_network_receive_errs_total[2m]) \u003e 10 相关指标： node_network_receive_errs_total：接收错误总量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:4","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.5 NodeNetworkTransmitErrs 网卡传输错误量。 表达式： increase(node_network_transmit_errs_total[2m]) \u003e 10 相关指标： node_network_transmit_errs_total：传输错误总量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:5","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.6 NodeFilesystemAlmostOutOfFiles inode 数量监测 表达式： ( node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\"} * 100 \u003c 5 and node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\"} == 0 ) 相关指标： node_filesystem_files_free：空闲的 inode node_filesystem_files：inodes 总量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:6","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.7 NodeFilesystemFilesFillingUp inode 耗尽预测，以6小时曲线变化预测接下来24小时和4小时可能使用的 inodes。 表达式： (node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\"} * 100 \u003c 20 and predict_linear(node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\"}[6h], 4*60*60) \u003c 0 and node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\"} == 0) 相关指标： node_filesystem_files_free：空闲的 inode node_filesystem_files：inodes 总量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:7","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.8 NodeFilesystemAlmostOutOfSpace 分区容量使用率。 表达式： (node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\"} * 100 \u003c 10 and node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\"} == 0 ) 相关指标： node_filesystem_avail_bytes：空闲容量 node_filesystem_size_bytes：总容量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:8","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"8.9 NodeFilesystemSpaceFillingUp 分区容量耗尽预测，以6小时曲线变化预测接下来24小时和4小时可能使用的容量。 表达式： (node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\"} * 100 \u003c 15 and predict_linear(node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\"}[6h], 4*60*60) \u003c 0 and node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\"} == 0) 相关指标： node_filesystem_avail_bytes：空闲容量 node_filesystem_size_bytes：总容量 ","date":"2022-09-13","objectID":"/posts/adb863/:8:9","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9. Etcd 相关 ","date":"2022-09-13","objectID":"/posts/adb863/:9:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.1 Etcdlived etcd 存活检测。 表达式： up{job=\"etcd\"} \u003c 1 ","date":"2022-09-13","objectID":"/posts/adb863/:9:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.2 EtcdCluseterUnavailable etcd 集群健康检查，down 数量大于集群可允许故障数量。 表达式： count(up{job=\"etcd\"} == 0) \u003e (count(up{job=\"etcd\"}) / 2 - 1) ","date":"2022-09-13","objectID":"/posts/adb863/:9:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.3 EtcdLeaderCheck 检查 leader。 表达式： max(etcd_server_has_leader) != 1 ","date":"2022-09-13","objectID":"/posts/adb863/:9:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.4 EtcdBackendFsync etcd io 监测，后端提交 延时。 表达式： histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le)) \u003e 100 ","date":"2022-09-13","objectID":"/posts/adb863/:9:4","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.5 EtcdWalFsync etcd io 监测，文件同步到磁盘延时。 表达式： histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (instance, le)) \u003e 100 ","date":"2022-09-13","objectID":"/posts/adb863/:9:5","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.6 EtcdDbSize 检测数据库大小。 表达式： etcd_debugging_mvcc_db_total_size_in_bytes/1024/1024 \u003e 1024 ","date":"2022-09-13","objectID":"/posts/adb863/:9:6","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"9.7 EtcdGrpc Grpc 调用速率。表达式： sum(rate(grpc_server_handled_total{grpc_type=\"unary\"}[1m])) \u003e 100 ","date":"2022-09-13","objectID":"/posts/adb863/:9:7","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"10. CoreDNS 相关 ","date":"2022-09-13","objectID":"/posts/adb863/:10:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"10.1 DnsRequest DNS 查询速率，每分钟查询超过100告警。 表达式： sum(irate(coredns_dns_request_count_total{zone !=\"dropped\"}[1m])) \u003e 100 相关指标： coredns_dns_request_count_total：总查询数 ","date":"2022-09-13","objectID":"/posts/adb863/:10:1","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"10.2 DnsRequestFaild 异常查询，异常状态码，不是 NOERROR。 表达式： irate(coredns_dns_response_rcode_count_total{rcode!=\"NOERROR\"} [1m]) \u003e 0 相关指标： coredns_dns_response_rcode_count_total：查询返回状态码 DNS-Rcode： DNS-Rcode 作为 DNS 应答报文中有效的字段，主要用来说明 DNS 应答状态，是排查域名解析失败的重要指标。通常常见的 Rcode 值如下： Rcode 值为0，对应的 DNS 应答状态为 NOERROR，意思是成功的响应，即这个域名解析是成功 Rcode 值为2，对应的 DNS 应答状态为 SERVFAIL，意思是服务器失败，也就是这个域名的权威服务器拒绝响应或者响应 REFUSE，递归服务器返回 Rcode 值为 2 给 CLIENT Rcode 值为3，对应的 DNS 应答状态为 NXDOMAIN，意思是不存在的记录，也就是这个具体的域名在权威服务器中并不存在 Rcode 值为5，对应的 DNS 应答状态为 REFUSE，意思是拒绝，也就是这个请求源IP不在服务的范围内 ","date":"2022-09-13","objectID":"/posts/adb863/:10:2","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"10.3 DnsPanic DNS 恐慌值，可能收到攻击。 表达式： irate(coredns_panic_count_total[1m]) \u003e 100 ","date":"2022-09-13","objectID":"/posts/adb863/:10:3","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"参考链接 https://my.oschina.net/54188zz/blog/4305978 https://github.com/coreos/kube-prometheus https://github.com/kubernetes-monitoring/kubernetes-mixin ","date":"2022-09-13","objectID":"/posts/adb863/:11:0","tags":["prometheus","monitor"],"title":"Prometheus Operator 常用指标","uri":"/posts/adb863/"},{"categories":["云原生"],"content":"Istio 指标 ","date":"2022-09-13","objectID":"/posts/6899d1/:0:0","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"Istio 自己的 Metrics ","date":"2022-09-13","objectID":"/posts/6899d1/:1:0","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"标准指标说明 参考：https://istio.io/latest/docs/reference/config/metrics/ Metrics 对于 HTTP、HTTP/2 和 GRPC 流量，Istio 默认生成以下指标： Request Count (istio_requests_total): This is a COUNTER incremented for every request handled by an Istio proxy. Request Duration (istio_request_duration_milliseconds): This is a DISTRIBUTION which measures the duration of requests. Request Size (istio_request_bytes): This is a DISTRIBUTION which measures HTTP request body sizes. Response Size (istio_response_bytes): This is a DISTRIBUTION which measures HTTP response body sizes. gRPC Request Message Count (istio_request_messages_total): This is a COUNTER incremented for every gRPC message sent from a client. gRPC Response Message Count (istio_response_messages_total): This is a COUNTER incremented for every gRPC message sent from a server. 对于 TCP 流量，Istio 生成以下指标： Tcp Bytes Sent (istio_tcp_sent_bytes_total): This is a COUNTER which measures the size of total bytes sent during response in case of a TCP connection. Tcp Bytes Received (istio_tcp_received_bytes_total): This is a COUNTER which measures the size of total bytes received during request in case of a TCP connection. Tcp Connections Opened (istio_tcp_connections_opened_total): This is a COUNTER incremented for every opened connection. Tcp Connections Closed (istio_tcp_connections_closed_total): This is a COUNTER incremented for every closed connection. Prometheus 的 Labels Reporter: This identifies the reporter of the request. It is set to destination if report is from a server Istio proxy and source if report is from a client Istio proxy or a gateway. Source Workload: This identifies the name of source workload which controls the source, or “unknown” if the source information is missing. Source Workload Namespace: This identifies the namespace of the source workload, or “unknown” if the source information is missing. Source Principal: This identifies the peer principal of the traffic source. It is set when peer authentication is used. Source App: This identifies the source application based on app label of the source workload, or “unknown” if the source information is missing. Source Version: This identifies the version of the source workload, or “unknown” if the source information is missing. Destination Workload: This identifies the name of destination workload, or “unknown” if the destination information is missing. Destination Workload Namespace: This identifies the namespace of the destination workload, or “unknown” if the destination information is missing. Destination Principal: This identifies the peer principal of the traffic destination. It is set when peer authentication is used. Destination App: This identifies the destination application based on app label of the destination workload, or “unknown” if the destination information is missing. Destination Version: This identifies the version of the destination workload, or “unknown” if the destination information is missing. Destination Service: This identifies destination service host responsible for an incoming request. Ex: details.default.svc.cluster.local. Destination Service Name: This identifies the destination service name. Ex: “details”. Destination Service Namespace: This identifies the namespace of destination service. Request Protocol: This identifies the protocol of the request. It is set to request or connection protocol. Response Code: This identifies the response code of the request. This label is present only on HTTP metrics. Connection Security Policy: This identifies the service authentication policy of the request. It is set to mutual_tls when Istio is used to make communication secure and report is from destination. It is set to unknown when report is from source since security policy cannot be properly populated. Response Flags: Additional details about the response or connection from proxy. In case of Envoy, see %RESPONSE_FLAGS% in Envoy Access Log for more detail. Canonical Service: A workload belongs to exactly one canonical s","date":"2022-09-13","objectID":"/posts/6899d1/:1:1","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"使用 istio-proxy 与应用的 Metrics 整合输出 参考：https://istio.io/v1.14/docs/ops/integrations/prometheus/#option-1-metrics-merging Istio 能够完全通过 prometheus.io annotations 来控制抓取。虽然 prometheus.io annotations 不是 Prometheus 的核心部分，但它们已成为配置抓取的事实标准。 此选项默认启用，但可以通过在 安装 期间传递 --set meshConfig.enablePrometheusMerge=false 来禁用。启用后，将向所有数据平面 pod 添加适当的 prometheus.io annotations 以设置抓取。如果这些注释已经存在，它们将被覆盖。使用此选项，Envoy sidecar 会将 Istio 的指标与应用程序指标合并。合并后的指标将从 /stats/prometheus:15020 中抓取。 此选项以明文形式公开所有指标。 定制：为 Metrics 增加维度 参考： https://istio.io/latest/docs/tasks/observability/metrics/customize-metrics/#custom-statistics-configuration 如，增加端口、与 HTTP HOST 头 维度。 apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: telemetry: v2: prometheus: configOverride: inboundSidecar: metrics: - name: requests_total dimensions: destination_port: string(destination.port) request_host: request.host outboundSidecar: metrics: - name: requests_total dimensions: destination_port: string(destination.port) request_host: request.host gateway: metrics: - name: requests_total dimensions: destination_port: string(destination.port) request_host: request.host 使用以下命令将以下 annotation 应用到所有注入的 pod，其中包含要提取到 Prometheus 时间序列 的维度列表： 仅当您的维度不在 [DefaultStatTags 列表] 中时才需要此步骤（https://github.com/istio/istio/blob/release-1.14/pkg/bootstrap/config.go） apiVersion: apps/v1 kind: Deployment spec: template: # pod template metadata: annotations: sidecar.istio.io/extraStatTags: destination_port,request_host 要在网格范围内启用额外 Tag ，您可以将 extraStatTags 添加到网格配置中： meshConfig: defaultConfig: extraStatTags: - destination_port - request_host 参考 : https://istio.io/latest/docs/reference/config/proxy_extensions/stats/#MetricConfig 定制：加入 request / response 元信息维度 可以把 request 或 repsonse 里一些基础信息 加入到 指标的维度。如，URL Path，这在需要为相同服务分隔统计不同 REST API 的指标时，相当有用。 参考 : https://istio.io/latest/docs/tasks/observability/metrics/classify-metrics/ ","date":"2022-09-13","objectID":"/posts/6899d1/:1:2","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"工作原理 istio stat filter 使用 Istio 在自己的定制版本 Envoy 中，加入了 stats-filter 插件，用于计算 Istio 自己想要的指标： $ k -n istio-system get envoyfilters.networking.istio.io stats-filter-1.14 -o yaml apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: annotations: labels: install.operator.istio.io/owning-resource-namespace: istio-system istio.io/rev: default operator.istio.io/component: Pilot operator.istio.io/version: 1.14.3 name: stats-filter-1.14 namespace: istio-system spec: configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_OUTBOUND listener: filterChain: filter: name: envoy.filters.network.http_connection_manager subFilter: name: envoy.filters.http.router proxy: proxyVersion: ^1\\.14.* patch: operation: INSERT_BEFORE value: name: istio.stats typed_config: '@type': type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value: config: configuration: '@type': type.googleapis.com/google.protobuf.StringValue value: | { \"debug\": \"false\", \"stat_prefix\": \"istio\" } root_id: stats_outbound vm_config: code: local: inline_string: envoy.wasm.stats runtime: envoy.wasm.runtime.null vm_id: stats_outbound - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND listener: filterChain: filter: name: envoy.filters.network.http_connection_manager subFilter: name: envoy.filters.http.router proxy: proxyVersion: ^1\\.14.* patch: operation: INSERT_BEFORE value: name: istio.stats typed_config: '@type': type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value: config: configuration: '@type': type.googleapis.com/google.protobuf.StringValue value: | { \"debug\": \"false\", \"stat_prefix\": \"istio\", \"disable_host_header_fallback\": true, \"metrics\": [ { \"dimensions\": { \"destination_cluster\": \"node.metadata['CLUSTER_ID']\", \"source_cluster\": \"downstream_peer.cluster_id\" } } ] } root_id: stats_inbound vm_config: code: local: inline_string: envoy.wasm.stats runtime: envoy.wasm.runtime.null vm_id: stats_inbound - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: envoy.filters.network.http_connection_manager subFilter: name: envoy.filters.http.router proxy: proxyVersion: ^1\\.14.* patch: operation: INSERT_BEFORE value: name: istio.stats typed_config: '@type': type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value: config: configuration: '@type': type.googleapis.com/google.protobuf.StringValue value: | { \"debug\": \"false\", \"stat_prefix\": \"istio\", \"disable_host_header_fallback\": true } root_id: stats_outbound vm_config: code: local: inline_string: envoy.wasm.stats runtime: envoy.wasm.runtime.null vm_id: stats_outbound priority: -1 istio stat Plugin 实现 https://github.com/istio/proxy/blob/release-1.14/extensions/stats/plugin.cc 内置的 Metric: const std::vector\u003cMetricFactory\u003e\u0026 PluginRootContext::defaultMetrics() { static const std::vector\u003cMetricFactory\u003e default_metrics = { // HTTP, HTTP/2, and GRPC metrics MetricFactory{\"requests_total\", MetricType::Counter, [](::Wasm::Common::RequestInfo\u0026) -\u003e uint64_t { return 1; }, static_cast\u003cuint32_t\u003e(Protocol::HTTP) | static_cast\u003cuint32_t\u003e(Protocol::GRPC), count_standard_labels, /* recurrent */ false}, MetricFactory{\"request_duration_milliseconds\", MetricType::Histogram, [](::Wasm::Common::RequestInfo\u0026 request_info) -\u003e uint64_t { return request_info.duration /* in nanoseconds */ / 1000000; }, static_cast\u003cuint32_t\u003e(Protocol::HTTP) | static_cast\u003cuint32_t\u003e(Protocol::GRPC), count_standard_labels, /* recurrent */ false}, MetricFactory{\"request_bytes\", MetricType::Histogram, [](::Wasm::Common::RequestInfo\u0026 request_info) -\u003e uint64_t { return request_info.request_size; }, static_cast\u003cuint32_t\u003e(Protocol::HTTP) | static_cast\u003cuint32_t\u003e(Protocol::GRPC), count_standard_labels, /* recurrent */ false}, MetricFactory{\"response_bytes\", MetricType::Histogram, [](::Wasm::Common::RequestInfo\u0026 request_info) -\u003e uint64_t { return reque","date":"2022-09-13","objectID":"/posts/6899d1/:1:3","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"Envoy 内置的 Metrics Istio 默认用 istio-agent 去整合 Envoy 的 metrics。 而 Istio 默认打开的 Envoy 内置 Metrics 很少： 见：https://istio.io/latest/docs/ops/configuration/telemetry/envoy-stats/ cluster_manager listener_manager server cluster.xds-grpc ","date":"2022-09-13","objectID":"/posts/6899d1/:2:0","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"定制 Envoy 内置的 Metrics 参考：https://istio.io/latest/docs/ops/configuration/telemetry/envoy-stats/ 如果要配置 Istio Proxy 以记录 其它 Envoy 原生的指标，您可以将 ProxyConfig.ProxyStatsMatcher 添加到网格配置中。 例如，要全局启用断路器、重试和上游连接的统计信息，您可以指定 stats matcher，如下所示： 代理需要重新启动以获取统计匹配器配置。 apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: proxyStatsMatcher: inclusionRegexps: - \".*circuit_breakers.*\" inclusionPrefixes: - \"upstream_rq_retry\" - \"upstream_cx\" 您还可以使用 proxy.istio.io/config annotation 为个别代码指定配置。 例如，要配置与上面相同的统计信息，您可以将 annotation 添加到 gateway proxy 或 workload，如下所示： metadata: annotations: proxy.istio.io/config: |- proxyStatsMatcher: inclusionRegexps: - \".*circuit_breakers.*\" inclusionPrefixes: - \"upstream_rq_retry\" - \"upstream_cx\" ","date":"2022-09-13","objectID":"/posts/6899d1/:2:1","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"原理 下面，看看 Istio 默认配置下，如何配置 Envoy。 istioctl proxy-config bootstrap fortio-server | yq eval -P \u003e envoy-config-bootstrap-default.yaml 输出： bootstrap: ... statsConfig: statsTags: # 从指标名中抓取 Tag(prometheus label) - tagName: cluster_name regex: ^cluster\\.((.+?(\\..+?\\.svc\\.cluster\\.local)?)\\.) - tagName: tcp_prefix regex: ^tcp\\.((.*?)\\.)\\w+?$ - tagName: response_code regex: (response_code=\\.=(.+?);\\.;)|_rq(_(\\.d{3}))$ - tagName: response_code_class regex: _rq(_(\\dxx))$ - tagName: http_conn_manager_listener_prefix regex: ^listener(?=\\.).*?\\.http\\.(((?:[_.[:digit:]]*|[_\\[\\]aAbBcCdDeEfF[:digit:]]*))\\.) ... useAllDefaultTags: false statsMatcher: inclusionList: patterns: # 选择要记录的指标 - prefix: reporter= - prefix: cluster_manager - prefix: listener_manager - prefix: server - prefix: cluster.xds-grpc ## 只记录 xDS cluster. 即不记录用户自己服务的 cluster !!! - prefix: wasm - suffix: rbac.allowed - suffix: rbac.denied - suffix: shadow_allowed - suffix: shadow_denied - prefix: component 这时，如果修改 pod 的定义为： annotations: proxy.istio.io/config: |- proxyStatsMatcher: inclusionRegexps: - \"cluster\\\\..*fortio.*\" #proxy upstream(outbound) - \"cluster\\\\..*inbound.*\" #proxy upstream(inbound，这里一般就是指到同一 pod 中运行的应用了) - \"http\\\\..*\" - \"listener\\\\..*\" 产生新的 Envoy 配置： \"stats_matcher\": { \"inclusion_list\": { \"patterns\": [ { \"prefix\": \"reporter=\" }, { \"prefix\": \"cluster_manager\" }, { \"prefix\": \"listener_manager\" }, { \"prefix\": \"server\" }, { \"prefix\": \"cluster.xds-grpc\" }, { \"safe_regex\": { \"google_re2\": {}, \"regex\": \"cluster\\\\..*fortio.*\" } }, { \"safe_regex\": { \"google_re2\": {}, \"regex\": \"cluster\\\\..*inbound.*\" } }, { \"safe_regex\": { \"google_re2\": {}, \"regex\": \"http\\\\..*\" } }, { \"safe_regex\": { \"google_re2\": {}, \"regex\": \"listener\\\\..*\" } }, ","date":"2022-09-13","objectID":"/posts/6899d1/:2:2","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"总结：Istio-Proxy 指标地图 要做好监控，首先要深入了解指标原理。而要了解指标原理，当然要知道指标是产生流程中的什么位置，什么组件。看完上面关于 Envoy 与 Istio 的指标说明后。可以大概得到以下结论： 本节的实验环境说明见于： {ref}`appendix-lab-env/appendix-lab-env-base:简单分层实验环境` 本文出自https://istio-insider.readthedocs.io/ ","date":"2022-09-13","objectID":"/posts/6899d1/:3:0","tags":["istio","转载"],"title":"Istio性能指标","uri":"/posts/6899d1/"},{"categories":["云原生"],"content":"Envoy 指标 ","date":"2022-09-13","objectID":"/posts/0affe7/:0:0","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"Envoy 指标概述 Envoy 的主要目标之一是使网络易于理解。 Envoy 会根据其配置方式产生大量统计信息。一般来说，统计数据(指标)分为三类： Downstream：Downstream 指标与外来的连接/请求有关。它们由 listener、HTTP connection manager(HCM)、TCP proxy filter 等产生。 Upstream：Upstream 指标与外向的连接/请求有关。它们由 connection pool、router filter、tcp proxy filter等产生。 Server：Server 指标信息描述 Envoy 服务器实例的运作情况。服务器正常运行时间或分配的内存量等统计信息。 在最简单场景下，单个 Envoy Proxy 通常涉及 Downstream 和 Upstream 统计数据。这两种指标反映了取该 网络节点 的运行情况。来自整个网格的统计数据提供了每个 网络节点 和整体网络健康状况的非常详细的汇总信息。Envoy 的文档对这些指标有一些简单的说明。 ","date":"2022-09-13","objectID":"/posts/0affe7/:1:0","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"Tag Envoy 的指标还有两个子概念，支持在指标中使用： 标签(tags)/维度(dimensions) 。这里的 tags 对等于 Prometheus 指标的 label。意义上，可以理解为：分类维度。 Envoy 的 指标 由规范的字符串来标识。这些字符串的动态部分（子字符串）被提取成为 标签(tag)。可以通过指定 tag 提取规则(Tag Specifier configuration.) 来定制 tag 。 举个例子： ### 1. 原始的 Envoy 指标 ### $ kubectl exec fortio-server -c istio-proxy -- curl 'localhost:15000/stats' # 返回： cluster.outbound|8080||fortio-server-l2.mark.svc.cluster.local.external.upstream_rq_2xx: 300 # 其中： # - `outbound|8080||fortio-server-l2.mark.svc.cluster.local` 部分是 upstream cluster 的名字。可以正则提取作为 tag。 # - `2xx` 部分是 HTTP Status Code 的分类。可以正则提取作为 tag。 下文将有这个提取规则的配置说明。 ### 2. 给 Prometheus 的指标 ### $ kubectl exec fortio-server -c istio-proxy -- curl 'localhost:15000/stats?format=prometheus' | grep 'outbound|8080||fortio-server-l2' | grep 'external.upstream_rq' # 返回： envoy_cluster_external_upstream_rq{response_code_class=\"2xx\",cluster_name=\"outbound|8080||fortio-server-l2.mark.svc.cluster.local\"} 300 ","date":"2022-09-13","objectID":"/posts/0affe7/:1:1","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"指标数据类型 Envoy 发出三种类型的值作为统计信息： 计数器(Counters)：无符号整数，只会增加而不会减少。例如，总请求。 仪表(Gauges)：增加和减少的无符号整数。例如，当前活动的请求。 直方图(Histograms)：作为指标流的一部分的无符号整数，然后由收集器聚合以最终产生汇总的百分位值(percentile，即平常说的 P99/P50/Pxx)。例如，Upsteam 响应时间。 在 Envoy 的内部实现中，Counters 和 Gauges 被分批并定期刷新以提高性能。Histograms 在接收时写入。 ","date":"2022-09-13","objectID":"/posts/0affe7/:1:2","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"指标释义 从指标的产出地点来划分，可以分为： cluster manager : 面向 upstream 的 L3/L4/L7 层指标 http connection manager(HCM) ： 面向 upstream \u0026 downstream 的 L7 层指标 listeners : 面向 downstream 的 L3/L4 层指标 server(全局) watch dog 下面我只选择了部分关键的性能指标来简单说明。 ","date":"2022-09-13","objectID":"/posts/0affe7/:2:0","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"cluster manager Envoy 文档:cluster manager stats 上面文档已经说得比较详细了。我只补充一些在性能调优时需要关注的方面。那么，一般需要关注什么指标？ 我们从著名的 Utilization Saturation and Errors (USE) 方法学来分析。 利用率(Utilization): upstream_cx_total (Counter): 连接数 upstream_rq_active 饱和度(Saturation): upstream_rq_time (Histogram): 响应时间 upstream_cx_connect_ms (Histogram) upstream_cx_rx_bytes_buffered upstream_cx_tx_bytes_buffered upstream_rq_pending_total upstream_rq_pending_active (Gauge) 错误(Error): upstream_cx_connect_fail (Counter): 连接失败数 upstream_cx_connect_timeout (Counter): 连接超时数 upstream_cx_overflow (Counter): 集群连接断路器溢出的总次数 upstream_cx_pool_overflow upstream_cx_destroy_local_with_active_rq upstream_cx_destroy_remote_with_active_rq upstream_rq_timeout upstream_rq_retry upstream_rq_rx_reset upstream_rq_tx_reset 其它： upstream_rq_total (Counter): TPS (吞吐) upstream_cx_destroy_local (Counter): Envoy 主动断开的连接计数 upstream_cx_destroy_remote (Counter): Envoy 被动断开的连接计数 upstream_cx_length_ms (Histogram) ","date":"2022-09-13","objectID":"/posts/0affe7/:2:1","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"http connection manager(HCM) Envoy 文档:http connection manager(HCM) stats 可以认为，这是面向 downstream \u0026 部分 upstream 的 L7 层指标 利用率(Utilization): downstream_cx_total downstream_cx_active downstream_cx_http1_active downstream_rq_total downstream_rq_http1_total downstream_rq_active 饱和度(Saturation): downstream_cx_rx_bytes_buffered downstream_cx_tx_bytes_buffered downstream_flow_control_paused_reading_total downstream_flow_control_resumed_reading_total 错误(Error): downstream_cx_destroy_local_active_rq downstream_cx_destroy_remote_active_rq downstream_rq_rx_reset downstream_rq_tx_reset downstream_rq_too_large downstream_rq_max_duration_reached downstream_rq_timeout downstream_rq_overload_close rs_too_large 其它： downstream_cx_destroy_remote downstream_cx_destroy_local downstream_cx_length_ms ","date":"2022-09-13","objectID":"/posts/0affe7/:2:2","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"listeners Envoy 文档:listener stats 可以认为，这是 downstream 的 L3/L4 层的指标。 利用率(Utilization): downstream_cx_total downstream_cx_active 饱和度(Saturation): downstream_pre_cx_active 错误(Error): downstream_cx_transport_socket_connect_timeout downstream_cx_overflow no_filter_chain_match downstream_listener_filter_error no_certificate 其它： downstream_cx_length_ms ","date":"2022-09-13","objectID":"/posts/0affe7/:2:3","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"server Envoy 基础信息指标 Envoy 文档:server stats 利用率(Utilization): concurrency 错误(Error): days_until_first_cert_expiring ","date":"2022-09-13","objectID":"/posts/0affe7/:2:4","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"watch dog Envoy 文档: Watchdog Envoy 还包括一个可配置的看门狗系统，它可以在 Envoy 没有响应时增加统计数据并选择性地终止服务器。 系统有两个独立的看门狗配置，一个用于主线程，一个用于工作线程； 因为不同的线程有不同的工作负载。 这些统计数据有助于从高层次上理解 Envoy 的事件循环是否因为它正在做太多工作、阻塞或没有被操作系统调度而没有响应。 饱和度(Saturation): watchdog_mega_miss(Counter): mega 未命中数 watchdog_miss(Counter): 未命中数 如果你对 watchdog 机制的兴趣，可以参考： https://github.com/envoyproxy/envoy/issues/11391 https://github.com/envoyproxy/envoy/issues/11388 ","date":"2022-09-13","objectID":"/posts/0affe7/:2:5","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"Event loop Envoy 文档: Event loop Envoy 架构旨在通过在少量线程上运行事件循环来优化可扩展性和资源利用率。 \"main\" 线程负责控制面处理，每个 \"worker\" 线程分担数据面的一部分任务。 Envoy 公开了两个统计信息来监控所有这些线程事件循环的性能。 跑一轮循环的耗时：事件循环的每次迭代都会执行一些任务。任务数量会随着负载的变化而变化。但是，如果一个或多个线程具有异常长尾循环执行耗时，则可能存在性能问题。例如，负责可能在工作线程之间分配不均，或者插件中可能存在长时间阻塞操作阻碍了任务进度。 轮询延迟：在事件循环的每次迭代中，事件调度程序都会轮询 I/O 事件，并在某些 I/O 事件就绪 或 发生 超时 时 “唤醒” 线程，以先发生者为准。在 超时 的情况下，我们可以测量轮询后预期唤醒时间与实际唤醒时间的差值；这种差异称为 “轮询延迟\"。看到一些小的 轮询延迟 是正常的，通常等于内核调度程序的 “时间片(time slice”)” 或 “量子(quantum)” ——这取决于运行 Envoy 的操作系统 —— 但如果这个数字大大高于其正常观察到的基线，它表示内核调度程序可能发生延迟。 可以通过将 enable_dispatcher_stats 设置为 true 来启用这些统计信息。 main 线程的事件调度器有一个以 server.dispatcher. 为根的统计树。 每个 worker 线程的事件调度器都有一个以 listener_manager.worker_\u003cid\u003e.dispatcher. 为根的统计树。 每棵树都有以下统计信息： Name Type Description loop_duration_us Histogram 以微秒为单位的事件循环持续时间 poll_delay_us Histogram 以微秒为单位的轮询延迟 请注意，此处不包括任何辅助(非 main 与 worker)线程。 Watch Dog 和 Event loop 都是解决与监控事件处理延迟与时效的工具，这里有很多细节和故事，甚至可以说到 Linux Kernel。希望本书后面有时间，可以和大家一起学习和分析这些有趣的细节。 ","date":"2022-09-13","objectID":"/posts/0affe7/:2:6","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"配置说明 本节参考： [Envoy 文档](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/metrics/v3/stats.proto) ","date":"2022-09-13","objectID":"/posts/0affe7/:3:0","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"config.bootstrap.v3.Bootstrap Envoy 文档:config.bootstrap.v3.Bootstrap proto { \"node\": {...}, \"static_resources\": {...}, \"dynamic_resources\": {...}, \"cluster_manager\": {...}, \"stats_sinks\": [], \"stats_config\": {...}, \"stats_flush_interval\": {...}, \"stats_flush_on_admin\": ..., ... } 什么是 `stats sink`？ 本书不作说明。Istio 默认没定制相关配置。以下只说关注的部分配置。 stats_config (config.metrics.v3.StatsConfig) 用于内部处理统计信息的配置。 stats_flush_interval (Duration) 刷新 stats sink 的时间间隔。。出于性能原因，Envoy 不会实时刷新 counter ，仅定期刷新 counter 和 gauge 。 如果未指定，则默认值为 5000 毫秒。 stats_flush_interval 或 stats_flush_on_admin 只能设置之一。 Duration 必须至少为 1 毫秒，最多为 5 分钟。 stats_flush_on_admin (bool) 仅当在 管理界面(admin interface) 上查询时才将统计信息刷新到 sink。 如果设置，则不会创建刷新计时器。 只能设置 stats_flush_on_admin 或 stats_flush_interval 之一。 ","date":"2022-09-13","objectID":"/posts/0affe7/:3:1","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"config.metrics.v3.StatsConfig Envoy 文档:config-metrics-v3-statsconfig { \"stats_tags\": [], \"use_all_default_tags\": {...}, \"stats_matcher\": {...}, \"histogram_bucket_settings\": [] } stats_tags - 维度提取规则(对应 Prometheus 的 label 提取) (多个 config.metrics.v3.TagSpecifier ) 每个 指标名称字符串 都通过这些标签规则独立处理。 当一个标签匹配时，第一个捕获组不会立即从名称中删除，所以后面的 TagSpecifiers 也可以重复匹配同一部分。在完成所有标签匹配后，再剪裁 指标名称字符串 的匹配部分，并作为 stats sink 的指标名，例如 Prometheus的指标名。 use_all_default_tags (BoolValue) 使用 Envoy 中指定的所有默认标签正则表达式。 这些可以与 stats_tags 中指定的自定义标签结合使用。 它们将在自定义标签之前进行处理。Istio 默认为 false. stats_matcher (config.metrics.v3.StatsMatcher) 指定 Envoy 要产出哪些指标。支持 包含/排除 规则指定。 如果未提供，则所有指标都将产出。 阻止某些指标集的统计可以提高一点 Envoy 运行性能。 ","date":"2022-09-13","objectID":"/posts/0affe7/:3:2","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"config.metrics.v3.StatsMatcher Envoy 文档:config-metrics-v3-statsmatcher 用于禁用/开启统计指标计算与产出的配置。 { \"reject_all\": ..., \"exclusion_list\": {...}, \"inclusion_list\": {...} } reject_all (bool) 如果 reject_all 为 true ，则禁用所有统计信息。 如果 reject_all 为 false，则启用所有统计信息。 exclusion_list (type.matcher.v3.ListStringMatcher) 排除列表 inclusion_list (type.matcher.v3.ListStringMatcher) 包含列表 本节参考了： https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/statistics 下一节，将以 Istio 如何使用上面的配置为例，举例说明。 ","date":"2022-09-13","objectID":"/posts/0affe7/:3:3","tags":["istio","envoy","转载"],"title":"Envoy性能指标","uri":"/posts/0affe7/"},{"categories":["云原生"],"content":"在前两篇博客中： Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 Sidecar 中的流量类型及 iptables 规则详解 我向你详细介绍了 Istio 数据平面中的流量，但数据平面并不能孤立的存在，本文将向你展示 Istio 中的控制平面和数据平面各组件的端口及其功能，有助于你了解这些流量之间的关系及故障排查。 ","date":"2022-09-12","objectID":"/posts/6be15f/:0:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"Istio 中的组件及端口示意图 按照习惯，我们首先展示一个全局示意图。下图展示的是 Istio 数据平面中 sidecar 的组成，以及与其交互的对象。 我们可以使用 nsenter 命令进入Bookinfo 示例的 productpage Pod的网络空间，查看其内部监听的端口信息。 从图中我们可以看到除了 productpage 应用本身监听的 9080 端口以外，Sidecar 容器还有监听大量的其他端口，如 15000、15001、15004、15006、15021、15090 等，你可以在 Istio 文档上了解 Istio 中使用的端口。 我们再进入 productpage Pod 中，使用 lsof -i 命令查看它打开的端口，如下图所示。 ! 我们可以看到其中有 pilot-agent 与 istiod 建立了 TCP 连接，上文中所述的监听中的端口，还有在 Pod 内部建立的 TCP 连接，这些连接对应了文章开头的示意图。 Sidecar 容器（istio-proxy ）的根进程是 pilot-agent，启动命令如下图所示： 从图中我们可以看到，它 pilot-agent 进程的 PID 是 1，是它拉起了 envoy 进程。 在 istiod 的 Pod 中查看它打开的端口，如下图所示。 我们可以看到其中的监听的端口、进程间和远程通信连接。 ","date":"2022-09-12","objectID":"/posts/6be15f/:1:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"Istio 中各端口的功能概述 这些端口在你进行问题排查时可以起着举足轻重的作用。下面将根据端口所在的组件和功能分类描述。 ","date":"2022-09-12","objectID":"/posts/6be15f/:2:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"Istiod 中的端口 Istiod 中的端口相对比较少且功能单一： 9876：ControlZ 用户界面，暴露 istiod 的进程信息 8080：istiod 调试端口，通过该端口可以查询网格的配置和状态信息 15010：暴露 xDS API 和颁发纯文本证书 15012：功能同 15010 端口，但使用 TLS 通信 15014：暴露控制平面的指标给 Prometheus 15017：Sidecar 注入和配置校验端口 ","date":"2022-09-12","objectID":"/posts/6be15f/:3:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"Sidecar 中的端口 从上文中，我们看到 sidecar 中有众多端口： 15000：Envoy 管理接口，你可以用它来查询和修改 Envoy 代理的的配置，详情请参考 Envoy 文档。 15001：用于处理出站流量。 15004：调试端口，将在下文中解释。 15006：用于处理入站流量。 15020：汇总统计数据，对 Envoy 和 DNS 代理进行健康检查，调试 pilot-agent 进程，将在下文中详细解释。 15021：用于 sidecar 健康检查，以判断已注入 Pod 是否准备好接收流量。我们在该端口的 /healthz/ready 路径上设置了就绪探针，Istio 把 sidecar 的就绪检测交给了 kubelet，最大化利用 Kubernetes 平台自身的功能。envoy 进程将健康检查路由到 pilot-agent 进程的 15020 端口，实际的健康检查将发生在那里。 15053：本地 DNS 代理，用于解析 Kubernetes DNS 解析不了的集群内部域名的场景。 15090：Envoy Prometheus 查询端口，pilot-agent 将通过此端口收集统计信息。 以上端口可以分为以下几类： 负责进程间通信，例如 15001、15006、15053 负责健康检查和信息统计，例如 150021、15090 调试：15000、15004 下文将对几个重点端口详解。 ","date":"2022-09-12","objectID":"/posts/6be15f/:4:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"15000 端口 15000 是 Envoy 的 Admin 接口，该接口允许我们修改 Envoy，并获得一个视图和查询指标和配置。 管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，你可以使用下面的命令开启 productpage Pod 中的 Envoy 管理接口视图。 kubectl -n default port-forward deploy/productpage-v1 15000 在浏览器中访问 http://localhost:15000，你将看到 Envoy Admin 界面如下图所示。 ","date":"2022-09-12","objectID":"/posts/6be15f/:5:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"15004 端口 通过 pilot-agent 代理 istiod 8080 端口上的调试端点，你可以进入数据平面 Pod 中访问 localhost 的 15004 端口查询网格信息，其效果与下面的 8080 端口等同。 ","date":"2022-09-12","objectID":"/posts/6be15f/:6:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"8080 端口 你还可以在本地转发 istiod 8080 端口，请运行下面的命令。 kubectl -n istio-system port-forward deploy/istiod 8080 在浏览器中访问 http://localhost:8080/debug，你将看到调试端点，如下图所示。 当然，这只是一种获取网格信息和调试网格的方式，你还可以使用 istioctl 命令或 Kiali 来调试，那样将更加高效和直观。 ","date":"2022-09-12","objectID":"/posts/6be15f/:7:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"15020 端口 15020 端口有三大功能： 汇总统计数据：查询 15090 端口获取 envoy 的指标，也可以配置查询应用程序的指标，将 envoy、应用程序和自身的指标汇总以供 Prometheus 收集。对应的调试端点是 /stats/prometheus。 对 Envoy 和 DNS 代理进行健康检查：对应的调试端点是 /healthz/ready 和 /app-health。 调试 pilot-agent 进程：对应的调试端点是 /quitquitquit、debug/ndsz 和 /debug/pprof。 下图展示的是使用本地端口转发后，在浏览器中打开 http://localhost:15020/debug/pprof 看到的调试信息。 图中信息展示的是 pilot-agent 的堆栈信息。 ","date":"2022-09-12","objectID":"/posts/6be15f/:8:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"总结 通过对 Istio 中各组件端口的了解，你应该对 Istio 中各组件的关系及其内部流量有了更进一步的认识，熟悉这些端口的功能，有助于对网格的故障排除。 ","date":"2022-09-12","objectID":"/posts/6be15f/:9:0","tags":["istio","sicader","转载"],"title":"Istio组件详解","uri":"/posts/6be15f/"},{"categories":["云原生"],"content":"我在之前的一篇博客中讲解过 Istio 中 sidecar 的注入、使用 iptables 进行透明流量拦截及流量路由的详细过程，并以 Bookinfo 示例中的 productpage 服务访问 reviews 服务，和 reviews 服务访问 ratings 服务为例绘制了透明流量劫持示意图。在那个示意图中仅展示了 reviews pod 接收流量和对外访问的路由，实际上 sidecar 内的流量远不止于此。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:0:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"ISTIO_OUTPUT 规则 在所有的 iptables 调用链中最复杂的一个是 ISTIO_OUTPUT，其中共有 9 条规则如下： Rule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere 本文将向你展示 Istio sidecar 中的六种流量类型及其 iptables 规则， 以示意图的形式带你一览其全貌，其中详细指出了路由具体使用的是 ISTIO_OUTPUT 中的哪一条规则。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:1:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"Sidecar 中的 iptables 流量路由 Sidecar 中的流量可以划分为以下几类： 远程服务访问本地服务：Remote Pod -\u003e Local Pod 本地服务访问远程服务：Local Pod -\u003e Remote Pod Prometheus 抓取本地服务的 metrics：Prometheus -\u003e Local Pod 本地 Pod 服务间的流量：Local Pod -\u003e Local Pod Envoy 内部的进程间 TCP 流量 Sidecar 到 Istiod 的流量 下面将依次解释每个场景下 Sidecar 内的 iptables 路由规则。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:2:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"类型一：Remote Pod -\u003e Local Pod 以下是远程服务、应用或客户端访问数据平面本地 Pod IP 的 iptables 规则。 Remote Pod -\u003e RREROUTING -\u003e ISTIO_INBOUND -\u003e ISTIO_IN_REDIRECT -\u003e Envoy 15006（Inbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 1 -\u003e POSTROUTING -\u003e Local Pod 我们看到流量只经过一次 Envoy 15006 Inbound 端口。这种场景下的 iptables 规则的示意图如下。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:3:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"类型二：Local Pod -\u003e Remote Pod 以下是本地 Pod IP 访问远程服务经过的 iptables 规则。 Local Pod-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 9 -\u003e ISTIO_REDIRECT -\u003e Envoy 15001（Outbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 4 -\u003e POSTROUTING -\u003e Remote Pod 我们看到流量只经过 Envoy 15001 Outbound 端口。 以上两种场景中的流量都只经过一次 Envoy，因为该 Pod 中只有发出或接受请求一种场景发生。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:4:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"类型三：Prometheus -\u003e Local Pod Prometheus 抓取数据平面 metrics 的流量不会也无须经过 Envoy 代理。 这些流量通过的 iptables 规则如下。 Prometheus-\u003e RREROUTING -\u003e ISTIO_INBOUND（对目的地为 15002、15090 端口流量将转到 INPUT）-\u003e INPUT -\u003e Local Pod 这种场景下的 iptables 规则的示意图如下。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:5:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"类型四：Local Pod -\u003e Local Pod 一个 Pod 可能同时存在两个或多个服务，如果 Local Pod 访问的服务也在该当前 Pod 上，流量会依次经过 Envoy 15001 和 Envoy 15006 端口最后到达本地 Pod 的服务端口上。 这些流量通过的 iptables 规则如下。 Local Pod-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 9 -\u003e ISTIO_REDIRECT -\u003e Envoy 15001（Outbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 2 -\u003e ISTIO_IN_REDIRECT -\u003e Envoy 15006（Inbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 1 -\u003e POSTROUTING -\u003e Local Pod ","date":"2022-09-11","objectID":"/posts/61aaaf/:6:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"类型五：Envoy 内部的进程间 TCP 流量 Envoy 内部进程的 UID 和 GID 为 1337，它们之间的流量将使用 lo 网卡，使用 localhost 域名来通信。 这些流量通过的 iptables 规则如下。 Envoy 进程（Localhost） -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 8 -\u003e POSTROUTING -\u003e Envoy 进程（Localhost） ","date":"2022-09-11","objectID":"/posts/61aaaf/:7:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"类型六：Sidecar 到 Istiod 的流量 Sidecar 需要访问 Istiod 以同步配置，pilot-agent 进程会向 Istiod 发送请求，以同步配置。 这些流量通过的 iptables 规则如下。 pilot-agent 进程 -\u003e OUTPUT -\u003e Istio_OUTPUT RULE 9 -\u003e Envoy 15001 (Outbound Handler) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 4 -\u003e POSTROUTING -\u003e Istiod ","date":"2022-09-11","objectID":"/posts/61aaaf/:8:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"总结 Istio 注入在 Pod 内或虚拟机中安装的所有 sidecar 代理组成了服务网格的数据平面，也是 Istio 的主要工作负载所在地，通过 Istio 中的透明流量劫持 及这篇博客，相信你一定对 sidecar 代理中的流量有了一个深刻的了解，但这还只是管中窥豹，略见一斑，在我的下一篇博客中，我将带你了解 Envoy 中各个组件的端口及其功能，这样可以让我们对 Istio 中的流量有一个更全面的了解。 ","date":"2022-09-11","objectID":"/posts/61aaaf/:9:0","tags":["sicader","转载"],"title":"Sidecar 中的流量类型及 iptables 规则详解","uri":"/posts/61aaaf/"},{"categories":["云原生"],"content":"本文最早是基于 Istio 1.11 撰写，之后随着 Istio 的版本陆续更新，最新更新时间为 2022 年 5 月 12 日，关于本文历史版本的更新说明请见文章最后。本文记录了详细的实践过程，力图能够让读者复现，因此事无巨细，想要理解某个部分过程的读者可以使用目录跳转到对应的小节阅读。 为了理解本文希望你先阅读以下内容： 理解 iptables Istio 数据平面 Pod 启动过程详解 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:0:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"内容介绍 本文基于 Istio 1.13 版本，将为大家介绍以下内容： 什么是 sidecar 模式和它的优势在哪里。 Istio 中是如何做 sidecar 注入的。 Sidecar 代理是如何做透明流量劫持的。 iptables 的路由规则。 Envoy 代理是如何路由流量到上游的。 请大家结合下图理解本文中的内容，本图基于 Istio 官方提供的 Bookinfo 示例绘制，展示的是 reviews Pod 的内部结构，包括 Linux Kernel 空间中的 iptables 规则、Sidecar 容器、应用容器。 productpage 访问 reviews Pod，入站流量处理过程对应于图示上的步骤：1、2、3、4、Envoy Inbound Handler、5、6、7、8、应用容器。 reviews Pod 访问 rating 服务的出站流量处理过程对应于图示上的步骤是：9、10、11、12、Envoy Outbound Handler、13、14、15。 注意：图中的路径 16 近用于路由规则说明，它不不出现在当前示例中。实际上仅当 Pod 内发出的对当前 Pod 内的服务访问的时候才会途径它。 上图中关于流量路由部分，包含： productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews Pod 内部时，流量是如何被 iptables 劫持到 Envoy 代理被 Inbound Handler 处理的； reviews 请求访问 ratings 服务的 Pod，应用程序发出的出站流量被 iptables 劫持到 Envoy 代理的 Outbound Handler 的处理。 在阅读下文时，请大家确立以下已知点： 首先，productpage 发出的对 reivews 的访问流量，是在 Envoy 已经通过 EDS 选择出了要请求的 reviews 服务的某个 Pod，知晓了其 IP 地址，直接向该 IP 发送的 TCP 连接请求。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以其中一个 Pod 中的 sidecar 流量转发步骤来说明。 所有进入 reviews Pod 的 TCP 流量都根据 Pod 中的 iptables 规则转发到了 Envoy 代理的 15006 端口，然后经过 Envoy 的处理确定转发给 Pod 内的应用容器还是透传。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:1:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"Sidecar 模式 将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。 就像连接了 Sidecar 的三轮摩托车一样，在软件架构中， Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观察性、监控、日志记录、配置、断路器等功能。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:2:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器（如 Envoy 或 MOSN），这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。 因其独特的部署结构，使得 sidecar 模式具有以下优势： 将与应用业务逻辑无关的功能抽象到共同基础设施，降低了微服务代码的复杂度。 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 Sidecar 可独立升级，降低应用程序代码和底层平台的耦合度。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:2:1","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"Sidecar 注入示例分析 以 Istio 官方提供的 bookinfo 中 productpage 的 YAML 为例，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml。 下文将从以下几个方面讲解： Sidecar 容器的注入 iptables 规则的创建 路由的详细过程 apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} 再查看下 productpage 容器的 Dockerfile。 FROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\"python\", \"productpage.py\", \"9080\"] 我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。 $ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml 我们只截取其中与 productpage 相关的 Deployment 配置中的部分 YAML 配置。 containers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # 应用镜像 name: productpage ports: - containerPort: 9080 - args: - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage.$(POD_NAMESPACE) - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istiod.istio-system.svc:15012 - --zipkinAddress - zipkin.istio-system:9411 - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --connectTimeout - 10s - --proxyAdminPort - \"15000\" - --concurrency - \"2\" - --controlPlaneAuthPolicy - NONE - --dnsRefreshRate - 300s - --statusPort - \"15020\" - --trust-domain=cluster.local - --controlPlaneBootstrap=false image: docker.io/istio/proxyv2:1.5.1 # sidecar proxy name: istio-proxy ports: - containerPort: 15090 name: http-envoy-prom protocol: TCP initContainers: - command: - istio-iptables - -p - \"15001\" - -z - \"15006\" - -u - \"1337\" - -m - REDIRECT - -i - '*' - -x - \"\" - -b - '*' - -d - 15090,15020 image: docker.io/istio/proxyv2:1.5.1 # init 容器 name: istio-init Istio 给应用 Pod 注入的配置主要包括： Init 容器 istio-init：用于 pod 中设置 iptables 端口转发 Sidecar 容器 istio-proxy：运行 sidecar 代理，如 Envoy 或 MOSN。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:3:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"iptables 规则注入解析 为了查看 iptables 配置，我们需要登陆到 sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage pod 所在的主机上使用 docker 命令登陆容器中查看。 如果您使用 minikube 部署的 Kubernetes，可以直接登录到 minikube 的虚拟机中并切换为 root 用户。查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables 传递的参数中指定将入站流量重定向到 sidecar 的模式为 REDIRECT，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables 命令。 我们仅查看与 productpage 有关的 iptables 规则如下，因为这些规则是运行在该容器特定的网络空间下，因此需要使用 nsenter 命令进入其网络空间。进入的时候需要指定进程 ID（PID），因此首先我们需要找到 productpage 容器的 PID。对于在不同平台上安装的 Kubernetes，查找容器的方式会略有不同，例如在 GKE 上，执行 docker ps -a 命令是查看不到任何容器进程的。下面已 minikube 和 GKE 两个典型的平台为例，指导你如何进入容器的网络空间。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:4:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"在 minikube 中查看容器中的 iptabes 规则 对于 minikube，因为所有的进程都运行在单个节点上，因此你只需要登录到 minikube 虚拟机，切换为 root 用户然后查找 productpage 进程即可，参考下面的步骤。 # 进入 minikube 并切换为 root 用户，minikube 默认用户为 docker $ minikube ssh $ sudo -i # 查看 productpage pod 的 istio-proxy 容器中的进程 $ docker top `docker ps|grep \"istio-proxy_productpage\"|cut -d \" \" -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 # 使用 nsenter 进入 sidecar 容器的命名空间（以上任何一个都可以） $ nsenter -n --target 10660 # 查看 NAT 表中规则配置的详细信息。 $ iptables -t nat -L ","date":"2022-09-11","objectID":"/posts/fe0ba1/:4:1","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"在 GKE 中查看容器的 iptables 规则 如果你在 GKE 中安装的多节点的 Kubernetes 集群，首先你需要确定这个 Pod 运行在哪个节点上，然后登陆到那台主机，使用下面的命令查找进程的 PID，你会得到类似下面的输出。 $ ps aux|grep \"productpage\" chronos 4268 0.0 0.6 43796 24856 ? Ss Apr22 0:00 python productpage.py 9080 chronos 4329 0.9 0.6 117524 24616 ? Sl Apr22 13:43 /usr/local/bin/python /opt/microservices/productpage.py 9080 root 361903 0.0 0.0 4536 812 pts/0 S+ 01:54 0:00 grep --colour=auto productpage 然后在终端中输出 iptables -t nat -L 即可查看 iptables 规则。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:4:2","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"iptables 流量劫持过程详解 经过上面的步骤，你已经可以查看到 init 容器向 Pod 中注入的 iptables 规则，如下所示。 # PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上。 Chain PREROUTING (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination 2701 162K ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链。 Chain INPUT (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上。 Chain OUTPUT (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination 15 900 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入 POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理。 Chain POSTROUTING (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有入站流量重定向到 ISTIO_IN_REDIRECT 链上。目的地为 15090（Prometheus 使用）和 15020（Ingress gateway 使用，用于 Pilot 健康检查）端口的流量除外，发送到以上两个端口的流量将返回 iptables 规则链的调用点，即 PREROUTING 链的后继 POSTROUTING 后直接调用原始目的地。 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN tcp -- any any anywhere anywhere tcp dpt:ssh 2 120 RETURN tcp -- any any anywhere anywhere tcp dpt:15090 2699 162K RETURN tcp -- any any anywhere anywhere tcp dpt:15020 0 0 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere # ISTIO_IN_REDIRECT 链：将所有的入站流量跳转到本地的 15006 端口，至此成功的拦截了流量到 sidecar 代理的 Inbound Handler 中。 Chain ISTIO_IN_REDIRECT (3 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15006 # ISTIO_OUTPUT 链：规则比较复杂，将在下文解释 Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- any lo 127.0.0.6 anywhere #规则1 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner UID match 1337 #规则2 0 0 RETURN all -- any lo anywhere anywhere ! owner UID match 1337 #规则3 15 900 RETURN all -- any any anywhere anywhere owner UID match 1337 #规则4 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner GID match 1337 #规则5 0 0 RETURN all -- any lo anywhere anywhere ! owner GID match 1337 #规则6 0 0 RETURN all -- any any anywhere anywhere owner GID match 1337 #规则7 0 0 RETURN all -- any any anywhere localhost #规则8 0 0 ISTIO_REDIRECT all -- any any anywhere anywhere #规则9 # ISTIO_REDIRECT 链：将所有流量重定向到 Envoy 代理的 15001 端口。 Chain ISTIO_REDIRECT (1 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 这里着重需要解释的是 ISTIO_OUTPUT 链中的 9 条规则，为了便于阅读，我将以上规则中的部分内容使用表格的形式来展示如下： 规则 target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere 下图展示了 ISTIO_ROUTE 规则的详细流程。 我将按照规则的出现顺序来解释每条规则的目的、对应文章开头图示中的步骤及详情。其中规则 5、6、7 是分别对规则 2、3、4 的应用范围扩大（从 UID 扩大为 GID），作用是类似的，将合并解释。注意，其中的规则是按顺序执行的，也就是说排序越靠后的规则将作为默认值。出站网卡（out）为 lo （本地回环地址，loopback 接口）时，表示流量的目的地是本地 Pod，对于 Pod 向外部发送的流量就不会经过这个接口。所有 review Pod 的出站流量只适用于规则 4、7、8、9。 规则 1 目的：透传 Envoy 代理发送到本地应用容器的流量，使其绕过 Envoy 代理，直达应用容器。 对应图示中的步骤：6 到 7。 详情：该规则使得所有来自 127.0.0.6（该 IP 地址将在下文解释） 的请求，跳出该链，返回 iptables 的调用点（即 OUTPUT）后继续执行其余路由规则，即 POSTROUTING 规则，把流量发送到任意目的地址，如本地 Pod 内的应用容器。如果没有这条规则，由 Pod 内 Envoy 代理发出的对 Pod 内容器访问的流量将会执行下一条规则，即规则 2，流量将再次进入到了 Inbound Handler 中，从而形成了死循环。将这条规则放在第一位可以避免流量在 Inbound Handler 中死循环的问题。 规则 2、5 目的：处理 Envoy 代理发出的站内流量（Pod 内部的流量），但不是对 localhost 的请求，通过后续规则将其转发给 Envoy 代理的 Inbound Handler。该规则适用于 Pod 对自身 IP 地址调用的场景，即 Pod 内服务之间的访问。 详情：如果流量的目的地非 localhost，且数据包是由 1337 UID（即 istio-proxy 用户，Envoy 代理）发出的，流量将被经过 ISTIO_IN_REDIRECT 最终转发到 Envoy 的 Inbound Handler。 规则 3、6 目的：透传 Pod 内","date":"2022-09-11","objectID":"/posts/fe0ba1/:5:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"流量路由过程详解 通过上文，你已经了解了 Istio 是如何在 Pod 中做透明流量劫持的，那么流量被劫持到 Envoy 代理中之后是如何被处理的呢？流量路由分为 Inbound 和 Outbound 两个过程，下面将根据上文中的示例及 sidecar 的配置为读者详细分析此过程。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:6:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"理解 Inbound Handler Inbound Handler 的作用是将 iptables 拦截到的 downstream 的流量转发给 Pod 内的应用程序容器。在我们的实例中，假设其中一个 Pod 的名字是 reviews-v1-545db77b95-jkgv2，运行 istioctl proxy-config listener reviews-v1-545db77b95-jkgv2 --port 15006 查看该 Pod 中 15006 端口上的监听器情况 ，你将看到下面的输出。 ADDRESS PORT MATCH DESTINATION 0.0.0.0 15006 Addr: *:15006 Non-HTTP/Non-TCP 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: istio,istio-peer-exchange,istio-http/1.0,istio-http/1.1,istio-h2; Addr: *:9080 Cluster: inbound|9080|| 0.0.0.0 15006 Trans: raw_buffer; Addr: *:9080 Cluster: inbound|9080|| 下面列出了以上输出中各字段的含义： ADDRESS：下游地址 PORT：Envoy 监听器监听的端口 MATCH：请求使用的传输协议或匹配的下游地址 DESTINATION：路由目的地 reviews Pod 中的 Iptables 将入站流量劫持到 15006 端口上，从上面的输出我们可以看到 Envoy 的 Inbound Handler 在 15006 端口上监听，对目的地为任何 IP 的 9080 端口的请求将路由到 inbound|9080|| Cluster 上。 从该 Pod 的 Listener 列表的最后两行中可以看到，0.0.0.0:15006/TCP 的 Listener（其实际名字是 virtualInbound）监听所有的 Inbound 流量，其中包含了匹配规则，来自任意 IP 的对 9080 端口的访问流量，将会路由到 inbound|9080|| Cluster，如果你想以 Json 格式查看该 Listener 的详细配置，可以执行 istioctl proxy-config listeners reviews-v1-545db77b95-jkgv2 --port 15006 -o json 命令，你将获得类似下面的输出。 [ /*省略部分内容*/ { \"name\": \"virtualInbound\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15006 } }, \"filterChains\": [ /*省略部分内容*/ { \"filterChainMatch\": { \"destinationPort\": 9080, \"transportProtocol\": \"tls\", \"applicationProtocols\": [ \"istio\", \"istio-peer-exchange\", \"istio-http/1.0\", \"istio-http/1.1\", \"istio-h2\" ] }, \"filters\": [ /*省略部分内容*/ { \"name\": \"envoy.filters.network.http_connection_manager\", \"typedConfig\": { \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\", \"statPrefix\": \"inbound_0.0.0.0_9080\", \"routeConfig\": { \"name\": \"inbound|9080||\", \"virtualHosts\": [ { \"name\": \"inbound|http|9080\", \"domains\": [ \"*\" ], \"routes\": [ { \"name\": \"default\", \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"inbound|9080||\", \"timeout\": \"0s\", \"maxStreamDuration\": { \"maxStreamDuration\": \"0s\", \"grpcTimeoutHeaderMax\": \"0s\" } }, \"decorator\": { \"operation\": \"reviews.default.svc.cluster.local:9080/*\" } } ] } ], \"validateClusters\": false }, /*省略部分内容*/ } } ], /*省略部分内容*/ ], \"listenerFilters\": [ /*省略部分内容*/ ], \"listenerFiltersTimeout\": \"0s\", \"continueOnListenerFiltersTimeout\": true, \"trafficDirection\": \"INBOUND\" } ] 既然 Inbound Handler 的流量中将来自任意地址的对该 Pod 9080 端口的流量路由到 inbound|9080|| Cluster，那么我们运行 istioctl pc cluster reviews-v1-545db77b95-jkgv2 --port 9080 --direction inbound -o json 查看下该 Cluster 配置，你将获得类似下面的输出。 [ { \"name\": \"inbound|9080||\", \"type\": \"ORIGINAL_DST\", \"connectTimeout\": \"10s\", \"lbPolicy\": \"CLUSTER_PROVIDED\", \"circuitBreakers\": { \"thresholds\": [ { \"maxConnections\": 4294967295, \"maxPendingRequests\": 4294967295, \"maxRequests\": 4294967295, \"maxRetries\": 4294967295, \"trackRemaining\": true } ] }, \"cleanupInterval\": \"60s\", \"upstreamBindConfig\": { \"sourceAddress\": { \"address\": \"127.0.0.6\", \"portValue\": 0 } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"services\": [ { \"host\": \"reviews.default.svc.cluster.local\", \"name\": \"reviews\", \"namespace\": \"default\" } ] } } } } ] 我们看其中的 TYPE 为 ORIGINAL_DST，将流量发送到原始目标地址（Pod IP），因为原始目标地址即当前 Pod，你还应该注意到 upstreamBindConfig.sourceAddress.address 的值被改写为了 127.0.0.6，而且对于 Pod 内流量是通过 lo 网卡发送的，这刚好呼应了上文中的 iptables ISTIO_OUTPUT 链中的第一条规则，根据该规则，流量将被透传到 Pod 内的应用容器。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:6:1","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"理解 Outbound Handler 在本示例中 reviews 会向 ratings 服务发送 HTTP 请求，请求的地址是：http://ratings.default.svc.cluster.local:9080/，Outbound Handler 的作用是将 iptables 拦截到的本地应用程序向外发出的流量，经由 Envoy 代理路由到上游。 Envoy 监听在 15001 端口上监听所有 Outbound 流量，Outbound Handler 处理，然后经过 virtualOutbound Listener、0.0.0.0_9080 Listener，然后通过 Route 9080 找到上游的 cluster，进而通过 EDS 找到 Endpoint 执行路由动作。 ratings.default.svc.cluster.local:9080 路由 运行 istioctl proxy-config routes reviews-v1-545db77b95-jkgv2 --name 9080 -o json 查看 route 配置，因为 sidecar 会根据 HTTP header 中的 domains 来匹配 VirtualHost，所以下面只列举了 ratings.default.svc.cluster.local:9080 这一个 VirtualHost。 [ { \"name\": \"9080\", \"virtualHosts\": [ { \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.8.8.106\", \"10.8.8.106:9080\" ], \"routes\": [ { \"name\": \"default\", \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0s\", \"retryPolicy\": { \"retryOn\": \"connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes\", \"numRetries\": 2, \"retryHostPredicate\": [ { \"name\": \"envoy.retry_host_predicates.previous_hosts\" } ], \"hostSelectionRetryMaxAttempts\": \"5\", \"retriableStatusCodes\": [ 503 ] }, \"maxStreamDuration\": { \"maxStreamDuration\": \"0s\", \"grpcTimeoutHeaderMax\": \"0s\" } }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" } } ], \"includeRequestAttemptCount\": true }, /*省略部分内容*/ ], \"validateClusters\": false } ] 从该 Virtual Host 配置中可以看到将流量路由到outbound|9080||ratings.default.svc.cluster.local 集群。 outbound|9080||ratings.default.svc.cluster.local 集群的端点 运行 istioctl proxy-config endpoint reviews-v1-545db77b95-jkgv2 --port 9080 -o json --cluster \"outbound|9080||ratings.default.svc.cluster.local\" 查看集群的 Endpoint 配置，结果如下。 [ { \"name\": \"outbound|9080||ratings.default.svc.cluster.local\", \"addedViaApi\": true, \"hostStatuses\": [ { \"address\": { \"socketAddress\": { \"address\": \"10.4.1.12\", \"portValue\": 9080 } }, \"stats\": [ { \"name\": \"cx_connect_fail\" }, { \"name\": \"cx_total\" }, { \"name\": \"rq_error\" }, { \"name\": \"rq_success\" }, { \"name\": \"rq_timeout\" }, { \"name\": \"rq_total\" }, { \"type\": \"GAUGE\", \"name\": \"cx_active\" }, { \"type\": \"GAUGE\", \"name\": \"rq_active\" } ], \"healthStatus\": { \"edsHealthStatus\": \"HEALTHY\" }, \"weight\": 1, \"locality\": { \"region\": \"us-west2\", \"zone\": \"us-west2-a\" } } ], \"circuitBreakers\": { \"thresholds\": [ { \"maxConnections\": 4294967295, \"maxPendingRequests\": 4294967295, \"maxRequests\": 4294967295, \"maxRetries\": 4294967295 }, { \"priority\": \"HIGH\", \"maxConnections\": 1024, \"maxPendingRequests\": 1024, \"maxRequests\": 1024, \"maxRetries\": 3 } ] }, \"observabilityName\": \"outbound|9080||ratings.default.svc.cluster.local\" } ] 我们看到端点的地址是 10.4.1.12。实际上，Endpoint 可以是一个或多个，sidecar 将根据一定规则选择适当的 Endpoint 来路由。至此 review Pod找到了它上游服务 rating 的 Endpoint。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:6:2","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"小结 本文使用了 Istio 官方提供的 bookinfo 示例，按图索骥得带领读者了解了 sidecar 注入、iptables 透明流量劫持及 sidecar 中流量路由背后的实现细节。Sidecar 模式和流量透明劫持是 Istio 服务网格的特色和基础功能，理解该功能的背后过程及实现细节，将有助于大家理解 Service Mesh 的原理和 Istio Handbook 后面章节中的内容，因此希望读者可以在自己的环境中从头来试验一遍以加深理解。 使用 iptables 做流量劫持只是 service mesh 的数据平面中做流量劫持的方式之一，还有更多的流量劫持方案，下面引用自 云原生网络代理 MOSN 官网中给出的流量劫持部分的描述。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:7:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"使用 iptables 做流量劫持时存在的问题 目前 Istio 使用 iptables 实现透明劫持，主要存在以下三个问题： 需要借助于 conntrack 模块实现连接跟踪，在连接数较多的情况下，会造成较大的消耗，同时可能会造成 track 表满的情况，为了避免这个问题，业内有关闭 conntrack 的做法。 iptables 属于常用模块，全局生效，不能显式的禁止相关联的修改，可管控性比较差。 iptables 重定向流量本质上是通过 loopback 交换数据，outbond 流量将两次穿越协议栈，在大并发场景下会损失转发性能。 上述几个问题并非在所有场景中都存在，比方说某些场景下，连接数并不多，且 NAT 表未被使用到的情况下，iptables 是一个满足要求的简单方案。为了适配更加广泛的场景，透明劫持需要解决上述三个问题。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:7:1","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"透明劫持方案优化 为了优化 Istio 中的透明流量劫持的性能，业界提出了以下方案。 使用 Merbridge 开源项目利用 eBPF 劫持流量 Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用 Merbridge 可以在一定程度上优化数据平面的网络性能。 Merbridge 利用 eBPF 的 sockops 和 redir 能力，可以直接将数据包从 inbound socket 传输到 outbound socket。eBPF 提供了 bpf_msg_redirect_hash 函数可以直接转发应用程序的数据包。 详见 Istio 服务网格 —— 云原生应用网络构建指南。 使用 tproxy 处理 inbound 流量 tproxy 可以用于 inbound 流量的重定向，且无需改变报文中的目的 IP/端口，不需要执行连接跟踪，不会出现 conntrack 模块创建大量连接的问题。受限于内核版本，tproxy 应用于 outbound 存在一定缺陷。目前 Istio 支持通过 tproxy 处理 inbound 流量。 使用 hook connect 处理 outbound 流量 为了适配更多应用场景，outbound 方向通过 hook connect 来实现，实现原理如下： hook-connect 原理示意图 无论采用哪种透明劫持方案，均需要解决获取真实目的 IP/端口的问题，使用 iptables 方案通过 getsockopt 方式获取，tproxy 可以直接读取目的地址，通过修改调用接口，hook connect 方案读取方式类似于 tproxy。 实现透明劫持后，在内核版本满足要求（4.16以上）的前提下，通过 sockmap 可以缩短报文穿越路径，进而改善 outbound 方向的转发性能。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:7:2","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"更新说明 下面是本文的几次更新说明。 2020 年 4 月 27 日，第一版，基于 Istio 1.5 本文的第一版，基于 Istio 1.5 创作，在此之前，我曾写过基于 Istio 1.1 版本的理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持，为了更细致的理解 Istio 中透明流量劫持的全过程，专门创作本文。 2022 年 1 月 17 日，第二版，基于 Istio 1.11 本文第一版发布后，在社区里获得了比较大的反响，收到了很多读者的评论和留言。基于这些评论，我也发现了第一版中的很多错误，在加上 Istio 版本发布频繁，在近两年的时间内，Istio 已经作出了众多更新，其中不乏重大更新。因此笔者撰写了本文的第二版，修改了之前版本中的纰漏并根据时下 Istio 的最新版本更新了本文。 Istio 1.11 与 Istio 1.1 中的 sidecar 注入和流量劫持环节最大的变化是： iptables 改用命令行工具，不再使用 shell 脚本。 sidecar inbound 和 outbound 分别指定了端口，而之前是使用同一个端口（15001）。 2022 年 4 月 24，第三版，基于 Istio 1.13 这个版本的文章主要是根据当时 Istio 的最新版本更新了文章的部分内容，并重新排版，增加更新说明。 Istio 1.13 相比 Istio 1.11 的变化是 istioctl proxy-config 命令的输出有了较大变化。 2022 年 5 月 6 日，第四版，基于 Istio 1.13 修改了对 ISTIO_ROUTE iptables 规则 2、5 的解释 在示意图中增加了路径 16 2022 年 5 月 12 日，第五版，基于 Istio 1.13 将 iptables 说明和 sidecar 注入、init 容器部分独立成了两篇单独的博客，以缩减博客的篇幅，见 Istio 数据平面 Pod 启动过程详解和理解 iptables。 ","date":"2022-09-11","objectID":"/posts/fe0ba1/:8:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["云原生"],"content":"参考 阅读原文 Debugging Envoy and Istiod - istio.io 揭开 Istio Sidecar 注入模型的神秘面纱 - istio.io MOSN 作为 Sidecar 使用时的流量劫持方案 - mosn.io ","date":"2022-09-11","objectID":"/posts/fe0ba1/:9:0","tags":["istio","转载"],"title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解","uri":"/posts/fe0ba1/"},{"categories":["VSCode"],"content":"打开setting.json \"[markdown]\": { \"editor.quickSuggestions\": true } ","date":"2022-09-11","objectID":"/posts/4faa15/:0:1","tags":["vscode","snippets"],"title":"VSCode 配置Markdown模板","uri":"/posts/4faa15/"},{"categories":["VSCode"],"content":"配置模板 Ctrl + Shift + P 输入snippet 点击首选项：配置用户代码片片段 选择markdown.json ","date":"2022-09-11","objectID":"/posts/4faa15/:0:2","tags":["vscode","snippets"],"title":"VSCode 配置Markdown模板","uri":"/posts/4faa15/"},{"categories":["VSCode"],"content":"编写模板 { // Place your snippets for markdown here. Each snippet is defined under a snippet name and has a prefix, body and // description. The prefix is what is used to trigger the snippet and the body will be expanded and inserted. Possible variables are: // $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. Placeholders with the // same ids are connected. // Example: // \"Print to console\": { // \"prefix\": \"log\", // \"body\": [ // \"console.log('$1');\", // \"$2\" // ], // \"description\": \"Log output to console\" // } \"blog meta template\": { \"prefix\": \"meta\", \"body\": [ \"---\", \"title: ${TM_FILENAME_BASE}\", \"slug: ${RANDOM_HEX}\", \"date: ${CURRENT_YEAR}-${CURRENT_MONTH}-${CURRENT_DATE}T${CURRENT_HOUR}:${CURRENT_MINUTE}:${CURRENT_SECOND}+08:00\", \"draft: false\", \"author: kbsonlong\", \"authorEmail: kbsonlong@gmail.com\", \"tags: [${1}]\", \"categories: [${2}]\", \"featuredImage: https://imgapi.cn/api.php?zd=zsy\u0026fl=fengjing\u0026random=${RANDOM}\", \"---\" ] }, \"last modifier time\": { \"prefix\": \"last\", \"body\": \"${CURRENT_YEAR}-${CURRENT_MONTH}-${CURRENT_DATE}T${CURRENT_HOUR}:${CURRENT_MINUTE}:${CURRENT_SECOND}+08:00\" }, \"more themplate\" :{ \"prefix\": \"more\", \"body\": \"\u003c!--more--\u003e\", \"description\": \"文章摘要手动分割\" } } ","date":"2022-09-11","objectID":"/posts/4faa15/:0:3","tags":["vscode","snippets"],"title":"VSCode 配置Markdown模板","uri":"/posts/4faa15/"},{"categories":["VSCode"],"content":"常用变量 TM_SELECTED_TEXT 当前选定的文本或空字符串 TM_CURRENT_LINE 当前行的内容 TM_CURRENT_WORD 光标下的单词的内容或空字符串 TM_LINE_INDEX 基于零索引的行号 TM_LINE_NUMBER 基于一索引的行号 TM_FILENAME 当前文档的文件名 TM_FILENAME_BASE 当前文档的文件名（不含后缀名) TM_DIRECTORY 当前文档的目录 RELATIVE_FILEPATH 当前文件的相对目录 TM_FILEPATH 当前文档的完整文件路径 CLIPBOARD 剪切板里的内容 CURRENT_YEAR 当前年(四位数) CURRENT_MONTH 当前月 CURRENT_DATE 当前日 CURRENT_DAY_NAME_SHORT 当天的短名称（’Mon’） CURRENT_HOUR 当前小时 CURRENT_MINUTE 当前分钟 CURRENT_SECOND 当前秒 插入随机值 RANDOM 6位随机10进制数 RANDOM_HEX 6位16进制数 UUID 一个版本4的UUID /** 286055 f570d8 0a831688-a7f1-4668-9964-f6100114792c */ BLOCK_COMMENT_START 块注释开始标识,如 PHP /* 或 HTML \u003c!-- BLOCK_COMMENT_END 块注释结束标识,如 PHP */ 或 HTML --\u003e LINE_COMMENT 行注释，如： PHP // 或 HTML \u003c!-- --\u003e ","date":"2022-09-11","objectID":"/posts/4faa15/:0:4","tags":["vscode","snippets"],"title":"VSCode 配置Markdown模板","uri":"/posts/4faa15/"},{"categories":["云原生"],"content":"准备工作 S2I自定义镜像构建器 ● assemble（必需）：从源代码构建应用程序制品的脚本 assemble。 ● run（必需）：用于运行应用程序的脚本。 ● save-artifacts（可选）：管理增量构建过程中的所有依赖。 ● usage（可选）：提供说明的脚本。 ● test （可选）：用于测试的脚本。 ","date":"2022-09-11","objectID":"/posts/d83d7d/:0:1","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["云原生"],"content":"创建镜像构建器 准备S2I目录 安装s2i wget https://github.com/openshift/source-to-image/releases/download/v1.2.04/source-to-image-v1.1.14-874754de-linux-386.tar.gz tar -xvf source-to-image-v1.1.14-874754de-linux-386.tar.gz ls s2i source-to-image-v1.1.14-874754de-linux-386.tar.gz sti cp s2i /usr/local/bin 初始化镜像构建器 s2i create java-ubuntu22 java-ubuntu22 cd java-ubuntu22 目录结构初始化 . ├── Dockerfile ├── Makefile ├── README.md ├── prometheus-config.yml ├── s2i │ └── bin │ ├── assemble │ ├── run │ ├── save-artifacts │ └── usage └── test ├── run └── test-app ├── b2i-jar-java8.jar ## 默认不存在 └── index.html 4 directories, 11 files ","date":"2022-09-11","objectID":"/posts/d83d7d/:0:2","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["云原生"],"content":"修改Dockerfile # java-ubuntu22 FROM ubuntu:22.04 # TODO: Put the maintainer name in the image metadata # LABEL maintainer=\"Your Name \u003cyour@email.com\u003e\" # TODO: Rename the builder environment variable to inform users about application you provide them # ENV BUILDER_VERSION 1.0 # TODO: Set labels used in OpenShift to describe the builder image LABEL io.k8s.description=\"Java 8 web application\" \\ io.k8s.display-name=\"Java 8 Web\" \\ io.openshift.expose-services=\"8080:http\" \\ io.openshift.tags=\"builder,java,web\" \\ # this label tells s2i where to find its mandatory scripts # (run, assemble, save-artifacts) io.openshift.s2i.scripts-url=\"image:///usr/libexec/s2i\" # TODO: Install required packages here: # RUN yum install -y ... \u0026\u0026 yum clean all -y RUN apt-get update \u0026\u0026 apt-get install -y \\ curl \\ wget \\ openjdk-8-jdk \u0026\u0026 \\ rm -rf /var/lib/apt/lists WORKDIR /opt # TODO (optional): Copy the builder files into /opt/app-root # COPY ./\u003cbuilder_folder\u003e/ /opt/app-root/ # TODO: Copy the S2I scripts to /usr/libexec/s2i, since openshift/base-centos7 image # sets io.openshift.s2i.scripts-url label that way, or update that label COPY ./s2i/bin/ /usr/libexec/s2i # TODO: Drop the root user and make the content of /opt/app-root owned by user 1001 # RUN chown -R 1001:1001 /opt/app-root RUN chgrp -R 0 /usr/libexec/s2i \\ \u0026\u0026 chmod -R u=rwx,go=rx /usr/libexec/s2i \u0026\u0026 \\ chgrp -R 0 /opt \\ \u0026\u0026 chmod -R u=rwx,go=rx /opt \u0026\u0026 \\ chown -R 1001:1001 /opt \u0026\u0026 \\ chown -R 1001:1001 /usr/libexec/s2i/assemble RUN mkdir -p /opt/prometheus/etc \\ \u0026\u0026 curl https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.17.0/jmx_prometheus_javaagent-0.17.0.jar \\ -o /opt/prometheus/jmx_prometheus_javaagent.jar COPY prometheus-config.yml /opt/prometheus/etc # This default user is created in the openshift/base-centos7 image USER 1001 # TODO: Set the default port for applications built using this image EXPOSE 8080 # TODO: Set the default CMD for the image CMD [\"/usr/libexec/s2i/usage\"] ","date":"2022-09-11","objectID":"/posts/d83d7d/:0:3","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["云原生"],"content":"修改S2I脚本 assemble #!/bin/bash -e # # S2I assemble script for the 'java-ubuntu22' image. # The 'assemble' script builds your application source so that it is ready to run. # # For more information refer to the documentation: # https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md # # If the 'java-ubuntu22' assemble script is executed with the '-h' flag, print the usage. if [[ \"$1\" == \"-h\" ]]; then exec /usr/libexec/s2i/usage fi # Restore artifacts from the previous build (if they exist). # echo \"---\u003e Installing application...\" mkdir -p /opt/app ls /tmp/src/* mv /tmp/src/* /opt/app/ chmod +x /opt/app/*.jar 默认情况下，s2i build将应用程序源代码放在/tmp/src。上述命令将应用程序jar包复制到/opt/app/目录下 run #!/bin/bash -e # # S2I run script for the 'java-ubuntu22' image. # The run script executes the server that runs your application. # # For more information see the documentation: # https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md # PROMETHEUS_JMX_OPTS=\"-javaagent:/opt/prometheus/jmx_prometheus_javaagent.jar=${PROMETHEUS_JMX_PORT}:/opt/prometheus/etc/prometheus-config.yml\" # Always include jolokia-opts, which can be empty if switched off via env JAVA_OPTIONS=\"${JAVA_OPTIONS:+${JAVA_OPTIONS} }\" # Temporary options variable until the harmonization hawt-app PR #5 has been applied (hopefully) JVM_ARGS=\"${JVM_ARGS:+${JVM_ARGS} }${JAVA_OPTIONS} ${PROMETHEUS_JMX_OPTS}\" export JAVA_OPTIONS JVM_ARGS PROMETHEUS_JMX_OPTS exec java -jar ${JVM_ARGS} /opt/app/app.jar ","date":"2022-09-11","objectID":"/posts/d83d7d/:0:4","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["云原生"],"content":"构建与运行 创建镜像构建器 make build 使用镜像构建器创建应用程序镜像 # s2i build ./test/test-app java-ubuntu22 sample-app ---\u003e Installing application... /tmp/src/b2i-jar-java8.jar /tmp/src/index.html Build completed successfully 按照assemble脚本定义的逻辑，S2I使用镜像构建器作为基础创建应用程序镜像，并从test/test-app目录注入源代码。 测试运行应用程序镜像 # docker run -p 8080:8080 -p 12345:12345 -e PROMETHEUS_JMX_PORT=12345 sample-app . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.4.1.BUILD-SNAPSHOT) 2022-07-28 07:51:48.829 INFO 1 --- [ main] io.kubesphere.devops.Application : Starting Application v0.0.1-SNAPSHOT on 322634c30cc2 with PID 1 (/opt/app/app.jar started by ? in /opt) 2022-07-28 07:51:48.839 INFO 1 --- [ main] io.kubesphere.devops.Application : No active profile set, falling back to default profiles: default 2022-07-28 07:51:48.906 INFO 1 --- [ main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5ae50ce6: startup date [Thu Jul 28 07:51:48 GMT 2022]; root of context hierarchy 2022-07-28 07:51:49.694 INFO 1 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http) 2022-07-28 07:51:49.700 INFO 1 --- [ main] o.apache.catalina.core.StandardService : Starting service Tomcat 2022-07-28 07:51:49.701 INFO 1 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat/8.5.5 2022-07-28 07:51:49.749 INFO 1 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2022-07-28 07:51:49.749 INFO 1 --- [ost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 845 ms 2022-07-28 07:51:49.820 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean : Mapping servlet: 'dispatcherServlet' to [/] 2022-07-28 07:51:49.822 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'characterEncodingFilter' to: [/*] 2022-07-28 07:51:49.822 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'hiddenHttpMethodFilter' to: [/*] 2022-07-28 07:51:49.823 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'httpPutFormContentFilter' to: [/*] 2022-07-28 07:51:49.823 INFO 1 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'requestContextFilter' to: [/*] 2022-07-28 07:51:49.988 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5ae50ce6: startup date [Thu Jul 28 07:51:48 GMT 2022]; root of context hierarchy 2022-07-28 07:51:50.047 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/]}\" onto public java.lang.String io.kubesphere.devops.HelloWorldController.sayHello() 2022-07-28 07:51:50.050 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse) 2022-07-28 07:51:50.050 INFO 1 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity\u003cjava.util.Map\u003cjava.lang.String, java.lang.Object\u003e\u003e org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest) 2022-07-28 07:51:50.063 INFO 1 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2022-07-28 07:51:50.063 INFO 1 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping :","date":"2022-09-11","objectID":"/posts/d83d7d/:0:5","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["云原生"],"content":"自定义S2I模板 apiVersion: devops.kubesphere.io/v1alpha1 kind: S2iBuilderTemplate metadata: labels: controller-tools.k8s.io: \"1.0\" builder-type.kubesphere.io/s2i: \"s2i\" name: java-ubuntu spec: containerInfo: - builderImage: java-ubuntu22 ## 自定义的镜像构建器镜像 codeFramework: java # type of code framework defaultBaseImage: java-ubuntu22 # default Image Builder (can be replaced by a customized image) version: 0.0.1 # Builder template version description: \"模板描述\" # Builder template description kubectl apply -f s2ibuildertemplate.yaml 标签名称 选项 定义 builder-type.kubesphere.io/s2i “s2i” 模板类型为 S2I，基于应用程序源代码构建镜像。 builder-type.kubesphere.io/b2i “b2i” 模板类型为 B2I，基于二进制文件或其他制品构建镜像。 binary-type.kubesphere.io “jar”,“war”,“binary” 该类型为 B2I 类型的补充，在选择 B2I 类型时需要。例如，当提供 Jar 包时，选择 “jar” 类型。在 KubeSphere v2.1.1 及更高版本，允许自定义 B2I 模板。 官方demo apiVersion: devops.kubesphere.io/v1alpha1 kind: S2iBuilderTemplate metadata: annotations: descriptionCN: Java 应用的构建器模版。通过该模版可构建出直接运行的应用镜像。 descriptionEN: This is a builder template for Java builds whose result can be run directly without any further application server.It's suited ideally for microservices with a flat classpath (including \"far jars\"). devops.kubesphere.io/s2i-template-url: https://github.com/kubesphere/s2i-java-container/blob/master/java/images helm.sh/hook: pre-install labels: binary-type.kubesphere.io: jar builder-type.kubesphere.io/b2i: b2i builder-type.kubesphere.io/s2i: s2i controller-tools.k8s.io: \"1.0\" name: java spec: codeFramework: java containerInfo: - buildVolumes: - s2i_java_cache:/tmp/artifacts builderImage: kubesphere/java-8-centos7:v3.2.0 runtimeArtifacts: - source: /deployments runtimeImage: kubesphere/java-8-runtime:v3.2.0 - buildVolumes: - s2i_java_cache:/tmp/artifacts builderImage: kubesphere/java-11-centos7:v3.2.0 runtimeArtifacts: - source: /deployments runtimeImage: kubesphere/java-11-runtime:v3.2.0 defaultBaseImage: kubesphere/java-8-centos7:v3.2.0 description: This is a builder template for Java builds whose result can be run directly without any further application server.It's suited ideally for microservices with a flat classpath (including \"far jars\") environment: - defaultValue: \"\" description: Arguments to use when calling Maven, replacing the default package hawt-app:build -DskipTests -e. Please be sure to run the hawt-app:build goal (when not already bound to the package execution phase), otherwise the startup scripts won't work. key: MAVEN_ARGS required: false type: string - defaultValue: \"\" description: Additional Maven arguments, useful for temporary adding arguments like -X or -am -pl . key: MAVEN_ARGS_APPEND required: false type: string - defaultValue: \"\" description: With Repositories you specify from which locations you want to download certain artifacts, such as dependencies and maven-plugins. key: MAVEN_MIRROR_URL required: false type: string - defaultValue: \"\" description: If set then the Maven repository is removed after the artifact is built. This is useful for keeping the created application image small, but prevents incremental builds. The default is false key: MAVEN_CLEAR_REPO required: false type: boolean - defaultValue: \"\" description: Path to target/ where the jar files are created for multi module builds. These are added to ${MAVEN_ARGS} key: ARTIFACT_DIR required: false type: string - defaultValue: \"\" description: Arguments to use when copying artifacts from the output dir to the application dir. Useful to specify which artifacts will be part of the image. It defaults to -r hawt-app/* when a hawt-app dir is found on the build directory, otherwise jar files only will be included (*.jar). key: ARTIFACT_COPY_ARGS required: false type: string - defaultValue: \"\" description: the directory where the application resides. All paths in your application are relative to this directory. By default it is the same directory where this startup script resides. key: JAVA_APP_DIR required: false type: string - defaultValue: \"\" description: directory holding the Java jar files as well a","date":"2022-09-11","objectID":"/posts/d83d7d/:0:6","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["云原生"],"content":"参考资料 s2i-base-container s2i-java-container s2i-java-runtimeImage ","date":"2022-09-11","objectID":"/posts/d83d7d/:0:7","tags":["kubesphere","operator"],"title":"S2I自定义构建器和模板","uri":"/posts/d83d7d/"},{"categories":["golang"],"content":"初始化demo项目 mkdir -p gin-middleware-demo cd gin-middleware-demo go mod init github.com/kbsonlong/gin-middleware-demo middleware/middle.go package middleware import ( \"fmt\" \"time\" \"github.com/gin-gonic/gin\" ) func MdOne() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第一个 gin 中间件:\" + time.Now().String()) c.Next() fmt.Println(\"第一个 gin 中间件返回内容:\" + time.Now().String()) } } func MdTwo() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第二个 gin 中间件:\" + time.Now().String()) c.Next() fmt.Println(\"第二个 gin 中间件返回内容:\" + time.Now().String()) } } func MdThree() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第三个 gin 中间件:\" + time.Now().String()) c.Next() fmt.Println(\"第三个 gin 中间件返回内容:\" + time.Now().String()) } } routers/router.go package routers import ( \"github.com/gin-gonic/gin\" \"github.com/kbsonlong/gin-middleware-demo/middleware\" ) func InitRouter() *gin.Engine { r := gin.New() r.Use(middleware.MdOne()) r.Use(middleware.MdTwo()) r.Use(middleware.MdThree()) gin.SetMode(\"debug\") r.GET(\"/\", func(ctx *gin.Context) { ctx.JSON(200, gin.H{ \"message\": \"test\", }) }) return r } main.go package main import ( \"github.com/kbsonlong/gin-middleware-demo/routers\" ) func main() { router := routers.InitRouter() router.Run() } ","date":"2022-09-11","objectID":"/posts/97ba92/:0:1","tags":["gin"],"title":"Gin中间件执行顺序","uri":"/posts/97ba92/"},{"categories":["golang"],"content":"运行测试 go mod tidy go run main.go [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET / --\u003e github.com/kbsonlong/gin-middleware-demo/routers.InitRouter.func1 (4 handlers) [GIN-debug] [WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value. Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details. [GIN-debug] Environment variable PORT is undefined. Using port :8080 by default [GIN-debug] Listening and serving HTTP on :8080 另一终端执行 curl 127.0.0.1:8080 {\"message\":\"test\"} 可以看到控制台输出内容 开始执行第一个 gin 中间件:2022-07-08 17:18:57.499033 +0800 CST m=+3.029327751 开始执行第二个 gin 中间件:2022-07-08 17:18:57.499215 +0800 CST m=+3.029509918 开始执行第三个 gin 中间件:2022-07-08 17:18:57.499218 +0800 CST m=+3.029513043 第三个 gin 中间件返回内容:2022-07-08 17:18:57.499295 +0800 CST m=+3.029589876 第二个 gin 中间件返回内容:2022-07-08 17:18:57.499298 +0800 CST m=+3.029593335 第一个 gin 中间件返回内容:2022-07-08 17:18:57.4993 +0800 CST m=+3.029594876 调整中间件Use顺序 ...... r.Use(middleware.MdOne()) r.Use(middleware.MdThree()) r.Use(middleware.MdTwo()) gin.SetMode(\"debug\") ...... 第二个中间件和第三个中间件顺序对调 开始执行第一个 gin 中间件:2022-07-08 17:20:57.702026 +0800 CST m=+2.567931210 开始执行第三个 gin 中间件:2022-07-08 17:20:57.702215 +0800 CST m=+2.568120210 开始执行第二个 gin 中间件:2022-07-08 17:20:57.702218 +0800 CST m=+2.568123543 第二个 gin 中间件返回内容:2022-07-08 17:20:57.702305 +0800 CST m=+2.568210460 第三个 gin 中间件返回内容:2022-07-08 17:20:57.702308 +0800 CST m=+2.568214043 第一个 gin 中间件返回内容:2022-07-08 17:20:57.702311 +0800 CST m=+2.568216293 可以看到第三个中间件在第二个中间件之前执行 注释第三个中间件Next func MdThree() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\"开始执行第三个 gin 中间件:\" + time.Now().String()) // c.Next() fmt.Println(\"第三个 gin 中间件返回内容:\" + time.Now().String()) } } 开始执行第一个 gin 中间件:2022-07-08 17:22:43.132983 +0800 CST m=+4.061931376 开始执行第三个 gin 中间件:2022-07-08 17:22:43.133126 +0800 CST m=+4.062074668 第三个 gin 中间件返回内容:2022-07-08 17:22:43.133128 +0800 CST m=+4.062076751 开始执行第二个 gin 中间件:2022-07-08 17:22:43.13313 +0800 CST m=+4.062078210 第二个 gin 中间件返回内容:2022-07-08 17:22:43.133203 +0800 CST m=+4.062151335 第一个 gin 中间件返回内容:2022-07-08 17:22:43.133205 +0800 CST m=+4.062153460 ","date":"2022-09-11","objectID":"/posts/97ba92/:0:2","tags":["gin"],"title":"Gin中间件执行顺序","uri":"/posts/97ba92/"},{"categories":["golang"],"content":"总结 Gin中间件的调用顺序与Use顺序有关，代码运行顺序和Next前后顺序有关。 Next之前代码先进先出 Next之后代码后进先出 没有引用Next代码直接运行 ","date":"2022-09-11","objectID":"/posts/97ba92/:0:3","tags":["gin"],"title":"Gin中间件执行顺序","uri":"/posts/97ba92/"},{"categories":["云原生","Istio"],"content":" Sidecar 自动注入机制是将 sidecar 代理自动添加到用户创建的 pod。 它使用 MutatingWebhook 机制在 pod 创建的时候将 sidecar 的容器和卷添加到每个 pod 的模版里。 用户可以通过 webhooks namespaceSelector 机制来限定需要启动自动注入的范围，也可以通过注解的方式针对每个 pod 来单独启用和禁用自动注入功能。 Sidecar 是否会被自动注入取决于下面 3 条配置和 2 条安全规则： 配置: webhooks namespaceSelector 默认策略 policy pod 级别的覆盖注解 安全规则: sidecar 默认不能被注入到 kube-system 和 kube-public 这两个 namespace sidecar 不能被注入到使用 host network 网络的 pod 里 下面的表格展示了基于上述三个配置条件的最终注入状态。上述的安全规则不会被覆盖。 namespaceSelector 匹配 默认策略 sidecar.istio.io/inject 注解 Sidecar 是否注入 是 enabled true (default) 是 是 enabled false 否 是 disabled true 是 是 disabled false (default) 否 否 enabled true (default) 否 否 enabled false 否 否 disabled true 否 否 disabled false (default) 否 以下内容基于Istio 1.13.2版本 ","date":"2022-09-11","objectID":"/posts/dd0bd7/:0:0","tags":["istio","Sidecar"],"title":"深入Istio系列-Sidecar自动注入","uri":"/posts/dd0bd7/"},{"categories":["云原生","Istio"],"content":"NewWehook方法 pkg/kube/inject/webhook.go func NewWebhook(p WebhookParameters) (*Webhook, error) { if p.Mux == nil { return nil, errors.New(\"expected mux to be passed, but was not passed\") } wh := \u0026Webhook{ watcher: p.Watcher, meshConfig: p.Env.Mesh(), env: p.Env, revision: p.Revision, } p.Watcher.SetHandler(wh.updateConfig) sidecarConfig, valuesConfig, err := p.Watcher.Get() if err != nil { return nil, err } wh.updateConfig(sidecarConfig, valuesConfig) //初始化Webhook实例的时候注册/inject对应的处理器 p.Mux.HandleFunc(\"/inject\", wh.serveInject) p.Mux.HandleFunc(\"/inject/\", wh.serveInject) p.Env.Watcher.AddMeshHandler(func() { wh.mu.Lock() wh.meshConfig = p.Env.Mesh() wh.mu.Unlock() }) return wh, nil } ","date":"2022-09-11","objectID":"/posts/dd0bd7/:0:1","tags":["istio","Sidecar"],"title":"深入Istio系列-Sidecar自动注入","uri":"/posts/dd0bd7/"},{"categories":["云原生","Istio"],"content":"serveInject方法 pkg/kube/inject/webhook.go 大概825-895行 func (wh *Webhook) serveInject(w http.ResponseWriter, r *http.Request) { totalInjections.Increment() var body []byte // 获取请求体 if r.Body != nil { if data, err := kube.HTTPConfigReader(r); err == nil { body = data } else { http.Error(w, err.Error(), http.StatusBadRequest) return } } if len(body) == 0 { handleError(\"no body found\") http.Error(w, \"no body found\", http.StatusBadRequest) return } // verify the content type is accurate contentType := r.Header.Get(\"Content-Type\") if contentType != \"application/json\" { handleError(fmt.Sprintf(\"contentType=%s, expect application/json\", contentType)) http.Error(w, \"invalid Content-Type, want `application/json`\", http.StatusUnsupportedMediaType) return } path := \"\" if r.URL != nil { path = r.URL.Path } var reviewResponse *kube.AdmissionResponse var obj runtime.Object var ar *kube.AdmissionReview // 解码请求体 if out, _, err := deserializer.Decode(body, nil, obj); err != nil { handleError(fmt.Sprintf(\"Could not decode body: %v\", err)) reviewResponse = toAdmissionResponse(err) } else { log.Debugf(\"AdmissionRequest for path=%s\\n\", path) ar, err = kube.AdmissionReviewKubeToAdapter(out) if err != nil { handleError(fmt.Sprintf(\"Could not decode object: %v\", err)) } // 进入inject方法逻辑判断 reviewResponse = wh.inject(ar, path) } response := kube.AdmissionReview{} response.Response = reviewResponse var responseKube runtime.Object var apiVersion string if ar != nil { apiVersion = ar.APIVersion response.TypeMeta = ar.TypeMeta if response.Response != nil { if ar.Request != nil { response.Response.UID = ar.Request.UID } } } responseKube = kube.AdmissionReviewAdapterToKube(\u0026response, apiVersion) resp, err := json.Marshal(responseKube) if err != nil { log.Errorf(\"Could not encode response: %v\", err) http.Error(w, fmt.Sprintf(\"could not encode response: %v\", err), http.StatusInternalServerError) } if _, err := w.Write(resp); err != nil { log.Errorf(\"Could not write response: %v\", err) http.Error(w, fmt.Sprintf(\"could not write response: %v\", err), http.StatusInternalServerError) } } ","date":"2022-09-11","objectID":"/posts/dd0bd7/:0:2","tags":["istio","Sidecar"],"title":"深入Istio系列-Sidecar自动注入","uri":"/posts/dd0bd7/"},{"categories":["云原生","Istio"],"content":"inject方法 pkg/kube/inject/webhook.go 大概在748-823行 func (wh *Webhook) inject(ar *kube.AdmissionReview, path string) *kube.AdmissionResponse { req := ar.Request var pod corev1.Pod if err := json.Unmarshal(req.Object.Raw, \u0026pod); err != nil { handleError(fmt.Sprintf(\"Could not unmarshal raw object: %v %s\", err, string(req.Object.Raw))) return toAdmissionResponse(err) } // Managed fields is sometimes extremely large, leading to excessive CPU time on patch generation // It does not impact the injection output at all, so we can just remove it. pod.ManagedFields = nil // Deal with potential empty fields, e.g., when the pod is created by a deployment podName := potentialPodName(pod.ObjectMeta) if pod.ObjectMeta.Namespace == \"\" { pod.ObjectMeta.Namespace = req.Namespace } log.Infof(\"Sidecar injection request for %v/%v\", req.Namespace, podName) log.Debugf(\"Object: %v\", string(req.Object.Raw)) log.Debugf(\"OldObject: %v\", string(req.OldObject.Raw)) wh.mu.RLock() // Sicader注入判断逻辑 if !injectRequired(IgnoredNamespaces.UnsortedList(), wh.Config, \u0026pod.Spec, pod.ObjectMeta) { log.Infof(\"Skipping %s/%s due to policy check\", pod.ObjectMeta.Namespace, podName) totalSkippedInjections.Increment() wh.mu.RUnlock() return \u0026kube.AdmissionResponse{ Allowed: true, } } proxyConfig := mesh.DefaultProxyConfig() if wh.env.PushContext != nil \u0026\u0026 wh.env.PushContext.ProxyConfigs != nil { if generatedProxyConfig := wh.env.PushContext.ProxyConfigs.EffectiveProxyConfig( \u0026model.NodeMetadata{ Namespace: pod.Namespace, Labels: pod.Labels, Annotations: pod.Annotations, }, wh.meshConfig); generatedProxyConfig != nil { proxyConfig = *generatedProxyConfig } } deploy, typeMeta := kube.GetDeployMetaFromPod(\u0026pod) params := InjectionParameters{ pod: \u0026pod, deployMeta: deploy, typeMeta: typeMeta, templates: wh.Config.Templates, defaultTemplate: wh.Config.DefaultTemplates, aliases: wh.Config.Aliases, meshConfig: wh.meshConfig, proxyConfig: \u0026proxyConfig, valuesConfig: wh.valuesConfig, revision: wh.revision, injectedAnnotations: wh.Config.InjectedAnnotations, proxyEnvs: parseInjectEnvs(path), } wh.mu.RUnlock() patchBytes, err := injectPod(params) if err != nil { handleError(fmt.Sprintf(\"Pod injection failed: %v\", err)) return toAdmissionResponse(err) } reviewResponse := kube.AdmissionResponse{ Allowed: true, Patch: patchBytes, PatchType: func() *string { pt := \"JSONPatch\" return \u0026pt }(), } totalSuccessfulInjections.Increment() return \u0026reviewResponse } ","date":"2022-09-11","objectID":"/posts/dd0bd7/:0:3","tags":["istio","Sidecar"],"title":"深入Istio系列-Sidecar自动注入","uri":"/posts/dd0bd7/"},{"categories":["云原生","Istio"],"content":"injectRequired方法 pkg/kube/inject/inject.go 大概在180-290行 func injectRequired(ignored []string, config *Config, podSpec *corev1.PodSpec, metadata metav1.ObjectMeta) bool { // nolint: lll // Skip injection when host networking is enabled. The problem is // that the iptables changes are assumed to be within the pod when, // in fact, they are changing the routing at the host level. This // often results in routing failures within a node which can // affect the network provider within the cluster causing // additional pod failures. // 主机网络模式不注入sicader if podSpec.HostNetwork { return false } // skip special kubernetes system namespaces // kube-system、kube-public、kube-node-lease、local-path-storage四个名称空间不被注入sicader for _, namespace := range ignored { if metadata.Namespace == namespace { return false } } annos := metadata.GetAnnotations() var useDefault bool var inject bool // annotation 是否开启注入 `sidecar.istio.io/inject: \"true\"` objectSelector := annos[annotation.SidecarInject.Name] if lbl, labelPresent := metadata.GetLabels()[annotation.SidecarInject.Name]; labelPresent { // The label is the new API; if both are present we prefer the label objectSelector = lbl } switch strings.ToLower(objectSelector) { // http://yaml.org/type/bool.html case \"y\", \"yes\", \"true\", \"on\": inject = true case \"\": useDefault = true } // If an annotation is not explicitly given, check the LabelSelectors, starting with NeverInject // 判断 configmap `istio-sidecar-injector` NeverInject 匹配标签选择器 if useDefault { for _, neverSelector := range config.NeverInjectSelector { selector, err := metav1.LabelSelectorAsSelector(\u0026neverSelector) if err != nil { log.Warnf(\"Invalid selector for NeverInjectSelector: %v (%v)\", neverSelector, err) } else if !selector.Empty() \u0026\u0026 selector.Matches(labels.Set(metadata.Labels)) { log.Debugf(\"Explicitly disabling injection for pod %s/%s due to pod labels matching NeverInjectSelector config map entry.\", metadata.Namespace, potentialPodName(metadata)) inject = false useDefault = false break } } } // If there's no annotation nor a NeverInjectSelector, check the AlwaysInject one // 判断 configmap `istio-sidecar-injector` AlwaysInject 匹配标签选择器 if useDefault { for _, alwaysSelector := range config.AlwaysInjectSelector { selector, err := metav1.LabelSelectorAsSelector(\u0026alwaysSelector) if err != nil { log.Warnf(\"Invalid selector for AlwaysInjectSelector: %v (%v)\", alwaysSelector, err) } else if !selector.Empty() \u0026\u0026 selector.Matches(labels.Set(metadata.Labels)) { log.Debugf(\"Explicitly enabling injection for pod %s/%s due to pod labels matching AlwaysInjectSelector config map entry.\", metadata.Namespace, potentialPodName(metadata)) inject = true useDefault = false break } } } var required bool // 判断 configmap `istio-sidecar-injector` 默认策略policy switch config.Policy { default: // InjectionPolicyOff log.Errorf(\"Illegal value for autoInject:%s, must be one of [%s,%s]. Auto injection disabled!\", config.Policy, InjectionPolicyDisabled, InjectionPolicyEnabled) required = false case InjectionPolicyDisabled: if useDefault { required = false } else { required = inject } case InjectionPolicyEnabled: if useDefault { required = true } else { required = inject } } if log.DebugEnabled() { // Build a log message for the annotations. annotationStr := \"\" for name := range AnnotationValidation { value, ok := annos[name] if !ok { value = \"(unset)\" } annotationStr += fmt.Sprintf(\"%s:%s \", name, value) } log.Debugf(\"Sidecar injection policy for %v/%v: namespacePolicy:%v useDefault:%v inject:%v required:%v %s\", metadata.Namespace, potentialPodName(metadata), config.Policy, useDefault, inject, required, annotationStr) } return required } ","date":"2022-09-11","objectID":"/posts/dd0bd7/:0:4","tags":["istio","Sidecar"],"title":"深入Istio系列-Sidecar自动注入","uri":"/posts/dd0bd7/"},{"categories":["云原生"],"content":"压测大纲 压测的必要性 压测部署架构图 环境准备 ","date":"2022-09-11","objectID":"/posts/129682/:0:0","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"部署 Istio ","date":"2022-09-11","objectID":"/posts/129682/:1:0","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"部署监控组件 ","date":"2022-09-11","objectID":"/posts/129682/:2:0","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"部署压测服务 ","date":"2022-09-11","objectID":"/posts/129682/:3:0","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"Fortio v1版本 apiVersion: apps/v1 kind: Deployment metadata: name: fortio-server-l3 labels: app: fortio-server-l3 version: v1 spec: replicas: 1 selector: matchLabels: app: fortio-server-l3 template: metadata: labels: app: fortio-server-l3 version: v1 spec: containers: - name: fortio-server-l3 # 在中国，你可以使用 docker.mirrors.ustc.edu.cn/fortio/fortio:latest image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio - server --- apiVersion: v1 kind: Service metadata: name: fortio-server-l3 spec: ports: - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-server-l3 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: fortio-server-l2 labels: app: fortio-server-l2 version: v1 spec: replicas: 1 selector: matchLabels: app: fortio-server-l2 template: metadata: labels: app: fortio-server-l2 version: v1 spec: containers: - name: fortio-server-l2 # 在中国，你可以使用 docker.mirrors.ustc.edu.cn/fortio/fortio:latest image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8072 name: http-m - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio args: [\"server\", \"-P\", \"8072 fortio-server-l3:8080\"] --- apiVersion: v1 kind: Service metadata: name: fortio-server-l2 spec: ports: - name: http-m port: 8072 protocol: TCP targetPort: 8072 - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-server-l2 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: fortio-server-l1 labels: app: fortio-server-l1 version: v1 spec: replicas: 1 selector: matchLabels: app: fortio-server-l1 template: metadata: labels: app: fortio-server-l1 version: v1 spec: containers: - name: fortio-server-l1 # 在中国，你可以使用 docker.mirrors.ustc.edu.cn/fortio/fortio:latest image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8071 name: http-m - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio args: [\"server\", \"-P\", \"8071 fortio-server-l2:8072\"] --- apiVersion: v1 kind: Service metadata: name: fortio-server-l1 spec: ports: - name: http-m port: 8071 protocol: TCP targetPort: 8071 - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-server-l1 type: NodePort ","date":"2022-09-11","objectID":"/posts/129682/:3:1","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"Fortio v2版本 apiVersion: apps/v1 kind: Deployment metadata: name: fortio-server-l3 labels: app: fortio-server-l3 version: v1 spec: replicas: 1 selector: matchLabels: app: fortio-server-l3 template: metadata: labels: app: fortio-server-l3 version: v1 spec: containers: - name: fortio-server-l3 # 在中国，你可以使用 docker.mirrors.ustc.edu.cn/fortio/fortio:latest image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio - server --- apiVersion: v1 kind: Service metadata: name: fortio-server-l3 spec: ports: - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-server-l3 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: fortio-server-l2 labels: app: fortio-server-l2 version: v1 spec: replicas: 1 selector: matchLabels: app: fortio-server-l2 template: metadata: labels: app: fortio-server-l2 version: v1 spec: containers: - name: fortio-server-l2 # 在中国，你可以使用 docker.mirrors.ustc.edu.cn/fortio/fortio:latest image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8072 name: http-m - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio args: [\"server\", \"-P\", \"8072 fortio-server-l3:8080\"] --- apiVersion: v1 kind: Service metadata: name: fortio-server-l2 spec: ports: - name: http-m port: 8072 protocol: TCP targetPort: 8072 - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-server-l2 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: fortio-server-l1 labels: app: fortio-server-l1 version: v1 spec: replicas: 1 selector: matchLabels: app: fortio-server-l1 template: metadata: labels: app: fortio-server-l1 version: v1 spec: containers: - name: fortio-server-l1 # 在中国，你可以使用 docker.mirrors.ustc.edu.cn/fortio/fortio:latest image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8071 name: http-m - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio args: [\"server\", \"-P\", \"8071 fortio-server-l2:8072\"] --- apiVersion: v1 kind: Service metadata: name: fortio-server-l1 spec: ports: - name: http-m port: 8071 protocol: TCP targetPort: 8071 - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-server-l1 type: NodePort apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: fortio-server-l1 spec: hosts: - fortio-server-l1 http: - route: - destination: host: fortio-server-l1 subset: v1 weight: 10 - destination: host: fortio-server-l1 subset: v2 weight: 90 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: fortio-server-l2 spec: hosts: - fortio-server-l2 http: - route: - destination: host: fortio-server-l2 subset: v1 weight: 0 - destination: host: fortio-server-l2 subset: v2 weight: 100 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: fortio-server-l1 spec: host: fortio-server-l1 subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: fortio-server-l2 spec: host: fortio-server-l2 subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 ","date":"2022-09-11","objectID":"/posts/129682/:3:2","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"部署压测工具 apiVersion: apps/v1 kind: Deployment metadata: name: fortio-client labels: app: fortio-client version: v2 spec: replicas: 1 selector: matchLabels: app: fortio-client template: metadata: labels: app: fortio-client version: v2 spec: containers: - name: fortio-client image: fortio/fortio:latest ports: - containerPort: 8080 name: http-port - containerPort: 8078 name: udp-port - containerPort: 8079 name: grpc-port - containerPort: 8081 name: https-port command: - fortio - server --- apiVersion: v1 kind: Service metadata: name: fortio-client spec: ports: - name: http-port port: 8080 protocol: TCP targetPort: 8080 - name: https-port port: 8081 protocol: TCP targetPort: 8081 - name: http2-grpc port: 8079 protocol: TCP targetPort: 8079 - name: udp-grpc port: 8078 protocol: UDP targetPort: 8078 selector: app: fortio-client type: NodePort 性能压测 压测报告 画红线部分 min: 最小响应时间2.97ms average: 平均响应时间11.286ms P50: 50%的请求响应时间在10.18ms内 P75: 75%的请求响应时间在13.48ms内 P90: 90%的请求响应时间在17.13ms内 P99: 99%的请求响应时间在29.89ms内 P99.9: 99.9%的请求响应时间在80.91ms内 max: 最大响应时间324.597ms 参考资料 记一次 Istio 冲刺调优 你的 Istio Mesh 性能及格吗 Istio 性能基准测试 性能绘图 Istio 性能和可扩展性 ","date":"2022-09-11","objectID":"/posts/129682/:4:0","tags":["istio"],"title":"Istio性能测试","uri":"/posts/129682/"},{"categories":["云原生"],"content":"Helm 安装 ","date":"2022-09-11","objectID":"/posts/bf667a/:1:0","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"安装基础资源 helm template istio-base manifests/charts/base \\ -n istio-system \\ --set base.enableCRDTemplates=true --- # Source: base/templates/reader-serviceaccount.yaml # This service account aggregates reader permissions for the revisions in a given cluster # Should be used for remote secret creation. apiVersion: v1 kind: ServiceAccount metadata: name: istio-reader-service-account namespace: istio-system labels: app: istio-reader release: istio-base --- # Source: base/templates/serviceaccount.yaml # -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=- # DO NOT EDIT! # THIS IS A LEGACY CHART HERE FOR BACKCOMPAT # UPDATED CHART AT manifests/charts/istio-control/istio-discovery # -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=- apiVersion: v1 kind: ServiceAccount metadata: name: istiod-service-account namespace: istio-system labels: app: istiod release: istio-base --- # Source: base/templates/crds.yaml # DO NOT EDIT - Generated by Cue OpenAPI generator based on Istio APIs. apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: annotations: \"helm.sh/resource-policy\": keep labels: app: istio-pilot chart: istio heritage: Tiller release: istio name: wasmplugins.extensions.istio.io spec: group: extensions.istio.io names: categories: - istio-io - extensions-istio-io kind: WasmPlugin listKind: WasmPluginList plural: wasmplugins singular: wasmplugin scope: Namespaced versions: - additionalPrinterColumns: - description: 'CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata' jsonPath: .metadata.creationTimestamp name: Age type: date name: v1alpha1 schema: openAPIV3Schema: properties: spec: description: 'Extend the functionality provided by the Istio proxy through WebAssembly filters. See more details at: https://istio.io/docs/reference/config/proxy_extensions/wasm-plugin.html' properties: imagePullPolicy: description: The pull behaviour to be applied when fetching an OCI image. enum: - UNSPECIFIED_POLICY - IfNotPresent - Always type: string imagePullSecret: description: Credentials to use for OCI image pulling. type: string phase: description: Determines where in the filter chain this `WasmPlugin` is to be injected. enum: - UNSPECIFIED_PHASE - AUTHN - AUTHZ - STATS type: string pluginConfig: description: The configuration that will be passed on to the plugin. type: object x-kubernetes-preserve-unknown-fields: true pluginName: type: string priority: description: Determines ordering of `WasmPlugins` in the same `phase`. nullable: true type: integer selector: properties: matchLabels: additionalProperties: type: string type: object type: object sha256: description: SHA256 checksum that will be used to verify Wasm module or OCI container. type: string url: description: URL of a Wasm module or OCI container. type: string verificationKey: type: string type: object status: type: object x-kubernetes-preserve-unknown-fields: true type: object served: true storage: true subresources: status: {} --- # Source: base/templates/crds.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: annotations: \"helm.sh/resource-policy\": keep labels: app: istio-pilot chart: istio heritage: Tiller release: istio name: destinationrules.networking.istio.io spec: group: networking.istio.io names: categories: - istio-io - networking-istio-io kind: DestinationRule listKind: DestinationRuleList plural: destinationrules shortNames: - dr singular: destinationrule scope: Namespaced versions: - additionalPrinterColumns: - description: The name of a service from the service registry jsonPath: .spec.host name: Host type: string - description: 'CreationTimestamp is a timestamp representing the server time when this ob","date":"2022-09-11","objectID":"/posts/bf667a/:1:1","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"安装Istiod helm template istiod manifests/charts/istio-control/istio-discovery \\ --set global.hub=\"registry.cn-hangzhou.aliyuncs.com/seam\" \\ --set global.tag=\"1.13.4\" \\ --set pilot.image=\"istio-pilot\" \\ --set meshConfig.trustDomain=\"alongparty.cn\" \\ --set global.proxy.clusterDomain=\"alongparty.cn\" \\ --set global.proxy.resources.limits.cpu=\"2000m\" \\ --set global.proxy.resources.limits.memory=\"4096Mi\" \\ --set pilot.resources.limits.cpu=\"2000m\" \\ --set pilot.resources.limits.memory=\"4096Mi\" \\ -n istio-system ","date":"2022-09-11","objectID":"/posts/bf667a/:1:2","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"安装IngressGateway helm template istio-ingress manifests/charts/gateways/istio-ingress \\ --set global.hub=\"registry.cn-hangzhou.aliyuncs.com/seam\" \\ --set global.tag=\"1.13.4\" \\ --set global.proxy.image=\"istio-proxyv2\" \\ --set meshConfig.trustDomain=\"alongparty.cn\" \\ --set global.proxy.clusterDomain=\"alongparty.cn\" \\ -n istio-system ","date":"2022-09-11","objectID":"/posts/bf667a/:1:3","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"安装EgressGateway helm template istio-egress manifests/charts/gateways/istio-egress \\ --set global.hub=\"registry.cn-hangzhou.aliyuncs.com/seam\" \\ --set global.tag=\"1.13.4\" \\ --set global.proxy.image=\"istio-proxyv2\" \\ --set meshConfig.trustDomain=\"alongparty.cn\" \\ --set global.proxy.clusterDomain=\"alongparty.cn\" \\ -n istio-system ","date":"2022-09-11","objectID":"/posts/bf667a/:1:4","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"Istio-operator安装 ","date":"2022-09-11","objectID":"/posts/bf667a/:2:0","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"创建 istio-operator 名称空间 kubectl apply -f - \u003c\u003cEOF --- apiVersion: v1 kind: Namespace metadata: name: istio-operator EOF ","date":"2022-09-11","objectID":"/posts/bf667a/:2:1","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"部署 Istio-operator kubectl apply -f - \u003c\u003cEOF --- # Source: istio-operator/templates/service_account.yaml apiVersion: v1 kind: ServiceAccount metadata: namespace: istio-operator name: istio-operator --- # Source: istio-operator/templates/crds.yaml # SYNC WITH manifests/charts/base/files apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: istiooperators.install.istio.io labels: release: istio spec: conversion: strategy: None group: install.istio.io names: kind: IstioOperator listKind: IstioOperatorList plural: istiooperators singular: istiooperator shortNames: - iop - io scope: Namespaced versions: - additionalPrinterColumns: - description: Istio control plane revision jsonPath: .spec.revision name: Revision type: string - description: IOP current state jsonPath: .status.status name: Status type: string - description: 'CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata' jsonPath: .metadata.creationTimestamp name: Age type: date name: v1alpha1 subresources: status: {} schema: openAPIV3Schema: type: object x-kubernetes-preserve-unknown-fields: true served: true storage: true --- # Source: istio-operator/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: creationTimestamp: null name: istio-operator rules: # istio groups - apiGroups: - authentication.istio.io resources: - '*' verbs: - '*' - apiGroups: - config.istio.io resources: - '*' verbs: - '*' - apiGroups: - install.istio.io resources: - '*' verbs: - '*' - apiGroups: - networking.istio.io resources: - '*' verbs: - '*' - apiGroups: - security.istio.io resources: - '*' verbs: - '*' # k8s groups - apiGroups: - admissionregistration.k8s.io resources: - mutatingwebhookconfigurations - validatingwebhookconfigurations verbs: - '*' - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions.apiextensions.k8s.io - customresourcedefinitions verbs: - '*' - apiGroups: - apps - extensions resources: - daemonsets - deployments - deployments/finalizers - replicasets verbs: - '*' - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - '*' - apiGroups: - monitoring.coreos.com resources: - servicemonitors verbs: - get - create - update - apiGroups: - policy resources: - poddisruptionbudgets verbs: - '*' - apiGroups: - rbac.authorization.k8s.io resources: - clusterrolebindings - clusterroles - roles - rolebindings verbs: - '*' - apiGroups: - coordination.k8s.io resources: - leases verbs: - get - create - update - apiGroups: - \"\" resources: - configmaps - endpoints - events - namespaces - pods - pods/proxy - pods/portforward - persistentvolumeclaims - secrets - services - serviceaccounts verbs: - '*' --- # Source: istio-operator/templates/clusterrole_binding.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: istio-operator subjects: - kind: ServiceAccount name: istio-operator namespace: istio-operator roleRef: kind: ClusterRole name: istio-operator apiGroup: rbac.authorization.k8s.io --- # Source: istio-operator/templates/service.yaml apiVersion: v1 kind: Service metadata: namespace: istio-operator labels: name: istio-operator name: istio-operator spec: ports: - name: http-metrics port: 8383 targetPort: 8383 protocol: TCP selector: name: istio-operator --- # Source: istio-operator/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: namespace: istio-operator name: istio-operator spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: name: istio-operator template: metadata: labels: name: istio-operator spec: serviceAccountName: istio-operator containers: - name: istio-operator image: docker","date":"2022-09-11","objectID":"/posts/bf667a/:2:2","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"创建 istio-system 名称空间 kubectl apply -f - \u003c\u003cEOF --- apiVersion: v1 kind: Namespace metadata: name: istio-system EOF ","date":"2022-09-11","objectID":"/posts/bf667a/:2:3","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"使用demo配置项安装istio组件 kubectl apply -f - \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istiocontrolplane spec: profile: demo EOF ","date":"2022-09-11","objectID":"/posts/bf667a/:2:4","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"更新IstioOperator kubectl apply -f - \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istiocontrolplane spec: profile: default EOF ","date":"2022-09-11","objectID":"/posts/bf667a/:2:5","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":["云原生"],"content":"启用 istio-egressgateway 组件并增加 pilot 的资源要求和HPA kubectl apply -f - \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istiocontrolplane spec: profile: default components: pilot: k8s: resources: requests: cpu: 1000m # override from default 500m memory: 4096Mi # ... default 2048Mi hpaSpec: maxReplicas: 10 # ... default 5 minReplicas: 2 # ... default 1 egressGateways: - name: istio-egressgateway enabled: true EOF operator 渲染顺序 ","date":"2022-09-11","objectID":"/posts/bf667a/:2:6","tags":["istio"],"title":"Istio部署实战","uri":"/posts/bf667a/"},{"categories":null,"content":"修改认证方式 # vim /etc/ocserv/ocserv.conf auth = \"plain[passwd=/etc/ocserv/ocpasswd,otp=/etc/ocserv/ocserv.otp]\" ","date":"2022-09-11","objectID":"/posts/e96811/:0:1","tags":null,"title":"openconnect双因素认证","uri":"/posts/e96811/"},{"categories":null,"content":"配置pam echo \"auth requisite pam_oath.so debug usersfile=/etc/ocserv/ocserv.otp window=20\" \u003e\u003e /etc/pam.d/ocserv ","date":"2022-09-11","objectID":"/posts/e96811/:0:2","tags":null,"title":"openconnect双因素认证","uri":"/posts/e96811/"},{"categories":null,"content":"创建用户OTP username=\"zengshenglong\" company=\"Company\" site_name=\"Site\" key=$(head -c 16 /dev/urandom |xxd -c 256 -ps) echo \"HOTP/T30 ${username} - ${key}\" \u003e\u003e/etc/ocserv/ocserv.otp oathtool --totp -w 5 $key secret=$(echo 0x${key}|xxd -r -c 256|base32) echo \"otpauth://hotp/${username}@${site_name}?secret=${secret}\u0026issuer=${company}\" | qrencode -o - -t UTF8 echo \"https://www.google.com/chart?chs=200x200\u0026chld=M|0\u0026cht=qr\u0026chl=otpauth://hotp/${username}@${site_name}?secret=$(echo ${secret}|cut -d = -f 1)\u0026issuer=${company}\" ","date":"2022-09-11","objectID":"/posts/e96811/:0:3","tags":null,"title":"openconnect双因素认证","uri":"/posts/e96811/"},{"categories":null,"content":"完整配置 auth = \"plain[passwd=/etc/ocserv/ocpasswd,otp=/etc/ocserv/ocserv.otp]\" tcp-port = 443 udp-port = 443 run-as-user = nobody run-as-group = daemon socket-file = /var/run/ocserv-socket server-cert = /etc/letsencrypt/live/myvpn.alongparty.cn/fullchain.pem server-key = /etc/letsencrypt/live/myvpn.alongparty.cn/privkey.pem isolate-workers = false banner = \"Welcome PCI DSS environment, please proceed with caution ! !\" pre-login-banner = \"You will enter the PCI DSS environment, please proceed with caution ! !\" max-clients = 16 max-same-clients = 2 rate-limit-ms = 100 server-stats-reset-time = 604800 keepalive = 32400 dpd = 90 mobile-dpd = 1800 switch-to-tcp-timeout = 25 try-mtu-discovery = true cert-user-oid = 0.9.2342.19200300.100.1.1 tls-priorities = \"NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1\" auth-timeout = 240 min-reauth-time = 300 max-ban-score = 80 ban-reset-time = 1200 cookie-timeout = 300 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true pid-file = /var/run/ocserv.pid device = vpns predictable-ips = true default-domain = example.com ipv4-network = 10.255.255.0 ipv4-netmask = 255.255.255.0 tunnel-all-dns = true dns = 8.8.8.8 dns = 4.2.2.4 dns = 2001:4860:4860::8888 dns = 2001:4860:4860::8844 ping-leases = false config-per-group = /etc/ocserv/group default-group-config = /etc/ocserv/group/default default-select-group = default auto-select-group = false cisco-client-compat = true dtls-legacy = true ","date":"2022-09-11","objectID":"/posts/e96811/:0:4","tags":null,"title":"openconnect双因素认证","uri":"/posts/e96811/"},{"categories":null,"content":"1.首先添加两个带分组的用户 ocpasswd -c /etc/ocserv/ocpasswd -g gruop1 user1 ocpasswd -c /etc/ocserv/ocpasswd -g gruop2 user2 ","date":"2022-09-11","objectID":"/posts/bf9138/:0:1","tags":null,"title":"openconnect设置用户组实现多路由","uri":"/posts/bf9138/"},{"categories":null,"content":"2.添加创建路由表组 mkdir /etc/ocserv/group echo -e \"route = 10.10.0.0/255.255.255.0\" \u003e\u003e /etc/ocserv/group/group1 echo -e \"no-route = 211.80.0.0/255.240.0.0\" \u003e\u003e /etc/ocserv/group/group2 以上连个路由表是演示group1和group2随便写的 请自行添加路由规则 此外路由表里还可以写DNS 短线时间的参数 ","date":"2022-09-11","objectID":"/posts/bf9138/:0:2","tags":null,"title":"openconnect设置用户组实现多路由","uri":"/posts/bf9138/"},{"categories":null,"content":"3.添加新的配置到ocserv.conf config-per-group = /etc/ocserv/group/ default-group-config = /etc/ocserv/group/group1 #如果创建用户的时候不分组 group1就是默认分组 用的就是group1的路由表 default-select-group = group1 #如果创建用户的时候不分组 group1就是默认分组 用的就是group1的路由表 auto-select-group = false ","date":"2022-09-11","objectID":"/posts/bf9138/:0:3","tags":null,"title":"openconnect设置用户组实现多路由","uri":"/posts/bf9138/"},{"categories":null,"content":"4.完整配置 auth = \"plain[/etc/ocserv/ocpasswd]\" tcp-port = 443 udp-port = 443 run-as-user = nobody run-as-group = daemon socket-file = /var/run/ocserv-socket server-cert = /etc/letsencrypt/live/myvpn.alongparty.cn/fullchain.pem server-key = /etc/letsencrypt/live/myvpn.alongparty.cn/privkey.pem isolate-workers = false max-clients = 16 max-same-clients = 2 rate-limit-ms = 100 server-stats-reset-time = 604800 keepalive = 32400 dpd = 90 mobile-dpd = 1800 switch-to-tcp-timeout = 25 try-mtu-discovery = true cert-user-oid = 0.9.2342.19200300.100.1.1 tls-priorities = \"NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1\" auth-timeout = 240 min-reauth-time = 300 max-ban-score = 80 ban-reset-time = 1200 cookie-timeout = 300 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true pid-file = /var/run/ocserv.pid device = vpns predictable-ips = true default-domain = example.com ipv4-network = 10.255.255.0 ipv4-netmask = 255.255.255.0 tunnel-all-dns = true dns = 8.8.8.8 dns = 4.2.2.4 dns = 2001:4860:4860::8888 dns = 2001:4860:4860::8844 ping-leases = false config-per-group = /etc/ocserv/group/ default-group-config = /etc/ocserv/group/default default-select-group = default auto-select-group = false cisco-client-compat = true dtls-legacy = true # cat /etc/ocserv/group/default route = 10.48.16.124/32 # cat /etc/ocserv/ocpasswd sysop01:sysop:$5$R7HQXGtZ1MpB.N82$iNf5viGa/XT/qLjfpFhPNqlvdEyw5KKaKZvK2jIVEG4 ","date":"2022-09-11","objectID":"/posts/bf9138/:0:4","tags":null,"title":"openconnect设置用户组实现多路由","uri":"/posts/bf9138/"},{"categories":null,"content":"I’m kbsonlong, a full-time full-stack freelance SRE 👨‍💻🚀 🔭 I’m currently working on Joyy 🌱 I’m currently learning Vue、Golang and Istio. ❓ ⚡ ","date":"2022-09-11","objectID":"/about/:0:1","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"正在学习 ","date":"2022-09-11","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Service Mesh ","date":"2022-09-11","objectID":"/about/:1:1","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Vue ","date":"2022-09-11","objectID":"/about/:1:2","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Go ","date":"2022-09-11","objectID":"/about/:1:3","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"汇编和工具: 🛠 ","date":"2022-09-11","objectID":"/about/:2:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Frontend ","date":"2022-09-11","objectID":"/about/:2:1","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Backend ","date":"2022-09-11","objectID":"/about/:2:2","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"DevOps ","date":"2022-09-11","objectID":"/about/:2:3","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Github Stats ","date":"2022-09-11","objectID":"/about/:3:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"kbsonlong's friends","date":"2022-09-10","objectID":"/friends/","tags":null,"title":"友链","uri":"/friends/"},{"categories":null,"content":"友链交换 - nickname: 蜷缩的蜗牛 avatar: https://raw.githubusercontent.com/kbsonlong/picgo-imgs/master/images/20220524130401.png url: https://www.alongparty.cn description: 运维自动化 Notice 如果您想交换链接，请以上述格式发表评论。 （仅限个人非商业博客/网站） 网站故障、停止维护和不当内容可能会被取消链接！ 不尊重他人劳动成果、无源转载、恶意行为的网站，请勿前来交流。 ","date":"2022-09-10","objectID":"/friends/:1:0","tags":null,"title":"友链","uri":"/friends/"},{"categories":["云原生"],"content":"本文将为你讲解： Istio 中 sidecar 自动注入过程 Istio 中的 init 容器启动过程 启用了 Sidecar 自动注入的 Pod 的启动流程 下图中展示了 Istio 数据平面中的 Pod 启动完后的组件。 ","date":"2022-09-10","objectID":"/posts/dc5625/:0:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"Istio 中的 sidecar 注入 Istio 中提供了以下两种 sidecar 注入方式： 使用 istioctl 手动注入。 基于 Kubernetes 的 突变 webhook 准入控制器（mutating webhook addmission controller 的自动 sidecar 注入方式。 不论是手动注入还是自动注入，sidecar 的注入过程都需要遵循如下步骤： Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧； 使用下面的命令可以手动注入 sidecar。 istioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - 该命令会使用 Istio 内置的 sidecar 配置来注入，下面使用 Istio详细配置请参考 Istio 官网。 注入完成后您将看到 Istio 为原有 pod template 注入了 initContainer 及 sidecar proxy相关的配置。 ","date":"2022-09-10","objectID":"/posts/dc5625/:1:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"Init 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。 Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。 在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。 关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。 ","date":"2022-09-10","objectID":"/posts/dc5625/:1:1","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"Init 容器解析 Istio 在 pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动命令是： istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i '*' -x \"\" -b '*' -d 15090,15020 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是怎么确定启动时执行的命令。 # 前面的内容省略 # The pilot-agent will bootstrap Envoy. ENTRYPOINT [\"/usr/local/bin/pilot-agent\"] 我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables 命令行，该命令行工具的代码的位置在 Istio 源码仓库的 tools/istio-iptables 目录。 注意：在 Istio 1.1 版本时还是使用 isito-iptables.sh 命令行来操作 IPtables。 ","date":"2022-09-10","objectID":"/posts/dc5625/:2:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"Init 容器启动入口 Init 容器的启动入口是 istio-iptables 命令行，该命令行工具的用法如下： $ istio-iptables [flags] -p: 指定重定向所有 TCP 流量的 sidecar 端口（默认为 $ENVOY_PORT = 15001） -m: 指定入站连接重定向到 sidecar 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 sidecar 中排除的入站端口列表（可选），以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -o：逗号分隔的出站端口列表，不包括重定向到 Envoy 的端口。 -i: 指定重定向到 sidecar 的 IP 地址范围（可选），以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 -k：逗号分隔的虚拟接口列表，其入站流量（来自虚拟机的）将被视为出站流量。 -g：指定不应用重定向的用户的 GID。(默认值与 -u param 相同) -u：指定不应用重定向的用户的 UID。通常情况下，这是代理容器的 UID（默认值是 1337，即 istio-proxy 的 UID）。 -z: 所有进入 pod/VM 的 TCP 流量应被重定向到的端口（默认 $INBOUND_CAPTURE_PORT = 15006）。 以上传入的参数都会重新组装成 iptables 规则，关于该命令的详细用法请访问 tools/istio-iptables/pkg/cmd/root.go。 该容器存在的意义就是让 sidecar 代理可以拦截所有的进出 pod 的流量，15090 端口（Mixer 使用）和 15092 端口（Ingress Gateway）除外的所有入站（inbound）流量重定向到 15006 端口（sidecar），再拦截应用容器的出站（outbound）流量经过 sidecar 处理（通过 15001 端口监听）后再出站。关于 Istio 中端口用途请参考 Istio 官方文档。 命令解析 这条启动命令的作用是： 将应用容器的所有流量都转发到 sidecar 的 15006 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 sidecar 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 sidecar 代理（通过 15001 端口）。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 sidecar 容器中。 ","date":"2022-09-10","objectID":"/posts/dc5625/:2:1","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"Pod 启动流程 启用了 Sidecar 自动注入的 Pod 启动流程如下： Init 容器先启动，向 Pod 中注入 iptables 规则，进行透明流量拦截。 随后，Kubernetes 会根据 Pod Spec 中容器的声明顺序依次启动容器，但这是非阻塞的，无法保证第一个容器启动完成后才启动下一个。istio-proxy 容器启动时，pilot-agent 将作为 PID 1 号进程，它是 Linux 用户空间的第一个进程，负责拉起其他进程和处理僵尸进程。pilot-agent 将生成 Envoy bootstrap 配置并拉起 envoy 进程；应用容器几乎跟 istio-proxy 容器同时启动，为了防止 Pod 内的容器在还没启动好的情况而接收到外界流量，这时候就绪探针就派上用场了。Kubernetes 会在 istio-proxy 容器的 15021 端口进行就绪检查，直到 isito-proxy 启动完成后 kubelet 才会将流量路由到 Pod 内。 在 Pod 启动完成后，pilot-agent 将变为守护进程监视系统其他进程，除此之外，该进程还为 Envoy 提供 Bootstrap 配置、证书、健康检查、配置热加载、身份支持及进程生命周期管理等。 ","date":"2022-09-10","objectID":"/posts/dc5625/:3:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"Pod 内容器启动顺序问题 在 Pod 启动的过程中存在容器启动顺序问题，假设下面这种情况，应用容器先启动，请求其他服务，这时候 istio-proxy 容器还没启动完成，那么该请求将会失败，如果你的应用的健壮性不足，甚至可能导致应用容器崩溃，进而 Pod 重启。对于这种情况的解决方案是： 修改应用程序，增加超时重试。 增加应用容器中进程的启动延迟，比如增加 sleep 时间。 在应用容器中增加一个 postStart 配置，检测应用进程是否启动完成，只有当检测成功时，Kubernetes 才会将 Pod 的状态标记为 Running。 ","date":"2022-09-10","objectID":"/posts/dc5625/:4:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"总结 这篇文章带领大家了解了 Istio 数据平面中的 Pod 启动过程，还有因为 Pod 内容器启动顺序带来的问题。 ","date":"2022-09-10","objectID":"/posts/dc5625/:5:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"参考 阅读原文 istio 常见问题: Sidecar 启动顺序问题 - imroc.cc ","date":"2022-09-10","objectID":"/posts/dc5625/:6:0","tags":["istio","转载"],"title":"Istio 数据平面Pod启动过程详解","uri":"/posts/dc5625/"},{"categories":["云原生"],"content":"iptables 作为 Linux 内核中的重要功能，有着广泛的应用，在 Istio 中默认就是利用 iptables 做透明流量劫持的。理解 iptables，对于我们理解 Istio 的运作有十分重要的作用。本文将为大家简单介绍下 iptbles。 ","date":"2022-09-10","objectID":"/posts/b7e1eb/:0:0","tags":["istio","iptable","转载"],"title":"理解Iptable","uri":"/posts/b7e1eb/"},{"categories":["云原生"],"content":"iptables 简介 iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。 在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。 下图展示了 iptables 调用链。 ","date":"2022-09-10","objectID":"/posts/b7e1eb/:1:0","tags":["istio","iptable","转载"],"title":"理解Iptable","uri":"/posts/b7e1eb/"},{"categories":["云原生"],"content":"iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表： raw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换（例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。 不同的表中的具有的链类型如下表所示： 规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ ","date":"2022-09-10","objectID":"/posts/b7e1eb/:2:0","tags":["istio","iptable","转载"],"title":"理解Iptable","uri":"/posts/b7e1eb/"},{"categories":["云原生"],"content":"理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。 $ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。 每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。 pkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，或者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。 target 支持的类型 target 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念。 ","date":"2022-09-10","objectID":"/posts/b7e1eb/:3:0","tags":["istio","iptable","转载"],"title":"理解Iptable","uri":"/posts/b7e1eb/"},{"categories":["云原生"],"content":"总结 以上就是对 iptables 的简要介绍，你已经了解了 iptables 是怎样运行的，规则链及其执行顺序。 ","date":"2022-09-10","objectID":"/posts/b7e1eb/:4:0","tags":["istio","iptable","转载"],"title":"理解Iptable","uri":"/posts/b7e1eb/"},{"categories":["云原生"],"content":"参考 阅读原文 ","date":"2022-09-10","objectID":"/posts/b7e1eb/:5:0","tags":["istio","iptable","转载"],"title":"理解Iptable","uri":"/posts/b7e1eb/"},{"categories":null,"content":"获取仓库信息 curl --request GET \\ --url \"https://api.github.com/repos/kbsonlong/kubernetes-guide\" \\ --header \"Accept: application/vnd.github.v3+json\" \\ --header \"Authorization: Bearer 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\" ","date":"2022-08-11","objectID":"/posts/5d7029/:0:1","tags":null,"title":"GitHub Api接口","uri":"/posts/5d7029/"},{"categories":null,"content":"删除仓库 ","date":"2022-08-11","objectID":"/posts/5d7029/:0:2","tags":null,"title":"GitHub Api接口","uri":"/posts/5d7029/"},{"categories":null,"content":"Ingress 资源是 Kubernetes 众多成功故事之一。它创建了一个不同的 Ingress 控制器生态系统，这些控制器以标准化和一致的方式在成千上万的集群中使用。这种标准化帮助用户采用 Kubernetes。然而，在 Ingress 创建 5 年后，有迹象表明，分裂为不同但惊人相似的 CRD 和超载的注释。使 Ingress 普及的可移植性同样也限制了它的未来。 在 2019 年圣地亚哥 Kubecon 大会上，一群热情的贡献者聚集在一起讨论 Ingress 的演变。讨论蔓延到了街对面的酒店大厅，结果就是后来被称为 Gateway API 的东西。这一讨论是基于以下几个关键假设： 作为路由匹配、流量管理和服务暴露基础的 API 标准已经商品化，作为自定义 API 对其实现者和用户几乎没有提供什么价值 可以通过共同的核心 API 资源来表示 L4/L7 路由和流量管理 以一种不牺牲核心 API 的用户体验的方式，为更复杂的功能提供可扩展性是可能的 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:0:0","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"引入 Gateway API 这就引出了允许 Gateway API 在 Ingress 基础上改进的设计原则： 表达能力——除了 HTTP 主机/路径匹配和 TLS 之外，Gateway API 还可以表达 HTTP 头操作、流量加权和镜像、TCP/UDP 路由以及其他只能在 Ingress 中通过自定义注释才能实现的功能。 面向角色的设计——API 资源模型反映了在路由和 Kubernetes 服务网络中常见的职责分离。 可扩展性——资源允许在 API 的不同层上附加任意的配置。这使得在最合适的地方可以进行细粒度定制。 灵活的一致性——Gateway API 定义了不同的一致性级别——核心（强制支持）、扩展（如果支持则可移植）和自定义（没有可移植性保证），统称为灵活的一致性[1]。这促进了一个高度可移植的核心 API（如 Ingress），它仍然为网关控制器实现者提供灵活性。 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:1:0","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"Gateway API 是什么样子的？ Gateway API 引入了一些新的资源类型： GatewayClasses 是集群范围的资源，作为模板来显式定义从它们派生的 Gateways 的行为。这在概念上类似于 StorageClasses，但用于联网数据平面。 Gateways 是 GatewayClasses 的部署实例。它们是执行路由的数据平面的逻辑表示，可以是集群内代理、硬件 LB 或云 LB。 路由不是单一的资源，而是代表许多不同协议特定的 Route 资源。HTTPRoute 具有匹配、过滤和路由规则，这些规则应用于能够处理 HTTP 和 HTTPS 流量的 Gateways。类似地，TCPRoutes、UDPRoutes 和 TLSRoutes 也具有特定于协议的语义。此模型还允许 Gateway API 将来增量地扩展其协议支持。 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:1:1","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"网关控制器实现 好消息是，尽管 Gateway 是在Alpha[2]阶段，但已经有几个你可以运行的Gateway 控制器实现[3]。由于它是一个标准化的规范，下面的示例可以在它们中的任何一个上运行，并且应该以完全相同的方式工作。查看入门手册[4]，了解如何安装和使用这些网关控制器之一。 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:1:2","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"Gateway API 例子 在下面的例子中，我们将演示不同 API 资源之间的关系，并带你浏览一个常见的用例： 团队 foo 将他们的应用部署在 foo 命名空间中。他们需要控制应用程序不同页面的路由逻辑。 团队 bar 运行在 bar 命名空间中。他们希望能够对他们的应用进行蓝绿发布以降低风险。 平台团队负责管理 Kubernetes 集群中所有应用的负载均衡器和网络安全。 下面的 foo-route 对 foo 命名空间中的各种服务进行路径匹配，并且还有一个到 404 服务器的默认路由。这将分别通过 foo.example.com/login 和 foo.example.com/home 暴露 foo-auth 和 foo-home 服务： kind: HTTPRoute apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: foo-route namespace: foo labels: gateway: external-https-prod spec: hostnames: - \"foo.example.com\" rules: - matches: - path: type: Prefix value: /login forwardTo: - serviceName: foo-auth port: 8080 - matches: - path: type: Prefix value: /home forwardTo: - serviceName: foo-home port: 8080 - matches: - path: type: Prefix value: / forwardTo: - serviceName: foo-404 port: 8080 bar 团队在同一个 Kubernetes 集群的 bar 命名空间中操作，也希望将他们的应用程序暴露给互联网，但他们也希望控制自己的灰度和蓝绿发布。下面的 HTTPRoute 被配置为以下行为： bar.example.com 的流量： 将 90%的流量发送到 bar-v1 将 10%的流量发送给 bar-v2 对于使用 HTTP 头 env:canary 到 bar.example.com 的流量： 将所有流量发送到 bar-v2 kind: HTTPRoute apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: bar-route namespace: bar labels: gateway: external-https-prod spec: hostnames: - \"bar.example.com\" rules: - forwardTo: - serviceName: bar-v1 port: 8080 weight: 90 - serviceName: bar-v2 port: 8080 weight: 10 - matches: - headers: values: env: canary forwardTo: - serviceName: bar-v2 port: 8080 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:2:0","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"路由和网关绑定 因此，我们有两个匹配的 HTTPRoutes，并将流量路由到不同的服务。你可能想知道，在哪里可以访问这些服务？它们通过哪些网络或 IP 暴露？ 路由如何向客户端暴露由路由绑定[5]来管理，该绑定描述了路由和网关之间如何创建双向关系。当 Routes 被绑定到一个 Gateway 时，这意味着它们的集合路由规则被配置在底层的负载均衡器或代理上，并且路由可以通过网关访问。因此，网关是可以通过路由配置的网络数据平面的逻辑表示。 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:2:1","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"行政委托 Gateway 和 Route 资源之间的分离允许集群管理员将一些路由配置委派给各个团队，同时仍然保持集中控制。以下网关资源在端口 443 上暴露 HTTPS，并使用由集群管理员控制的证书终止端口上的所有通信流。 kind: Gateway apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: prod-web spec: gatewayClassName: acme-lb listeners: - protocol: HTTPS port: 443 routes: kind: HTTPRoute selector: matchLabels: gateway: external-https-prod namespaces: from: All tls: certificateRef: name: admin-controlled-cert 下面的 HTTPRoute 展示了 Route 如何通过它的 kind（HTTPRoute）和资源标签（gateway=external-https-prod）来确保它匹配网关的选择器。 # Matches the required kind selector on the Gateway kind: HTTPRoute apiVersion: networking.x-k8s.io/v1alpha1 metadata: name: foo-route namespace: foo-ns labels: # Matches the required label selector on the Gateway gateway: external-https-prod ... ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:2:2","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"面向角色的设计 当你将它们放在一起时，你就拥有了一个可以被多个团队安全地共享的负载平衡基础设施。Gateway API 不仅是用于高级路由的更具表现力的 API，而且是面向角色的 API，专为多租户基础设施设计。它的可扩展性确保了它将在保持可移植性的同时为未来的用例发展。最终，这些特性将允许 Gateway API 适应不同的组织模型和实现，直到未来。 ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:2:3","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"尝试一下，并参与其中 有许多资源可以查看以了解更多。 查看入门手册，看看可以解决哪些用例。 尝试使用现有的网关控制器之一 或者参与[6]并帮助设计和影响 Kubernetes 服务网络的未来！ ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:2:4","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":null,"content":"参考资料 [1] 灵活的一致性: https://gateway-api.sigs.k8s.io/concepts/guidelines/#conformance [2] Alpha: https://github.com/kubernetes-sigs/gateway-api/releases [3] Gateway 控制器实现: https://gateway-api.sigs.k8s.io/references/implementations/ [4] 入门手册: https://gateway-api.sigs.k8s.io/guides/getting-started/ [5] 路由绑定: https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-binding [6] 参与: https://gateway-api.sigs.k8s.io/contributing/community/ ","date":"2022-07-29","objectID":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/:2:5","tags":null,"title":"通过Gateway API不断演变的Kubernetes网络","uri":"/posts/%E9%80%9A%E8%BF%87gateway-api%E4%B8%8D%E6%96%AD%E6%BC%94%E5%8F%98%E7%9A%84kubernetes%E7%BD%91%E7%BB%9C/"},{"categories":["云原生"],"content":"安装kubebuilder brew install kubebuilder kubebuilder version ","date":"2022-07-21","objectID":"/posts/da6323/:0:1","tags":["kubebuilder","operator"],"title":"kubebuilder开发operator","uri":"/posts/da6323/"},{"categories":["云原生"],"content":"创建项目目录 mkdir custom-controllers cd custom-controllers go mod init controllers.alongparty.cn ","date":"2022-07-21","objectID":"/posts/da6323/:0:2","tags":["kubebuilder","operator"],"title":"kubebuilder开发operator","uri":"/posts/da6323/"},{"categories":["云原生"],"content":"kubebuilder初始化项目 kubebuilder init --domain controller.alongparty.cn --license apache2 --owner \"kbsonlong\" kubebuilder create api --group controller --version v1 --kind Application make ","date":"2022-07-21","objectID":"/posts/da6323/:0:3","tags":["kubebuilder","operator"],"title":"kubebuilder开发operator","uri":"/posts/da6323/"},{"categories":["云原生"],"content":"参考资料 使用kubebuilder开发operator详解 KubeBuilder 简明教程 ","date":"2022-07-21","objectID":"/posts/da6323/:0:4","tags":["kubebuilder","operator"],"title":"kubebuilder开发operator","uri":"/posts/da6323/"},{"categories":null,"content":"安装ocserv apt-get install ocserv -y ","date":"2022-07-21","objectID":"/posts/90ef46/:0:1","tags":null,"title":"ocserv部署","uri":"/posts/90ef46/"},{"categories":null,"content":"申请免费证书 修改dns指向服务器 生成证书 certbot certonly --standalone --preferred-challenges http --agree-tos --email kbsonlong@gmail.com -d myvpn.alongparty.cn # 续签证书 certbot renew --quiet --no-self-upgrade ","date":"2022-07-21","objectID":"/posts/90ef46/:0:2","tags":null,"title":"ocserv部署","uri":"/posts/90ef46/"},{"categories":null,"content":"修改配置 auth = \"plain[/etc/ocserv/ocpasswd]\" tcp-port = 443 udp-port = 443 run-as-user = nobody run-as-group = daemon socket-file = /var/run/ocserv-socket server-cert = /etc/letsencrypt/live/myvpn.alongparty.cn/fullchain.pem server-key = /etc/letsencrypt/live/myvpn.alongparty.cn/privkey.pem isolate-workers = false max-clients = 16 max-same-clients = 2 rate-limit-ms = 100 server-stats-reset-time = 604800 keepalive = 32400 dpd = 90 mobile-dpd = 1800 switch-to-tcp-timeout = 25 try-mtu-discovery = true cert-user-oid = 0.9.2342.19200300.100.1.1 tls-priorities = \"NORMAL:%SERVER_PRECEDENCE:%COMPAT:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1\" auth-timeout = 240 min-reauth-time = 300 max-ban-score = 80 ban-reset-time = 1200 cookie-timeout = 300 deny-roaming = false rekey-time = 172800 rekey-method = ssl use-occtl = true pid-file = /var/run/ocserv.pid device = vpns predictable-ips = true default-domain = example.com ipv4-network = 10.255.255.0 ipv4-netmask = 255.255.255.0 tunnel-all-dns = true dns = 8.8.8.8 dns = 4.2.2.4 dns = 2001:4860:4860::8888 dns = 2001:4860:4860::8844 ping-leases = false cisco-client-compat = true dtls-legacy = true ","date":"2022-07-21","objectID":"/posts/90ef46/:0:3","tags":null,"title":"ocserv部署","uri":"/posts/90ef46/"},{"categories":["云原生"],"content":"1限流 ","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:0:0","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.1什么是限流 ​ 举个例子，比如我们有个桶，桶里有两个开关，一个往外出水，一个网内注水，当出水的速度慢于注水的速度时，到一定时间水就会从桶里溢出。如果我们限制注水速率，就可以防止水从桶里溢出，这就是限流。 ​ 具体到软件层面，我们把请求速率看做是注水，把系统cpu，内存等资源看做是放水，当请求速率过快，消耗太多资源时系统就可能崩溃。软件限流就是限制tps或qps指标，以达到保护系统的目的，虽然可能部分用户无法服务，但是系统整体还是健康的，还可以对外部提供服务，不是整体挂掉。 ","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:1:0","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2限流算法 1.2.1漏桶算法 就像一个漏斗以下，下面小，上面大。漏桶流出的速率被限制在比较小的范围，当漏桶满时，漏桶就会溢出，进来的请求就会被抛弃掉。特别是应对突发流量，漏桶的速率是恒定的，这样可以有效防止应突发流量导致系统崩溃。 ","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:2:0","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2.2令牌桶算法 令牌桶算法的原理，关键在令牌，它是指往桶里以一个不变的速率放入令牌，当有请求时，如果桶里有令牌，请求就消费一个令牌，请求继续进行；当请求到来，桶里没有令牌时，请求就会被抛弃掉，拒绝服务；当桶里的令牌满时，令牌就会被抛弃掉。 ","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:2:1","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2.3计数器算法 计数器算法是指一段时间设置一个计数器，当有请求时计数器就加一，请求继续进行；在技术器时间范围内，当计数器数值超过指定值，请求就被拒绝；当时间范围结束，就重置计数器。技术器算法有个缺陷，就是如果计数器时间是1分钟，当前1秒来了大量请求，讲技术器用完了，后续59秒时间就没法提供服务。 ","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:2:2","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2实操 ","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:3:0","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["云原生"],"content":"1.2.1 http 1.2.1.1单集群 istio部署和bookinfo实例部署大家自行完成，都看这种深度的文章了这个应该不是事。 1.2.1.1.1集群内服务限流 1.2.1.1.1.1本地限流 cat \u003c\u003cEOF \u003e envoyfilter-local-rate-limit.yaml apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-local-ratelimit-svc spec: workloadSelector: labels: app: productpage configPatches: - applyTo: HTTP_FILTER match: listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: INSERT_BEFORE value: name: envoy.filters.http.local_ratelimit typed_config: \"@type\": type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit value: stat_prefix: http_local_rate_limiter token_bucket: max_tokens: 10 tokens_per_fill: 10 fill_interval: 60s filter_enabled: runtime_key: local_rate_limit_enabled default_value: numerator: 100 denominator: HUNDRED filter_enforced: runtime_key: local_rate_limit_enforced default_value: numerator: 100 denominator: HUNDRED response_headers_to_add: - append: false header: key: x-local-rate-limit value: 'true' EOF kubectl apply -f envoyfilter-local-rate-limit.yaml -n istio 说明：本地限流需要通过EnvoyFilter来实现，他不会请求外部服务，在envoy内部实现支持，是一个令牌桶的算法。http filter的名称必须是envoy.filters.http.local_ratelimit，type和typeurl是固定的，stat_prefix可以随便改，表示生成stat的指标前缀。token_bucket配置令牌桶，max_tokens表示最大令牌数量，tokens_per_fill表示每次填充的令牌数量，fill_interval表示填充令牌的间隔。filter_enabled表示启用但不是强制，filter_enforced表示强制，可以配置百分比。response_headers_to_add修改响应头信息，append为false表示修改，true表示添加。runtime_key 运行时的key，具体有啥用不清楚。 执行压测： [root@node01 45]# go-stress-testing -c 10 -n 10000 -u http://192.168.229.134:30945/productpage 开始启动 并发数:10 请求数:10000 请求参数: request: form:http url:http://192.168.229.134:30945/productpage method:GET headers:map[] data: verify:statusCode timeout:30s debug:false ─────┬───────┬───────┬───────┬────────┬────────┬────────┬────────┬────────┬────────┬──────── 耗时│ 并发数│ 成功数│ 失败数│ qps │最长耗时│最短耗时│平均耗时│下载字节│字节每秒│ 错误码 ─────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┼──────── 1s│ 7│ 2│ 761│ 2.94│ 124.68│ 1.98│ 3406.97│ 21,476│ 21,470│200:2;429:761 2s│ 10│ 5│ 1636│ 2.55│ 1788.46│ 1.98│ 3928.11│ 52,771│ 26,383│200:5;429:1636 3s│ 10│ 5│ 2962│ 1.70│ 1788.46│ 1.04│ 5871.68│ 76,639│ 25,545│200:5;429:2962 4s│ 10│ 5│ 4459│ 1.28│ 1788.46│ 1.04│ 7810.78│ 103,585│ 25,896│200:5;429:4459 429 Too Many Requests (太多请求) 当你需要限制客户端请求某个服务的数量，也就是限制请求速度时，该状态码就会非常有用 清理： kubectl delete envoyfilter filter-local-ratelimit-svc -n istio 1.2.1.1.1.2全局限流 部署ratelimit 1创建cm cat \u003c\u003c EOF \u003e ratelimit-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: ratelimit-config data: config.yaml: | domain: productpage-ratelimit descriptors: - key: PATH value: \"/productpage\" rate_limit: unit: minute requests_per_unit: 1 - key: PATH rate_limit: unit: minute requests_per_unit: 100 EOF kubectl apply -f ratelimit-config.yaml -n istio 说明: 这个configmap是限速服务用到的配置文件，他是envoy v3版本的限速格式。domain是域名，他会在envoyfilter中被引用，descriptors的PATH,表示请求的路径可以有多个值，rate_limit配置限速配额，这里productpage配了1分钟1个请求，其他url是1分钟100个请求 2创建限速服务deployment cat \u003c\u003c EOF \u003e ratelimit-deploy.yaml apiVersion: v1 kind: Service metadata: name: redis labels: app: redis spec: ports: - name: redis port: 6379 selector: app: redis --- apiVersion: apps/v1 kind: Deployment metadata: name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: redis:alpine imagePullPolicy: Always name: redis ports: - name: redis containerPort: 6379 restartPolicy: Always serviceAccountName: \"\" --- apiVersion: v1 kind: Service metadata: name: ratelimit labels: app: ratelimit spec: ports: - name: http-port port: 8080 targetPort: 8080 protocol: TCP - name: grpc-port port: 8081 targetPort: 8081 protocol: TCP - name: http-debug port: 6070 targetPort: 6070 protocol: TCP selector: app: ratelimit --- apiVersion: apps/v1 kind: Deployment metadata: name: ratelimit spec: replicas: 1 selector: matchLabels: app: ratelimit strategy: type: Recreate template: metadata: labe","date":"2022-05-23","objectID":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/:3:1","tags":["istio","转载"],"title":"Istio防故障利器","uri":"/posts/istio%E9%98%B2%E6%95%85%E9%9A%9C%E5%88%A9%E5%99%A8/"},{"categories":["hugo"],"content":"创建Github Actions流水线 mkdir -p .github/workflows/ touch hugo.yaml 发布到本仓库 name: GitHub Pages Deploy Local REPOSITORY on: push: branches: - master # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 1 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.91.2' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 发布到另外仓库 name: CI #自动化的名称 on: push: # push的时候触发 branches: # 那些分支需要触发 - master jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 1 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.EXTERNAL_REPOSITORY_TOKEN }} publish_dir: ./public external_repository: kbsonlong/devops.alongparty.cn 注意: 发布到本仓库github_token不需要手动创建GITHUB_TOKEN, 发布到其他仓库personal_token需要提前创建并配置secret变量 创建SSH证书 ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\" 配置SSH公钥 配置SSH私钥 使用SSH私钥发布 name: CI #自动化的名称 on: push: # push的时候触发 branches: # 那些分支需要触发 - master jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 1 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build run: hugo --minify - name: Deploy with PRIVATE KEY uses: peaceiris/actions-gh-pages@v3 with: cname: devops.alongparty.cn env: ACTIONS_DEPLOY_KEY: ${{ secrets.HUGO_DEPLOY_PRIVATE_KEY }} EXTERNAL_REPOSITORY: kbsonlong/devops.alongparty.cn PUBLISH_BRANCH: gh-pages PUBLISH_DIR: ./public 注意: 部署到另外仓库时ssh私钥配置在源仓库, ssh公钥配置在目标仓库或者全局 ","date":"2022-05-23","objectID":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:1","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"创建GITHUB Personal TOKEN ","date":"2022-05-23","objectID":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:2","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"创建仓库Secrets变量 ","date":"2022-05-23","objectID":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:3","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"Github Page配置自定义域名 echo \"\u003c自定义域名\u003e\" \u003estatic/CNAME ","date":"2022-05-23","objectID":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:4","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["hugo"],"content":"Deploy时过滤静态文件 - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.EXTERNAL_REPOSITORY_TOKEN }} external_repository: kbsonlong/devops.alongparty.cn publish_branch: gh-pages # default: gh-pages publish_dir: ./public exclude_assets: './algolia.json,./*/*.md' # 支持正则过滤,基于编译后的静态文件,根目录publish_dir user_name: 'github-actions[bot]' user_email: 'github-actions[bot]@users.noreply.github.com' commit_message: ${{ github.event.head_commit.message }} tag_name: ${{ steps.prepare_tag.outputs.deploy_tag_name }} tag_message: 'Deployment ${{ steps.prepare_tag.outputs.tag_name }}' ","date":"2022-05-23","objectID":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/:0:5","tags":["github actions"],"title":"Github Actions自动部署hugo博客","uri":"/posts/github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/"},{"categories":["《Git》学习笔记"],"content":"rebase分支合并 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:0:0","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"说明 以下 v2 是某个需求的开发分支， dev是总的开发分支，v2 是基于dev分支签出的。 当完成v2的开发后，需要把代码合并到dev，我们可以使用rebase进行合并： # 首先将 v2 push到远程仓库 git add . git commit -m 'xxx' git push origin v2 # 切换到 dev 拉取最新代码 git checkout dev git pull origin dev # 切换到 v2 git checkout v2 git rebase dev # 将 v2 的所有[commit] 变基到(应用到) dev # 切换到 dev git checkout dev git merge v2 # 将 dev分支 快进合并 （此时 (HEAD -\u003e dev, v2) [commit] 两个分支指向同一个提交） # 查看 原v2的[commit]记录 是否在dev分支的最前面（变基成功会把v2的提交记录应用到dev分支的最前面） git log # 如果到这一步发现有问题，尝试使用 git --abort中止变基，如果还是有问题的可以在dev分支上使用《后悔药》操作， 再到v2分支上使用《后悔药》操作，即可使两个分支都回退到 rebase变基 之前的状态 # 试运行项目是否有问题 yarn start git status # 查看状态是否有问题 git push origin dev # 推送到远程仓库的 dev ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:1:0","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"变基要遵守的准则 几个人同时在一个分支上进行开发和提交时，你不要中途执行变基，只有在大家都完成工作之后才可以执行变基。 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:1:1","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"变基的实质 变基操作的实质是丢弃一些现有的提交，然后相应地新建一些内容一样但实际上不同的提交。 因此，变基操作过后的分支将不要再使用。 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:1:2","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"后悔药 # 查看HEAD指针变动记录 git reflog # 记录示例(当前分支是v2): 07c398f (HEAD -\u003e v2, master) HEAD@{0}: checkout: moving from master to v2 07c398f (HEAD -\u003e v2, master) HEAD@{1}: rebase (finish): returning to refs/heads/master 07c398f (HEAD -\u003e v2, master) HEAD@{2}: rebase (start): checkout v2 15a97d8 HEAD@{3}: reset: moving to 15a97d8 07c398f (HEAD -\u003e v2, master) HEAD@{4}: merge v2: Fast-forward 15a97d8 HEAD@{5}: checkout: moving from v2 to master 07c398f (HEAD -\u003e v2, master) HEAD@{6}: rebase (finish): returning to refs/heads/v2 07c398f (HEAD -\u003e v2, master) HEAD@{7}: rebase (pick): C 15a97d8 HEAD@{8}: rebase (start): checkout master # 首次rebase d278ecd HEAD@{9}: checkout: moving from master to v2 # rebase前的状态 15a97d8 HEAD@{10}: commit: D # 可见，示例中最初的 rebase 操作是 HEAD@{8}，想回退到变基前的状态需让指针指向 HEAD@{9} git reset --hard d278ecd # 重置当前分支的HEAD为指定[commit]，同时重置暂存区和工作区，与指定[commit]一致 # 此时打印 log 查看是否回到之前的状态 git log 注意：此操作只能回退当前的分支，如其他分支也要回退，需要切换到该分支并执行上面操作。 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:2:0","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"开发期间的rebase操作 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:3:0","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"背景 有两个分支： dev *v2 2.4-dev 是基于dev切出来的。 提交记录如下： dev a - b - c v2 开发期间，两个分支同时有新的commit ： dev a - b - c - d - e \\ - f - g v2 当前你正在v2进行开发，dev也同时进行开发，并有重大的改变，你需要把dev的提交同步到v2。 需求： 把dev中新的提交同步到v2，且不能影响dev分支。 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:3:1","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"操作步骤 基于最新的 dev 切一个新的分支 dev-copy dev-copy 和 dev 两者的 commit ID 一致。 在dev-copy中执行rebase，将 dev-copy 的提交变基到 v2 git rebase v2 # 将 dev-copy 的提交[commit] 变基到(应用到) v2 删除原v2分支，将dev-copy分支名改为v2 # 当前在 dev-copy 分支 git branch -d v2 # 删除分支 git branch -m dev-copy v2 # 重命名 ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:3:2","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"git cherry-pick 来源：《git cherry-pick 教程》 用于将单个或几个[commit]复制到另一个分支。 基本应用 git cherry-pick \u003ccommitHash\u003e # 将commitHash应用于当前分支 上面命令就会将指定的提交commitHash，应用于当前分支。这会在当前分支产生一个新的提交，当然它们的哈希值会不一样。 git cherry-pick命令的参数，不一定是提交的哈希值，分支名也是可以的，表示转移该分支的最新提交。 转移多个提交 Cherry pick 支持一次转移多个提交。 git cherry-pick \u003cHashA\u003e \u003cHashB\u003e # A和B提交 上面的命令将 A 和 B 两个提交应用到当前分支。这会在当前分支生成两个对应的新提交。 如果想要转移一系列的连续提交，可以使用下面的简便语法。 git cherry-pick A..B # A到B提交，不包含A 上面的命令可以转移从 A 到 B 的所有提交。它们必须按照正确的顺序放置：提交 A 必须早于提交 B，否则命令将失败，但不会报错。 注意，使用上面的命令，提交 A 将不会包含在 Cherry pick 中。如果要包含提交 A，可以使用下面的语法。 git cherry-pick A^..B # A到B提交，包含A ","date":"2020-11-18","objectID":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/:3:3","tags":["Git"],"title":"Git变基合并","uri":"/posts/git%E5%8F%98%E5%9F%BA%E5%90%88%E5%B9%B6/"},{"categories":["《Git》学习笔记"],"content":"Git分支-变基 在 Git 中整合来自不同分支的修改主要有两种方法：merge 以及 rebase。 在本节中我们将学习什么是“变基”，怎样使用“变基”，并将展示该操作的惊艳之处，以及指出在何种情况下你应避免使用它。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:0:0","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"变基的基本操作 请回顾之前在 分支的合并 中的一个例子，你会看到开发任务分叉到两个不同分支，又各自提交了更新。 图0. 分叉的提交历史 ▲ 之前介绍过，整合分支最容易的方法是 merge 命令。 它会把两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交）。 图1. 通过合并操作来整合分叉的历史 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:1:0","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"概念 变基就是：将某一分支上的所有修改复制到另一分支上 除了merge，还有一种方法：你可以提取在 C4 中引入的补丁和修改，然后在 C3 的基础上应用一次。 在 Git 中，这种操作就叫做 变基（rebase）。 你可以使用 rebase 命令将提交到某一分支上的所有修改都移到另一分支上，就好像“重新播放”一样。 在这个例子中，你可以检出 experiment 分支，然后将它变基到 master 分支上： $ git checkout experiment $ git rebase master # 将experiment上的修改变基到master分支上（将experiment的提交移动到master上。） First, rewinding head to replay your work on top of it... Applying: added staged command 它的原理是首先找到这两个分支（即当前分支 experiment、变基操作的目标基底分支 master） 的最近共同祖先 C2，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件， 然后将当前分支指向目标基底 C3, 最后以此将之前另存为临时文件的修改依序应用。 （译注：写明了 commit id，以便理解，下同） ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:1:1","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"原理 找到当前分支和目标分支的最近共同祖先 对比当前分支相对于该共同祖先的历次提交 提取相应的修改并存为临时文件 将当前分支指向目标分支 将之前临时文件的修改依序应用 图2.将 C4 中的修改变基到 C3 上 ▲ 现在回到 master 分支，进行一次快进合并。 $ git checkout master $ git merge experiment 图3.master 分支的快进合并 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:1:2","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"步骤 先检出源分支，将源分支的修改变基到目标分支。切回目标分支，进行一次快进合并 # 示意： git checkout \u003c源分支\u003e git (源分支的修改)rebase(到) \u003c目标分支\u003e git checkout \u003c目标分支\u003e git merge \u003c源分支\u003e 此时，C4' 指向的快照就和 the merge example 中 C5 指向的快照一模一样了。 这两种整合方法的最终结果没有任何区别，但是 变基使得提交历史更加整洁。 你在查看一个经过变基的分支的历史记录时会发现，尽管实际的开发工作是并行的， 但它们看上去就像是串行的一样，提交历史是一条直线没有分叉。 一般我们这样做的目的是为了确保在向远程分支推送时能保持提交历史的整洁——例如向某个其他人维护的项目贡献代码时。 在这种情况下，你首先在自己的分支里进行开发，当开发完成时你需要先将你的代码变基到 origin/master 上，然后再向主项目提交修改。 这样的话，该项目的维护者就不再需要进行整合工作，只需要快进合并便可。 请注意，无论是通过变基，还是通过三方合并，整合的最终结果所指向的快照始终是一样的，只不过提交历史不同罢了。 变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:1:3","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"优点 变基的优点： 使提交记录更加整洁。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:1:4","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"更有趣的变基例子 在对两个分支进行变基时，所生成的“重放”并不一定要在目标分支上应用，你也可以指定另外的一个分支进行应用。 就像 从一个主题分支里再分出一个主题分支的提交历史 中的例子那样。 你创建了一个主题分支 server，为服务端添加了一些功能，提交了 C3 和 C4。 然后从 C3 上创建了主题分支 client，为客户端添加了一些功能，提交了 C8 和 C9。 最后，你回到 server 分支，又提交了 C10。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:2:0","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"更有趣的变基例子 在对两个分支进行变基时，所生成的“重放”并不一定要在目标分支上应用，你也可以指定另外的一个分支进行应用。 就像 从一个主题分支里再分出一个主题分支的提交历史 中的例子那样。 你创建了一个主题分支 server，为服务端添加了一些功能，提交了 C3 和 C4。 然后从 C3 上创建了主题分支 client，为客户端添加了一些功能，提交了 C8 和 C9。 最后，你回到 server 分支，又提交了 C10。 图4.从一个主题分支里再分出一个主题分支的提交历史 ▲ 假设你希望将 client 中的修改合并到主分支并发布，但暂时并不想合并 server 中的修改， 因为它们还需要经过更全面的测试。这时，你就可以使用 git rebase 命令的 --onto 选项， 选中在 client 分支里但不在 server 分支里的修改（即 C8 和 C9），将它们在 master 分支上重放： $ git rebase --onto master server client 以上命令的意思是：“取出 client 分支，找出它从 server 分支分歧之后的补丁， 然后把这些补丁在 master 分支上重放一遍，让 client 看起来像直接基于 master 修改一样”。这理解起来有一点复杂，不过效果非常酷。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:2:1","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"–onto选项 选中C分支中的但不在B分支里的修改，应用到A分支。 图5.截取主题分支上的另一个主题分支，然后变基到其他分支 ▲ 现在可以快进合并 master 分支了。（如图 快进合并 master 分支，使之包含来自 client 分支的修改）： $ git checkout master $ git merge client 图6.快进合并 `master` 分支，使之包含来自 `client` 分支的修改 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:2:2","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"省去先切换到源分支的步骤 git rebase \u003c目标(当前)分支\u003e \u003c源分支\u003e # 将源分支变基到目标分支。执行此命令后会自动切换到源分支 git checkout \u003c目标分支\u003e git merge \u003c源分支\u003e 注意：使用这个方法要确保源分支上的代码是最新的。 接下来你决定将 server 分支中的修改也整合进来。 使用 git rebase \u003cbasebranch\u003e \u003ctopicbranch\u003e 命令可以直接将主题分支 （即本例中的 server）变基到目标分支（即 master）上。 这样做能省去你先切换到 server 分支，再对其执行变基命令的多个步骤。 $ git rebase master server 如图 将 server 中的修改变基到 master 上 所示，server 中的代码被“续”到了 master 后面。 图7.将 `server` 中的修改变基到 `master` 上 ▲ 然后就可以快进合并主分支 master 了： $ git checkout master $ git merge server 至此，client 和 server 分支中的修改都已经整合到主分支里了， 你可以删除这两个分支，最终提交历史会变成图 最终的提交历史 中的样子： $ git branch -d client $ git branch -d server 图8. 最终的提交历史 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:2:3","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"变基的风险 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:3:0","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"金科玉律 呃，奇妙的变基也并非完美无缺，要用它得遵守一条准则： 如果提交存在于你的仓库之外，而别人可能基于这些提交进行开发，那么不要执行变基。 如果你遵循这条金科玉律，就不会出差错。 否则，人民群众会仇恨你，你的朋友和家人也会嘲笑你，唾弃你。 ::: tip 例如：几个人同时在一个主题分支上进行开发和提交时，你不要中途执行变基，只有在大家都完成工作之后才可以执行变基。 ::: ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:3:1","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"变基的实质 变基操作的实质是丢弃一些现有的提交，然后相应地新建一些内容一样但实际上不同的提交。 如果你已经将提交推送至某个仓库，而其他人也已经从该仓库拉取提交并进行了后续工作，此时，如果你用 git rebase 命令重新整理了提交并再次推送，你的同伴因此将不得不再次将他们手头的工作与你的提交进行整合，如果接下来你还要拉取并整合他们修改过的提交，事情就会变得一团糟。 让我们来看一个在公开的仓库上执行变基操作所带来的问题。 假设你从一个中央服务器克隆然后在它的基础上进行了一些开发。 你的提交历史如图所示： 图9. 克隆一个仓库，然后在它的基础上进行了一些开发 ▲ 然后，某人又向中央服务器提交了一些修改，其中还包括一次合并。 你抓取了这些在远程分支上的修改，并将其合并到你本地的开发分支，然后你的提交历史就会变成这样： 图10. 抓取别人的提交，合并到自己的开发分支 ▲ 接下来，这个人又决定把合并操作回滚，改用变基；继而又用 git push --force 命令覆盖了服务器上的提交历史。 之后你从服务器抓取更新，会发现多出来一些新的提交。 图11. 有人推送了经过变基的提交，并丢弃了你的本地开发所基于的一些提交 ▲ 结果就是你们两人的处境都十分尴尬。 如果你执行 git pull 命令，你将合并来自两条提交历史的内容，生成一个新的合并提交，最终仓库会如图所示： 图12. 你将相同的内容又合并了一次，生成了一个新的提交 ▲ 此时如果你执行 git log 命令，你会发现有两个提交的作者、日期、日志居然是一样的，这会令人感到混乱。 此外，如果你将这一堆又推送到服务器上，你实际上是将那些已经被变基抛弃的提交又找了回来，这会令人感到更加混乱。 很明显对方并不想在提交历史中看到 C4 和 C6，因为之前就是他把这两个提交通过变基丢弃的。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:3:2","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"用变基解决变基 如果你 真的 遭遇了类似的处境，Git 还有一些高级魔法可以帮到你。 如果团队中的某人强制推送并覆盖了一些你所基于的提交，你需要做的就是检查你做了哪些修改，以及他们覆盖了哪些修改。 实际上，Git 除了对整个提交计算 SHA-1 校验和以外，也对本次提交所引入的修改计算了校验和——即 “patch-id”。 如果你拉取被覆盖过的更新并将你手头的工作基于此进行变基的话，一般情况下 Git 都能成功分辨出哪些是你的修改，并把它们应用到新分支上。 举个例子，如果遇到前面提到的 有人推送了经过变基的提交，并丢弃了你的本地开发所基于的一些提交 那种情境，如果我们不是执行合并，而是执行 git rebase teamone/master, Git 将会： 检查哪些提交是我们的分支上独有的（C2，C3，C4，C6，C7） 检查其中哪些提交不是合并操作的结果（C2，C3，C4） 检查哪些提交在对方覆盖更新时并没有被纳入目标分支（只有 C2 和 C3，因为 C4 其实就是 C4’） 把查到的这些提交应用在 teamone/master 上面 从而我们将得到与 你将相同的内容又合并了一次，生成了一个新的提交 中不同的结果，如图 在一个被变基然后强制推送的分支上再次执行变基 所示。 图13. 在一个被变基然后强制推送的分支上再次执行变基 ▲ 要想上述方案有效，还需要对方在变基时确保 C4' 和 C4 是几乎一样的。 否则变基操作将无法识别，并新建另一个类似 C4 的补丁（而这个补丁很可能无法整洁的整合入历史，因为补丁中的修改已经存在于某个地方了）。 在本例中另一种简单的方法是使用 git pull --rebase 命令而不是直接 git pull。 又或者你可以自己手动完成这个过程，先 git fetch，再 git rebase teamone/master。 如果你习惯使用 git pull ，同时又希望默认使用选项 --rebase，你可以执行这条语句 git config --global pull.rebase true 来更改 pull.rebase 的默认配置。 如果你只对不会离开你电脑的提交执行变基，那就不会有事。 如果你对已经推送过的提交执行变基，但别人没有基于它的提交，那么也不会有事。 如果你对已经推送至共用仓库的提交上执行变基命令，并因此丢失了一些别人的开发所基于的提交， 那你就有大麻烦了，你的同事也会因此鄙视你。 如果你或你的同事在某些情形下决意要这么做，请一定要通知每个人执行 git pull --rebase 命令，这样尽管不能避免伤痛，但能有所缓解。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:4:0","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"变基 vs. 合并 至此，你已在实战中学习了变基和合并的用法，你一定会想问，到底哪种方式更好。 在回答这个问题之前，让我们退后一步，想讨论一下提交历史到底意味着什么。 有一种观点认为，仓库的提交历史即是 记录实际发生过什么。 它是针对历史的文档，本身就有价值，不能乱改。 从这个角度看来，改变提交历史是一种亵渎，你使用 谎言 掩盖了实际发生过的事情。 如果由合并产生的提交历史是一团糟怎么办？ 既然事实就是如此，那么这些痕迹就应该被保留下来，让后人能够查阅。 另一种观点则正好相反，他们认为提交历史是 项目过程中发生的事。 没人会出版一本书的第一版草稿，软件维护手册也是需要反复修订才能方便使用。 持这一观点的人会使用 rebase 及 filter-branch 等工具来编写故事，怎么方便后来的读者就怎么写。 现在，让我们回到之前的问题上来，到底合并还是变基好？希望你能明白，这并没有一个简单的答案。 Git 是一个非常强大的工具，它允许你对提交历史做许多事情，但每个团队、每个项目对此的需求并不相同。 既然你已经分别学习了两者的用法，相信你能够根据实际情况作出明智的选择。 总的原则是，只对尚未推送或分享给别人的本地修改执行变基操作清理历史， 从不对已推送至别处的提交执行变基操作，这样，你才能享受到两种方式带来的便利。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/:5:0","tags":["Git"],"title":"Git分支-变基","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA/"},{"categories":["《Git》学习笔记"],"content":"Git分支-分支原理 Git 处理分支的方式可谓是难以置信的轻量，创建新分支这一操作几乎能在瞬间完成，并且在不同分支之间的切换操作也是一样便捷。 与许多其它版本控制系统不同，Git 鼓励在工作流程中频繁地使用分支与合并，哪怕一天之内进行许多次。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:0","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"首次提交 在进行提交操作时，Git 会保存一个提交对象（commit object）。 假设现在有一个工作目录，里面包含了三个将要被暂存和提交的文件。 暂存操作会为每一个文件计算校验和（使用 SHA-1 哈希算法），然后会把当前版本的文件快照保存到 Git 仓库中 （Git 使用 blob 对象来保存它们），最终将校验和加入到暂存区域等待提交： $ git add README test.rb LICENSE $ git commit -m 'The initial commit of my project' 当使用 git commit 进行提交操作时，Git 会先计算每一个子目录（本例中只有项目根目录）的校验和， 然后在 Git 仓库中这些校验和保存为树对象。随后，Git 便会创建一个提交对象， 它除了包含上面提到的那些信息外，还包含指向这个树对象（项目根目录）的指针。 如此一来，Git 就可以在需要的时候重现此次保存的快照。 现在，Git 仓库中有五个对象：三个 blob 对象（保存着文件快照）、一个 树对象 （记录着目录结构和 blob 对象索引）以及一个 提交对象（包含着指向前述树对象的指针和所有提交信息）。 图1. 首次提交对象及其树结构 ▲ 小结： git add 加入暂存操作，会为每个文件创建计算校验和，以及每个文件对应的文件快照（blob对象）。 git commit 提交操作，计算子目录或跟目录的校验和 保存为树对象。随后，创建一个提交对象，包含着指向树对象的指针和所有提交信息。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:1","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"再次提交 做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。 图2. 提交对象及其父对象 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:2","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"Git 的分支 Git 的分支，其实本质上仅仅是指向提交对象的可变指针。 Git 的默认分支名字是 master。 在多次提交操作之后，你其实已经有一个指向最后那个提交对象的 master 分支。 master 分支指针会在每次提交时自动向前移动。 Git 的 master 分支并不是一个特殊分支。 它就跟其它分支完全没有区别。 图3. 分支及其提交历史 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:3","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"创建分支 Git 是怎么创建新分支的呢？ 很简单，它只是为你创建了一个可以移动的新的指针。 比如，创建一个 testing 分支， 你需要使用 git branch 命令： $ git branch testing 这会在当前所在的提交对象上创建一个指针。 图4. 两个指向相同提交历史的分支 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:4","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"当前分支的指针 Git 是怎么知道当前在哪一个分支上呢？ 很简单，它有一个名为 HEAD 的特殊指针，指向当前所在的本地分支（译注：将 HEAD 想象为当前分支的别名）。 在本例中，你仍然在 master 分支上。 因为 git branch 命令仅仅 创建 一个新分支，并不会自动切换到新分支中去。 图5. HEAD 指向当前所在的分支 ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:5","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"查看当前所在分支 你可以简单地使用 git log 命令查看各个分支当前所指的对象。 提供这一功能的参数是 --decorate。 $ git log --oneline --decorate f30ab (HEAD -\u003e master, testing) add feature # f30ab提交对象 (HEAD当前所在分支 -\u003e master分支，testing 分支) 34ac2 Fixed bug # 34ac2 提交对象 98ca9 The initial commit of my project # 98ca9 提交对象 正如你所见，当前 master 和 testing 分支均指向校验和以 f30ab 开头的提交对象。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:6","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"分支切换 $ git checkout testing # git checkout \u003c分支名\u003e 这样 HEAD 就指向 testing 分支了。 图6. HEAD 指向当前所在的分支 ▲ 那么，这样的实现方式会给我们带来什么好处呢？ 现在不妨再提交一次： $ vim test.rb $ git commit -a -m 'made a change' 图7. HEAD 分支随着提交操作自动向前移动 ▲ 如图所示，你的 testing 分支向前移动了，但是 master 分支却没有，它仍然指向运行 git checkout 时所指的对象。 这就有意思了，现在我们切换回 master 分支看看： $ git checkout master 图8. 检出时 HEAD 随之移动 ▲ 这条命令做了两件事。 一是使 HEAD 指回 master 分支，二是将工作目录恢复成 master 分支所指向的快照内容。 也就是说，你现在做修改的话，项目将始于一个较旧的版本。 本质上来讲，这就是忽略 testing 分支所做的修改，以便于向另一个方向进行开发。 我们不妨再稍微做些修改并提交： $ vim test.rb $ git commit -a -m 'made other changes' 现在，这个项目的提交历史已经产生了分叉（参见 项目分叉历史）。 因为刚才你创建了一个新分支，并切换过去进行了一些工作，随后又切换回 master 分支进行了另外一些工作。 上述两次改动针对的是不同分支：你可以在不同分支间不断地来回切换和工作，并在时机成熟时将它们合并起来。 而所有这些工作，你需要的命令只有 branch、checkout 和 commit。 图9. 项目分叉历史 ▲ 你可以简单地使用 git log 命令查看分叉历史。 运行 git log --oneline --decorate --graph --all ，它会输出你的提交历史、各个分支的指向以及项目的分支分叉情况。 $ git log --oneline --decorate --graph --all * c2b9e (HEAD, master) made other changes | * 87ab2 (testing) made a change |/ * f30ab add feature * 34ac2 fixed bug * 98ca9 initial commit of my project 由于 Git 的分支实质上仅是包含所指对象校验和（长度为 40 的 SHA-1 值字符串）的文件，所以它的创建和销毁都异常高效。 创建一个新分支就相当于往一个文件中写入 41 个字节（40 个字符和 1 个换行符），如此的简单能不快吗？ 这与过去大多数版本控制系统形成了鲜明的对比，它们在创建分支时，将所有的项目文件都复制一遍，并保存到一个特定的目录。 完成这样繁琐的过程通常需要好几秒钟，有时甚至需要好几分钟。所需时间的长短，完全取决于项目的规模。 而在 Git 中，任何规模的项目都能在瞬间创建新分支。 同时，由于每次提交都会记录父对象，所以寻找恰当的合并基础（译注：即共同祖先）也是同样的简单和高效。 这些高效的特性使得 Git 鼓励开发人员频繁地创建和使用分支。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:7","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"创建分支同时切换 通常我们会在创建一个新分支后立即切换过去，可以使用如下命令： git checkout -b \u003cnewbranchname\u003e ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/:0:8","tags":["Git"],"title":"Git分支-分支原理","uri":"/posts/git%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E5%8E%9F%E7%90%86/"},{"categories":["《Git》学习笔记"],"content":"Git分支-远程分支 远程引用是对远程仓库的引用（指针），包括分支、标签等等。 远程分支本质上也是一个指针，指向远程地址 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:0","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"查看远程引用列表与信息 git ls-remote \u003cremote\u003e # 远程引用的完整列表 git remote show \u003cremote\u003e # 远程分支的更多信息 上面两行命令比较少用，更常见的做法是利用远程跟踪分支。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:1","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"远程跟踪分支 远程跟踪分支是远程分支状态的引用。它们是你无法移动的本地引用。一旦你进行了网络通信， Git 就会为你移动它们以精确反映远程仓库的状态。请将它们看做书签， 这样可以提醒你该分支在远程仓库中的位置就是你最后一次连接到它们的位置。 它们以 \u003cremote\u003e/\u003cbranch\u003e 的形式命名。 例如，如果你想要查看最后一次与远程仓库 origin 通信时 master 分支的状态，你可以查看 origin/master 分支。 你与同事合作解决一个问题并且他们推送了一个 iss53 分支，你可能有自己的本地 iss53 分支， 然而在服务器上的分支会以 origin/iss53 来表示。 这可能有一点儿难以理解，让我们来看一个例子。 假设你的网络里有一个在 git.ourcompany.com 的 Git 服务器。 如果你从这里克隆，Git 的 clone 命令会为你自动将其命名为 origin，拉取它的所有数据， 创建一个指向它的 master 分支的指针，并且在本地将其命名为 origin/master。 Git 也会给你一个与 origin 的 master 分支在指向同一个地方的本地 master 分支，这样你就有工作的基础。 笔记：从远程克隆下来的仓库有一个叫origin/master的远程跟踪分支 和 一个本地的master分支 笔记：“origin” 并无特殊含义远程仓库名字 “origin” 与分支名字 “master” 一样，在 Git 中并没有任何特别的含义一样。 同时 “master” 是当你运行 git init 时默认的起始分支名字，原因仅仅是它的广泛使用， “origin” 是当你运行 git clone 时默认的远程仓库名字。 如果你运行 git clone -o booyah，那么你默认的远程分支名字将会是 booyah/master。 图1. 克隆之后的服务器与本地仓库 ▲ 如果你在本地的 master 分支做了一些工作，在同一段时间内有其他人推送提交到 git.ourcompany.com 并且更新了它的 master 分支，这就是说你们的提交历史已走向不同的方向。 即便这样，只要你保持不与 origin 服务器连接（并拉取数据），你的 origin/master 指针就不会移动。 图2. 本地与远程的工作可以分叉 ▲ 如果要与给定的远程仓库同步数据，运行 git fetch \u003cremote\u003e 命令（在本例中为 git fetch origin）。 这个命令查找 “origin” 是哪一个服务器（在本例中，它是 git.ourcompany.com）， 从中抓取本地没有的数据，并且更新本地数据库，移动 origin/master 指针到更新之后的位置。 图3. git fetch 更新你的远程跟踪分支 ▲ 笔记: 本地的 master 分支 可能 和 远程跟踪分支 origin/master 分叉 为了演示有多个远程仓库与远程分支的情况，我们假定你有另一个内部 Git 服务器，仅服务于你的某个敏捷开发团队。 这个服务器位于 git.team1.ourcompany.com。 你可以运行 git remote add 命令添加一个新的远程仓库引用到当前的项目，这个命令我们会在 Git 基础 中详细说明。 将这个远程仓库命名为 teamone，将其作为完整 URL 的缩写。远程仓库名本质上是远程URL的缩写 图4. 添加另一个远程仓库 ▲ 现在，可以运行 git fetch teamone 来抓取远程仓库 teamone 有而本地没有的数据。 因为那台服务器上现有的数据是 origin 服务器上的一个子集， 所以 Git 并不会抓取数据而是会设置远程跟踪分支 teamone/master 指向 teamone 的 master 分支。 图5. 远程跟踪分支 teamone/master ▲ ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:2","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"推送 当你想要公开分享一个分支时，需要将其推送到有写入权限的远程仓库上。 本地的分支并不会自动与远程仓库同步——你必须显式地推送想要分享的分支。 这样，你就可以把不愿意分享的内容放到私人分支上，而将需要和别人协作的内容推送到公开分支。 如果希望和别人一起在名为 serverfix 的分支上工作，你可以像推送第一个分支那样推送它。 运行 git push \u003cremote\u003e \u003cbranch\u003e: $ git push origin serverfix Counting objects: 24, done. Delta compression using up to 8 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done. Total 24 (delta 2), reused 0 (delta 0) To https://github.com/schacon/simplegit * [new branch] serverfix -\u003e serverfix 这里有些工作被简化了。 Git 自动将 serverfix 分支名字展开为 refs/heads/serverfix:refs/heads/serverfix， 那意味着，“推送本地的 serverfix 分支来更新远程仓库上的 serverfix 分支。” 我们将会详细学习 Git 内部原理 的 refs/heads/ 部分， 但是现在可以先把它放在儿。你也可以运行 git push origin serverfix:serverfix， 它会做同样的事——也就是说“推送本地的 serverfix 分支，将其作为远程仓库的 serverfix 分支” 可以通过这种格式来推送本地分支到一个命名不相同的远程分支。 重命名远程仓库上的分支名 如果并不想让远程仓库上的分支叫做 serverfix，可以运行 git push origin serverfix:awesomebranch 来将本地的 serverfix 分支推送到远程仓库上的 awesomebranch 分支。 Note 如何避免每次输入密码如果你正在使用 HTTPS URL 来推送，Git 服务器会询问用户名与密码。 默认情况下它会在终端中提示服务器是否允许你进行推送。如果不想在每一次推送时都输入用户名与密码，你可以设置一个 “credential cache”。 最简单的方式就是将其保存在内存中几分钟，可以简单地运行 git config --global credential.helper cache 来设置它。想要了解更多关于不同验证缓存的可用选项，查看 凭证存储。 下一次其他协作者从服务器上抓取数据时，他们会在本地生成一个远程分支 origin/serverfix，指向服务器的 serverfix 分支的引用： $ git fetch origin remote: Counting objects: 7, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 3 (delta 0) Unpacking objects: 100% (3/3), done. From https://github.com/schacon/simplegit * [new branch] serverfix -\u003e origin/serverfix 要特别注意的一点是当抓取到新的远程跟踪分支时，本地不会自动生成一份可编辑的副本（拷贝）。 换一句话说，这种情况下，不会有一个新的 serverfix 分支——只有一个不可以修改的 origin/serverfix 指针。 可以运行 git merge origin/serverfix 将这些工作合并到当前所在的分支。 如果想要在自己的 serverfix 分支上工作，可以将其建立在远程跟踪分支之上： $ git checkout -b serverfix origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' 这会给你一个用于工作的本地分支，并且起点位于 origin/serverfix。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:3","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"跟踪分支 从一个远程跟踪分支检出一个本地分支会自动创建所谓的“跟踪分支”（它跟踪的分支叫做“上游分支”）。 跟踪分支是与远程分支有直接关系的本地分支。 如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。 当克隆一个仓库时，它通常会自动地创建一个跟踪 origin/master 的 master 分支。 然而，如果你愿意的话可以设置其他的跟踪分支，或是一个在其他远程仓库上的跟踪分支，又或者不跟踪 master 分支。 最简单的实例就是像之前看到的那样，运行 git checkout -b \u003cbranch\u003e \u003cremote\u003e/\u003cbranch\u003e。 这是一个十分常用的操作所以 Git 提供了 --track 快捷方式： $ git checkout --track origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' 由于这个操作太常用了，该捷径本身还有一个捷径。 如果你尝试检出的分支 (a) 不存在且 (b) 刚好只有一个名字与之匹配的远程分支，那么 Git 就会为你创建一个跟踪分支： $ git checkout serverfix Branch serverfix set up to track remote branch serverfix from origin. Switched to a new branch 'serverfix' 如果想要将本地分支与远程分支设置为不同的名字，你可以轻松地使用上一个命令增加一个不同名字的本地分支： $ git checkout -b sf origin/serverfix Branch sf set up to track remote branch serverfix from origin. Switched to a new branch 'sf' 现在，本地分支 sf 会自动从 origin/serverfix 拉取。 设置已有的本地分支跟踪一个刚刚拉取下来的远程分支，或者想要修改正在跟踪的上游分支， 你可以在任意时间使用 -u 或 --set-upstream-to 选项运行 git branch 来显式地设置。 $ git branch -u origin/serverfix Branch serverfix set up to track remote branch serverfix from origin. Note 上游快捷方式当设置好跟踪分支后，可以通过简写 @{upstream} 或 @{u} 来引用它的上游分支。 所以在 master 分支时并且它正在跟踪 origin/master 时，如果愿意的话可以使用 git merge @{u} 来取代 git merge origin/master。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:4","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"查看跟踪分支 如果想要查看设置的所有跟踪分支，可以使用 git branch 的 -vv 选项。 这会将所有的本地分支列出来并且包含更多的信息，如每一个分支正在跟踪哪个远程分支与本地分支是否是领先、落后或是都有。 $ git branch -vv iss53 7e424c3 [origin/iss53: ahead 2] forgot the brackets master 1ae2a45 [origin/master] deploying index fix * serverfix f8674d9 [teamone/server-fix-good: ahead 3, behind 1] this should do it testing 5ea463a trying something new 这里可以看到 iss53 分支正在跟踪 origin/iss53 并且 “ahead” 是 2，意味着本地有两个提交还没有推送到服务器上。 也能看到 master 分支正在跟踪 origin/master 分支并且是最新的。 接下来可以看到 serverfix 分支正在跟踪 teamone 服务器上的 server-fix-good 分支并且领先 3 落后 1， 意味着服务器上有一次提交还没有合并入同时本地有三次提交还没有推送。 最后看到 testing 分支并没有跟踪任何远程分支。 需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。 如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。 可以像这样做： $ git fetch --all; git branch -vv ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:5","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"拉取 当 git fetch 命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。 它只会获取数据然后让你自己合并。 然而，有一个命令叫作 git pull 在大多数情况下它的含义是一个 git fetch 紧接着一个 git merge 命令。 如果有一个像之前章节中演示的设置好的跟踪分支，不管它是显式地设置还是通过 clone 或 checkout 命令为你创建的，git pull 都会查找当前分支所跟踪的服务器与分支， 从服务器上抓取数据然后尝试合并入那个远程分支。 由于 git pull 的魔法经常令人困惑所以通常单独显式地使用 fetch 与 merge 命令会更好一些。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:6","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"删除远程分支 假设你已经通过远程分支做完所有的工作了——也就是说你和你的协作者已经完成了一个特性， 并且将其合并到了远程仓库的 master 分支（或任何其他稳定代码分支）。 可以运行带有 --delete 选项的 git push 命令来删除一个远程分支。 如果想要从服务器上删除 serverfix 分支，运行下面的命令： $ git push origin --delete serverfix To https://github.com/schacon/simplegit - [deleted] serverfix 基本上这个命令做的只是从服务器上移除这个指针。 Git 服务器通常会保留数据一段时间直到垃圾回收运行，所以如果不小心删除掉了，通常是很容易恢复的。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/:0:7","tags":["Git"],"title":"Git分支-远程分支","uri":"/posts/git%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"Git分支的新建与合并-分支操作 文档：Git 分支 - 分支的新建与合并 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:0","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"创建分支并切换 此时有一个需求需要在新的分支iss53上工作： $ git checkout -b iss53 # b表示branch 它是下面两条命令的简写： $ git branch iss53 $ git checkout iss53 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:1","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"切换分支 突然有一个紧急问题要解决，需要在原来的master分支进行修复： $ git checkout master 在切换到master之前，需要iss53分支保持好一个干净的状态（修改都已提交）。 注意：切换分支Git 会重置你的工作目录。 checkout 中文含义 “检出”，checkout \u003cbranch\u003e 检出分支 =\u003e 检出指定分支的代码 =\u003e 重置工作目录并切换分支。 接下来，你要修复这个紧急问题。 建立一个 hotfix 分支，在该分支上工作直到问题解决： $ git checkout -b hotfix # 中间过程在hotfix上修改了代码并提交 $ echo 'test' \u003e ./hotfix.txt $ git add . $ git commit -m 'fixed' ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:2","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"合并分支 $ git checkout master # 首先切回master分支 $ git merge hotfix # 把 hotfix 分支合并过来 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:3","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"删除分支 $ git branch -d hotfix # d表示delete # 然后切回iss53继续工作 $ git checkout iss53 注意删除分支是在 branch 命令上 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:4","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"多次提交之后合并分支 假设你已经修正了 #53 问题，打算合并到master分支： $ git checkout master $ git merga iss53 这看似和之前的合并区别不大。此时你的开发历史从一个更早的地方开始分叉开来（diverged）。 因为，master 分支所在提交并不是 iss53 分支所在提交的直接祖先，Git 不得不做一些额外的工作。 出现这种情况的时候，Git 会使用两个分支的末端所指的快照以及这两个分支的公共祖先，做一个简单的三方合并。 和之前将分支指针向前推进所不同的是，Git 将此次三方合并的结果做了一个新的快照并且自动创建一个新的提交指向它。 这个被称作一次合并提交，它的特别之处在于他有不止一个父提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:5","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"遇到冲突时的分支合并 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法干净的合并它们，就产生了冲突。 合并过程中出现CONFLICT提升，表示有冲突 $ git merge iss53 Auto-merging index.html CONFLICT (content): Merge conflict in index.html Automatic merge failed; fix conflicts and then commit the result. 使用git status查看未合并状态。 任何因包含合并冲突而有待解决的文件，都会以未合并状态标识出来。 Git 会在有冲突的文件中加入标准的冲突解决标记，这样你可以打开这些包含冲突的文件然后手动解决冲突。 出现冲突的文件会包含一些特殊区段，看起来像下面这个样子： \u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD:index.html \u003cdiv id=\"footer\"\u003econtact : email.support@github.com\u003c/div\u003e ======= \u003cdiv id=\"footer\"\u003e please contact us at support@github.com \u003c/div\u003e \u003e\u003e\u003e\u003e\u003e\u003e\u003e iss53:index.html 你需要手动解决冲突，解决了所有文件里的冲突之后，对每个文件使用 git add 命令来将其标记为冲突已解决。 一旦暂存这些原本有冲突的文件，Git 就会将它们标记为冲突已解决。 如果你对结果感到满意，并且确定之前有冲突的的文件都已经暂存了，这时你可以输入 git commit 来完成合并提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/:0:6","tags":["Git"],"title":"Git分支的新建与合并-分支操作","uri":"/posts/git%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6-%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C/"},{"categories":["《Git》学习笔记"],"content":"Git分支管理-查看分支 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/:0:0","tags":["Git"],"title":"Git分支管理-查看分支","uri":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"查看分支 $ git branch iss53 * master # 带星号*表示当前所在分支 testing git branch 命令不只是可以创建与删除分支。 如果不加任何参数运行它，会得到当前所有分支的一个列表。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/:0:1","tags":["Git"],"title":"Git分支管理-查看分支","uri":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"查看每个分支的最后提交 $ git branch -v iss53 93b412c fix javascript issue * master 7a98805 Merge branch 'iss53' testing 782fd34 test ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/:0:2","tags":["Git"],"title":"Git分支管理-查看分支","uri":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"查看已(未)合并的分支 --merged 与 --no-merged 这两个选项可以查看哪些分支已经合并或未合并到 当前 分支。 $ git branch --merged # 查看已合并分支列表 iss53 * master 上面列表中分支名字前没有 * 号的分支通常可以使用 git branch -d 删除掉； $ git branch --no-merged # 查看未合并的分支列表 testing 上面显示未合并的分支，尝试使用 git branch -d 命令删除它时会失败： $ git branch -d testing error: The branch 'testing' is not fully merged. If you are sure you want to delete it, run 'git branch -D testing'. 强制删除未合并的分支: $ git branch -D testing 查看指定分支的已(未)合并的分支 上面描述的选项 --merged 和 --no-merged 会在没有给定提交或分支名作为参数时， 分别列出已合并或未合并到 当前 分支的分支。 你总是可以提供一个附加的参数来查看其它分支的合并状态而不必检出它们。 例如，尚未合并到 testing 分支的有哪些？ $ git branch --no-merged testing topicA featureB ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/:0:3","tags":["Git"],"title":"Git分支管理-查看分支","uri":"/posts/git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86-%E6%9F%A5%E7%9C%8B%E5%88%86%E6%94%AF/"},{"categories":["《Git》学习笔记"],"content":"Git分支开发工作流 文档：Git分支开发工作流 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/:0:0","tags":["Git"],"title":"Git分支开发工作流","uri":"/posts/git%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["《Git》学习笔记"],"content":"长期分支 因为 Git 使用简单的三方合并，所以就算在一段较长的时间内，反复把一个分支合并入另一个分支，也不是什么难事。 也就是说，在整个项目开发周期的不同阶段，你可以同时拥有多个开放的分支；你可以定期地把某些主题分支合并入其他分支中。 许多使用 Git 的开发者都喜欢使用这种方式来工作，比如只在 master 分支上保留完全稳定的代码，开发过程在dev分支，开发完成后并入test分支进行测试，通过测试的稳定代码才并入master分支中。 dev和test分支不需要保持绝对稳定，但在test通过测试达到稳定状态，就可以被合并入master分支。 事实上我们刚才讨论的，是随着你的提交而不断右移的指针。 稳定分支(master)的指针总是在提交历史中落后一大截，而前沿分支(dev或test)的指针往往比较靠前。 你可以用这种方法维护不同层次的稳定性。 一些大型项目还有一个 proposed（建议） 或 pu: proposed updates（建议更新）分支，它可能因包含一些不成熟的内容而不能进入master 分支。 这么做的目的是使你的分支具有不同级别的稳定性；当它们具有一定程度的稳定性后，再把它们合并入具有更高级别稳定性的分支中。 再次强调一下，使用多个长期分支的方法并非必要，但是这么做通常很有帮助，尤其是当你在一个非常庞大或者复杂的项目中工作时。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/:0:1","tags":["Git"],"title":"Git分支开发工作流","uri":"/posts/git%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["《Git》学习笔记"],"content":"主题分支 (短期分支) 主题分支对任何规模的项目都适用。 主题分支是一种短期分支，它被用来实现单一特性或其相关工作。 你已经在上一节中你创建的 iss53 和 hotfix 主题分支中看到过这种用法。 你在上一节用到的主题分支（iss53 和 hotfix 分支）中提交了一些更新，并且在它们合并入主干分支之后，你又删除了它们。 这项技术能使你快速并且完整地进行上下文切换（context-switch）——因为你的工作被分散到不同的流水线中，在不同的流水线中每个分支都仅与其目标特性相关，因此，在做代码审查之类的工作的时候就能更加容易地看出你做了哪些改动。 你可以把做出的改动在主题分支中保留几分钟、几天甚至几个月，等它们成熟之后再合并，而不用在乎它们建立的顺序或工作进度。 考虑这样一个例子，你在 master 分支上工作到 C1，这时为了解决一个问题而新建 iss91 分支，在 iss91 分支上工作到 C4，然而对于那个问题你又有了新的想法，于是你再新建一个 iss91v2 分支试图用另一种方法解决那个问题，接着你回到 master 分支工作了一会儿，你又冒出了一个不太确定的想法，你便在 C10 的时候新建一个 dumbidea 分支，并在上面做些实验。 你的提交历史看起来像下面这个样子： 图1. 拥有多个主题分支的提交历史 ▲ 现在，我们假设两件事情：你决定使用第二个方案来解决那个问题，即使用在 iss91v2 分支中方案。 另外，你将 dumbidea 分支拿给你的同事看过之后，结果发现这是个惊人之举。 这时你可以抛弃 iss91 分支（即丢弃 C5 和 C6 提交），然后把另外两个分支合并入主干分支。 最终你的提交历史看起来像下面这个样子： 图2. 合并了 dumbidea 和 iss91v2 分支之后的提交历史 ▲ 我们将会在 分布式 Git 中向你揭示更多有关分支工作流的细节， 因此，请确保你阅读完那个章节之后，再来决定你的下个项目要使用什么样的分支策略（branching scheme）。 请牢记，当你做这么多操作的时候，这些分支全部都存于本地。 当你新建和合并分支的时候，所有这一切都只发生在你本地的 Git 版本库中 —— 没有与服务器发生交互。 ","date":"2020-11-18","objectID":"/posts/git%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/:0:2","tags":["Git"],"title":"Git分支开发工作流","uri":"/posts/git%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["《Git》学习笔记"],"content":"Git工具-查看修订版本 Git 能够以多种方式来指定单个提交、一组提交、或者一定范围内的提交。 了解它们并不是必需的，但是了解一下总没坏处。 修订版本指的是：提交 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:0:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"单个修订版本 你可以通过任意一个提交的 40 个字符的完整 SHA-1 散列值来指定它， 不过还有很多更人性化的方式来做同样的事情。本节将会介绍获取单个提交的多种方法。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:1:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"简短的 SHA-1 Git 十分智能，你只需要提供 SHA-1 的前几个字符就可以获得对应的那次提交， 当然你提供的 SHA-1 字符数量不得少于 4 个，并且没有歧义——也就是说， 当前对象数据库中没有其它对象以这段 SHA-1 开头。 例如，要查看你知道其中添加了某个功能的提交，首先运行 git log 命令来定位该提交： $ git log commit 734713bc047d87bf7eac9674765ae793478c50d3 Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Fri Jan 2 18:32:33 2009 -0800 fixed refs handling, added gc auto, updated tests commit d921970aadf03b3cf0e71becdaab3147ba71cdef Merge: 1c002dd... 35cfb2b... Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Thu Dec 11 15:08:43 2008 -0800 Merge commit 'phedders/rdocs' commit 1c002dd4b536e7479fe34593e72e6c6c1819e53b Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Thu Dec 11 14:58:32 2008 -0800 added some blame and merge stuff ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:2:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"查看给定SHA-1值的提交 在本例中，假设你想要的提交其 SHA-1 以 1c002dd.... 开头， 那么你可以用如下几种 git show 的变体来检视该提交（假设简短的版本没有歧义）： $ git show 1c002dd4b536e7479fe34593e72e6c6c1819e53b $ git show 1c002dd4b536e7479f $ git show 1c002d Git 可以为 SHA-1 值生成出简短且唯一的缩写。 如果你在 git log 后加上 --abbrev-commit 参数，输出结果里就会显示简短且唯一的值； 默认使用七个字符，不过有时为了避免 SHA-1 的歧义，会增加字符数： $ git log --abbrev-commit --pretty=oneline ca82a6d changed the version number 085bb3b removed unnecessary test code a11bef0 first commit 通常 8 到 10 个字符就已经足够在一个项目中避免 SHA-1 的歧义。 例如，到 2019 年 2 月为止，Linux 内核这个相当大的 Git 项目， 其对象数据库中有超过 875,000 个提交，包含七百万个对象，也只需要前 12 个字符就能保证唯一性。 Note 关于 SHA-1 的简短说明许多人觉得他们的仓库里有可能出现两个不同的对象其 SHA-1 值相同。 然后呢？如果你真的向仓库里提交了一个对象，它跟之前的某个 不同 对象的 SHA-1 值相同， Git 会发现该对象的散列值已经存在于仓库里了，于是就会认为该对象被写入，然后直接使用它。 如果之后你想检出那个对象时，你将得到先前那个对象的数据。但是这种情况发生的概率十分渺小。 SHA-1 摘要长度是 20 字节，也就是 160 位。 2^80 个随机哈希对象才有 50% 的概率出现一次冲突 （计算冲突机率的公式是 p = (n(n-1)/2) * (1/2^160)) ）。 2^80 是 1.2 x 10^24，也就是一亿亿亿，这是地球上沙粒总数的 1200 倍。举例说一下怎样才能产生一次 SHA-1 冲突。 如果地球上 65 亿个人类都在编程，每人每秒都在产生等价于整个 Linux 内核历史（650 万个 Git 对象）的代码， 并将之提交到一个巨大的 Git 仓库里面，这样持续两年的时间才会产生足够的对象， 使其拥有 50% 的概率产生一次 SHA-1 对象冲突， 这比你编程团队的成员同一个晚上在互不相干的意外中被狼袭击并杀死的机率还要小。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:2:1","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"分支引用 引用特定提交的一种直接方法是，若它是一个分支的顶端的提交， 那么可以在任何需要引用该提交的 Git 命令中直接使用该分支的名称。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:3:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"查看最后一次提交 例如，你想要查看一个分支的最后一次提交的对象，假设 topic1 分支指向提交 ca82a6d... ， 那么以下的命令是等价的： $ git show ca82a6dff817ec66f44342007202690a93763949 $ git show topic1 # topic1是分支名 如果你想知道某个分支指向哪个特定的 SHA-1，或者想看任何一个例子中被简写的 SHA-1， 你可以使用一个叫做 rev-parse 的 Git 探测工具。 你可以在 Git 内部原理 中查看更多关于探测工具的信息。 简单来说，rev-parse 是为了底层操作而不是日常操作设计的。 不过，有时你想看 Git 现在到底处于什么状态时，它可能会很有用。 你可以在你的分支上执行 rev-parse $ git rev-parse topic1 ca82a6dff817ec66f44342007202690a93763949 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:3:1","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"引用日志 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:4:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"HEAD的指向历史 当你在工作时， Git 会在后台保存一个引用日志（reflog）， 引用日志记录了最近几个月你的 HEAD 和分支引用所指向的历史。 你可以使用 git reflog 来查看引用日志 $ git reflog 734713b HEAD@{0}: commit: fixed refs handling, added gc auto, updated d921970 HEAD@{1}: merge phedders/rdocs: Merge made by the 'recursive' strategy. 1c002dd HEAD@{2}: commit: added some blame and merge stuff 1c36188 HEAD@{3}: rebase -i (squash): updating HEAD 95df984 HEAD@{4}: commit: # This is a combination of two commits. 1c36188 HEAD@{5}: rebase -i (squash): updating HEAD 7e05da5 HEAD@{6}: rebase -i (pick): updating HEAD 每当你的 HEAD 所指向的位置发生了变化，Git 就会将这个信息存储到引用日志这个历史记录里。 你也可以通过 reflog 数据来获取之前的提交历史。 如果你想查看仓库中 HEAD 在五次前的所指向的提交，你可以使用 @{n} 来引用 reflog 中输出的提交记录。 $ git show HEAD@{5} 你同样可以使用这个语法来查看某个分支在一定时间前的位置。 例如，查看你的 master 分支在昨天的时候指向了哪个提交，你可以输入 $ git show master@{yesterday} 就会显示昨天 master 分支的顶端指向了哪个提交。 这个方法只对还在你引用日志里的数据有用，所以不能用来查好几个月之前的提交。 可以运行 git log -g 来查看类似于 git log 输出格式的引用日志信息： $ git log -g master commit 734713bc047d87bf7eac9674765ae793478c50d3 Reflog: master@{0} (Scott Chacon \u003cschacon@gmail.com\u003e) Reflog message: commit: fixed refs handling, added gc auto, updated Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Fri Jan 2 18:32:33 2009 -0800 fixed refs handling, added gc auto, updated tests commit d921970aadf03b3cf0e71becdaab3147ba71cdef Reflog: master@{1} (Scott Chacon \u003cschacon@gmail.com\u003e) Reflog message: merge phedders/rdocs: Merge made by recursive. Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Thu Dec 11 15:08:43 2008 -0800 Merge commit 'phedders/rdocs' 值得注意的是，引用日志只存在于本地仓库，它只是一个记录你在 自己 的仓库里做过什么的日志。 其他人拷贝的仓库里的引用日志不会和你的相同，而你新克隆一个仓库的时候，引用日志是空的，因为你在仓库里还没有操作。 git show HEAD@{2.months.ago} 这条命令只有在你克隆了一个项目至少两个月时才会显示匹配的提交—— 如果你刚刚克隆了仓库，那么它将不会有任何结果返回。 Tip 将引用日志想作 Git 版的 shell 历史记录如果你有 UNIX 或者 Linux 的背景，不妨将引用日志想作 Git 版的 shell 历史记录， 重点在于仅与你和你的会话相关，而与他人无关。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:4:1","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"祖先引用 祖先引用是另一种指明一个提交的方式。 如果你在引用的尾部加上一个 ^（脱字符）， Git 会将其解析为该引用的上一个提交。 假设你的提交历史是： $ git log --pretty=format:'%h %s' --graph * 734713b fixed refs handling, added gc auto, updated tests * d921970 Merge commit 'phedders/rdocs' |\\ | * 35cfb2b Some rdoc changes * | 1c002dd added some blame and merge stuff |/ * 1c36188 ignore *.gem * 9b29157 add open3_detach to gemspec file list 你可以使用 HEAD^ 来查看上一个提交，也就是 “HEAD 的父提交”： $ git show HEAD^ commit d921970aadf03b3cf0e71becdaab3147ba71cdef Merge: 1c002dd... 35cfb2b... Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Thu Dec 11 15:08:43 2008 -0800 Merge commit 'phedders/rdocs' Note 在 Windows 上转义脱字符在 Windows 的 cmd.exe 中，^ 是一个特殊字符，因此需要区别对待。 你可以双写它或者将提交引用放在引号中：$ git show HEAD^ # 在 Windows 上无法工作 $ git show HEAD^^ # 可以 $ git show \"HEAD^\" # 可以 你也可以在 ^ 后面添加一个数字来指明想要 哪一个 父提交——例如 d921970^2 代表 “d921970 的第二父提交” 这个语法只适用于合并的提交，因为合并提交会有多个父提交。 合并提交的第一父提交是你合并时所在分支（通常为 master），而第二父提交是你所合并的分支（例如 topic）： $ git show d921970^ commit 1c002dd4b536e7479fe34593e72e6c6c1819e53b Author: Scott Chacon \u003cschacon@gmail.com\u003e Date: Thu Dec 11 14:58:32 2008 -0800 added some blame and merge stuff $ git show d921970^2 commit 35cfb2b795a55793d7cc56a6cc2060b4bb732548 Author: Paul Hedderly \u003cpaul+git@mjr.org\u003e Date: Wed Dec 10 22:22:03 2008 +0000 Some rdoc changes 另一种指明祖先提交的方法是 ~（波浪号）。 同样是指向第一父提交，因此 HEAD~ 和 HEAD^ 是等价的。 而区别在于你在后面加数字的时候。 HEAD~2 代表“第一父提交的第一父提交”，也就是“祖父提交”——Git 会根据你指定的次数获取对应的第一父提交。 例如，在之前的列出的提交历史中，HEAD~3 就是 $ git show HEAD~3 commit 1c3618887afb5fbcbea25b7c013f4e2114448b8d Author: Tom Preston-Werner \u003ctom@mojombo.com\u003e Date: Fri Nov 7 13:47:59 2008 -0500 ignore *.gem 也可以写成 HEAD~~~，也是第一父提交的第一父提交的第一父提交： $ git show HEAD~~~ commit 1c3618887afb5fbcbea25b7c013f4e2114448b8d Author: Tom Preston-Werner \u003ctom@mojombo.com\u003e Date: Fri Nov 7 13:47:59 2008 -0500 ignore *.gem 你也可以组合使用这两个语法——你可以通过 HEAD~3^2 来取得之前引用的第二父提交（假设它是一个合并提交）。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:5:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"提交区间 你已经学会如何单次的提交，现在来看看如何指明一定区间的提交。 当你有很多分支时，这对管理你的分支时十分有用， 你可以用提交区间来解决“这个分支还有哪些提交尚未合并到主分支？”的问题 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:6:0","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"双点 最常用的指明提交区间语法是双点。 这种语法可以让 Git 选出在一个分支中而不在另一个分支中的提交。 例如，你有如下的提交历史 Example history for range selection. Figure 137. Example history for range selection. 你想要查看 experiment 分支中还有哪些提交尚未被合并入 master 分支。 你可以使用 master..experiment 来让 Git 显示这些提交。也就是“在 experiment 分支中而不在 master 分支中的提交”。 为了使例子简单明了，我使用了示意图中提交对象的字母来代替真实日志的输出，所以会显示： $ git log master..experiment D C 反过来，如果你想查看在 master 分支中而不在 experiment 分支中的提交，你只要交换分支名即可。 experiment..master 会显示在 master 分支中而不在 experiment 分支中的提交： $ git log experiment..master F E 查看即将推送到远端的内容 这可以让你保持 experiment 分支跟随最新的进度以及查看你即将合并的内容。 另一个常用的场景是查看你即将推送到远端的内容： $ git log origin/master..HEAD 这个命令会输出在你当前分支中而不在远程 origin 中的提交。 如果你执行 git push 并且你的当前分支正在跟踪 origin/master，由 git log origin/master..HEAD 所输出的提交就是会被传输到远端服务器的提交。如果你留空了其中的一边， Git 会默认为 HEAD。 例如， git log origin/master.. 将会输出与之前例子相同的结果 —— Git 使用 HEAD 来代替留空的一边。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:6:1","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"多点 双点语法很好用，但有时候你可能需要两个以上的分支才能确定你所需要的修订， 比如查看哪些提交是被包含在某些分支中的一个，但是不在你当前的分支上。 Git 允许你在任意引用前加上 ^ 字符或者 --not 来指明你不希望提交被包含其中的分支。 因此下列三个命令是等价的： $ git log refA..refB $ git log ^refA refB $ git log refB --not refA 这个语法很好用，因为你可以在查询中指定超过两个的引用，这是双点语法无法实现的。 比如，你想查看所有被 refA 或 refB 包含的但是不被 refC 包含的提交，你可以使用以下任意一个命令： $ git log refA refB ^refC $ git log refA refB --not refC 这就构成了一个十分强大的修订查询系统，你可以通过它来查看你的分支里包含了哪些东西。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:6:2","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"三点 最后一种主要的区间选择语法是三点，这个语法可以选择出被两个引用 之一 包含但又不被两者同时包含的提交。 再看看之前双点例子中的提交历史。 如果你想看 master 或者 experiment 中包含的但不是两者共有的提交，你可以执行： $ git log master...experiment F E D C 这和通常 log 按日期排序的输出一样，仅仅给出了4个提交的信息。 这种情形下，log 命令的一个常用参数是 --left-right，它会显示每个提交到底处于哪一侧的分支。 这会让输出数据更加清晰。 $ git log --left-right master...experiment \u003c F \u003c E \u003e D \u003e C 有了这些工具，你就可以十分方便地查看你 Git 仓库中的提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/:6:3","tags":["Git"],"title":"Git工具-查看修订版本","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E6%9F%A5%E7%9C%8B%E4%BF%AE%E8%AE%A2%E7%89%88%E6%9C%AC/"},{"categories":["《Git》学习笔记"],"content":"Git工具-交互式暂存 本节中的几个交互式 Git 命令可以帮助你将文件的特定部分组合成提交。 当你在修改了大量文件后，希望这些改动能拆分为若干提交而不是混杂在一起成为一个提交时，这几个工具会非常有用。 通过这种方式，可以确保提交是逻辑上独立的变更集，同时也会使其他开发者在与你工作时很容易地审核。 如果运行 git add 时使用 -i 或者 --interactive 选项，Git 将会进入一个交互式终端模式，显示类似下面的东西： $ git add -i staged unstaged path 1: unchanged +0/-1 TODO 2: unchanged +1/-1 index.html 3: unchanged +5/-1 lib/simplegit.rb *** Commands *** 1: [s]tatus 2: [u]pdate 3: [r]evert 4: [a]dd untracked 5: [p]atch 6: [d]iff 7: [q]uit 8: [h]elp What now\u003e 可以看到这个命令以和平时非常不同的视图显示了暂存区——基本上与 git status 是相同的信息，但是更简明扼要一些。 它将暂存的修改列在左侧，未暂存的修改列在右侧。 在这块区域后是“Commands”命令区域。 在这里你可以做一些工作，包括暂存文件、取消暂存文件、暂存文件的一部分、添加未被追踪的文件、显示暂存内容的区别。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%9A%82%E5%AD%98/:0:0","tags":["Git"],"title":"Git工具-交互式暂存","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%9A%82%E5%AD%98/"},{"categories":["《Git》学习笔记"],"content":"暂存与取消暂存文件 如果在 What now\u003e 提示符后键入 u 或 2（更新），它会问你想要暂存哪个文件： What now\u003e u staged unstaged path 1: unchanged +0/-1 TODO 2: unchanged +1/-1 index.html 3: unchanged +5/-1 lib/simplegit.rb Update\u003e\u003e 要暂存 TODO 和 index.html 文件，可以输入数字： Update\u003e\u003e 1,2 staged unstaged path * 1: unchanged +0/-1 TODO * 2: unchanged +1/-1 index.html 3: unchanged +5/-1 lib/simplegit.rb Update\u003e\u003e 每个文件前面的 * 意味着选中的文件将会被暂存。 如果在 Update\u003e\u003e 提示符后不输入任何东西并直接按回车，Git 将会暂存之前选择的文件： Update\u003e\u003e updated 2 paths *** Commands *** 1: [s]tatus 2: [u]pdate 3: [r]evert 4: [a]dd untracked 5: [p]atch 6: [d]iff 7: [q]uit 8: [h]elp What now\u003e s staged unstaged path 1: +0/-1 nothing TODO 2: +1/-1 nothing index.html 3: unchanged +5/-1 lib/simplegit.rb 现在可以看到 TODO 与 index.html 文件已经被暂存而 simplegit.rb 文件还未被暂存。 如果这时想要取消暂存 TODO 文件，使用 r 或 3（撤消）选项： *** Commands *** 1: [s]tatus 2: [u]pdate 3: [r]evert 4: [a]dd untracked 5: [p]atch 6: [d]iff 7: [q]uit 8: [h]elp What now\u003e r staged unstaged path 1: +0/-1 nothing TODO 2: +1/-1 nothing index.html 3: unchanged +5/-1 lib/simplegit.rb Revert\u003e\u003e 1 staged unstaged path * 1: +0/-1 nothing TODO 2: +1/-1 nothing index.html 3: unchanged +5/-1 lib/simplegit.rb Revert\u003e\u003e [enter] reverted one path 再次查看 Git 状态，可以看到已经取消暂存 TODO 文件： *** Commands *** 1: [s]tatus 2: [u]pdate 3: [r]evert 4: [a]dd untracked 5: [p]atch 6: [d]iff 7: [q]uit 8: [h]elp What now\u003e s staged unstaged path 1: unchanged +0/-1 TODO 2: +1/-1 nothing index.html 3: unchanged +5/-1 lib/simplegit.rb 如果想要查看已暂存内容的区别，可以使用 d 或 6（区别）命令。 它会显示暂存文件的一个列表，可以从中选择想要查看的暂存区别。 这跟你在命令行指定 git diff --cached 非常相似： *** Commands *** 1: [s]tatus 2: [u]pdate 3: [r]evert 4: [a]dd untracked 5: [p]atch 6: [d]iff 7: [q]uit 8: [h]elp What now\u003e d staged unstaged path 1: +1/-1 nothing index.html Review diff\u003e\u003e 1 diff --git a/index.html b/index.html index 4d07108..4335f49 100644 --- a/index.html +++ b/index.html @@ -16,7 +16,7 @@ Date Finder \u003cp id=\"out\"\u003e...\u003c/p\u003e -\u003cdiv id=\"footer\"\u003econtact : support@github.com\u003c/div\u003e +\u003cdiv id=\"footer\"\u003econtact : email.support@github.com\u003c/div\u003e \u003cscript type=\"text/javascript\"\u003e 通过这些基本命令，可以使用交互式添加模式来轻松地处理暂存区。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%9A%82%E5%AD%98/:1:0","tags":["Git"],"title":"Git工具-交互式暂存","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%9A%82%E5%AD%98/"},{"categories":["《Git》学习笔记"],"content":"暂存补丁 Git 也可以暂存文件的特定部分。 例如，如果在 simplegit.rb 文件中做了两处修改，但只想要暂存其中的一个而不是另一个，Git 会帮你轻松地完成。 在和上一节一样的交互式提示符中，输入 p 或 5（补丁）。 Git 会询问你想要部分暂存哪些文件；然后，对已选择文件的每一个部分，它都会一个个地显示文件区别并询问你是否想要暂存它们： diff --git a/lib/simplegit.rb b/lib/simplegit.rb index dd5ecc4..57399e0 100644 --- a/lib/simplegit.rb +++ b/lib/simplegit.rb @@ -22,7 +22,7 @@ class SimpleGit end def log(treeish = 'master') - command(\"git log -n 25 #{treeish}\") + command(\"git log -n 30 #{treeish}\") end def blame(path) Stage this hunk [y,n,a,d,/,j,J,g,e,?]? 这时有很多选项。 输入 ? 显示所有可以使用的命令列表： Stage this hunk [y,n,a,d,/,j,J,g,e,?]? ? y - stage this hunk n - do not stage this hunk a - stage this and all the remaining hunks in the file d - do not stage this hunk nor any of the remaining hunks in the file g - select a hunk to go to / - search for a hunk matching the given regex j - leave this hunk undecided, see next undecided hunk J - leave this hunk undecided, see next hunk k - leave this hunk undecided, see previous undecided hunk K - leave this hunk undecided, see previous hunk s - split the current hunk into smaller hunks e - manually edit the current hunk ? - print help 通常情况下可以输入 y 或 n 来选择是否要暂存每一个区块， 当然，暂存特定文件中的所有部分或为之后的选择跳过一个区块也是非常有用的。 如果你只暂存文件的一部分，状态输出可能会像下面这样： What now\u003e 1 staged unstaged path 1: unchanged +0/-1 TODO 2: +1/-1 nothing index.html 3: +1/-1 +4/-0 lib/simplegit.rb simplegit.rb 文件的状态很有趣。 它显示出若干行被暂存与若干行未被暂存。 已经部分地暂存了这个文件。 在这时，可以退出交互式添加脚本并且运行 git commit 来提交部分暂存的文件。 也可以不必在交互式添加模式中做部分文件暂存——可以在命令行中使用 git add -p 或 git add --patch 来启动同样的脚本。 更进一步地，可以使用 git reset --patch 命令的补丁模式来部分重置文件， 通过 git checkout --patch 命令来部分检出文件与 git stash save --patch 命令来部分暂存文件。 我们将会在接触这些命令的高级使用方法时了解更多详细信息。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%9A%82%E5%AD%98/:2:0","tags":["Git"],"title":"Git工具-交互式暂存","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%9A%82%E5%AD%98/"},{"categories":["《Git》学习笔记"],"content":"Git 工具 - 重写历史 许多时候，在使用 Git 时，你可能想要修订提交历史。 Git 很棒的一点是它允许你在最后时刻做决定。 你可以在将暂存区内容提交前决定哪些文件进入提交，可以通过 git stash 来决定不与某些内容工作， 也可以重写已经发生的提交就像它们以另一种方式发生的一样。 这可能涉及改变提交的顺序，改变提交中的信息或修改文件，将提交压缩或是拆分， 或完全地移除提交——在将你的工作成果与他人共享之前。 在本节中，你可以学到如何完成这些工作，这样在与他人分享你的工作成果时你的提交历史将如你所愿地展示出来。 Note 在满意之前不要推送你的工作Git 的基本原则之一是，由于克隆中有很多工作是本地的，因此你可以 在本地 随便重写历史记录。 然而一旦推送了你的工作，那就完全是另一回事了，除非你有充分的理由进行更改，否则应该将推送的工作视为最终结果。 简而言之，在对它感到满意并准备与他人分享之前，应当避免推送你的工作。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:0:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"修改最后一次提交 修改你最近一次提交可能是所有修改历史提交的操作中最常见的一个。 对于你的最近一次提交，你往往想做两件事情：简单地修改提交信息， 或者通过添加、移除或修改文件来更改提交实际的内容。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:1:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"修改提交信息 如果，你只是想修改最近一次提交的提交信息，那么很简单： $ git commit --amend 上面这条命令会将最后一次的提交信息载入到编辑器中供你修改。 当保存并关闭编辑器后，编辑器会将更新后的提交信息写入新提交中，它会成为新的最后一次提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:1:1","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"修改实际内容 另一方面，如果你想要修改最后一次提交的实际内容，那么流程很相似：首先作出你想要补上的修改， 暂存它们，然后用 git commit --amend 以新的改进后的提交来 替换 掉旧有的最后一次提交， 使用这个技巧的时候需要小心，因为修正会改变提交的 SHA-1 校验和。 它类似于一个小的变基——如果已经推送了最后一次提交就不要修正它。 Tip 修补后的提交可能需要修补提交信息当你在修补一次提交时，可以同时修改提交信息和提交内容。 如果你修补了提交的内容，那么几乎肯定要更新提交消息以反映修改后的内容。另一方面，如果你的修补是琐碎的（如修改了一个笔误或添加了一个忘记暂存的文件）， 那么之前的提交信息不必修改，你只需作出更改，暂存它们，然后通过以下命令避免不必要的编辑器环节即可：$ git commit --amend --no-edit ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:1:2","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"修改多个提交信息 为了修改在提交历史中较远的提交，必须使用更复杂的工具。 Git 没有一个改变历史工具，但是可以使用变基工具来变基一系列提交，基于它们原来的 HEAD 而不是将其移动到另一个新的上面。 通过交互式变基工具，可以在任何想要修改的提交后停止，然后修改信息、添加文件或做任何想做的事情。 可以通过给 git rebase 增加 -i 选项来交互式地运行变基。 必须指定想要重写多久远的历史，这可以通过告诉命令将要变基到的提交来做到。 例如，如果想要修改最近三次提交信息，或者那组提交中的任意一个提交信息， 将想要修改的最近一次提交的父提交作为参数传递给 git rebase -i 命令，即 HEAD~2^ 或 HEAD~3。 记住 ~3 可能比较容易，因为你正尝试修改最后三次提交；但是注意实际上指定了以前的四次提交，即想要修改提交的父提交： $ git rebase -i HEAD~3 再次记住这是一个变基命令——在 HEAD~3..HEAD 范围内的每一个修改了提交信息的提交及其 所有后裔 都会被重写。 不要涉及任何已经推送到中央服务器的提交——这样做会产生一次变更的两个版本，因而使他人困惑。 运行这个命令会在文本编辑器上给你一个提交的列表，看起来像下面这样： pick f7f3f6d changed my name a bit pick 310154e updated README formatting and added blame pick a5f4a0d added cat-file # Rebase 710f0f8..a5f4a0d onto 710f0f8 # # Commands: # p, pick \u003ccommit\u003e = use commit # r, reword \u003ccommit\u003e = use commit, but edit the commit message # e, edit \u003ccommit\u003e = use commit, but stop for amending # s, squash \u003ccommit\u003e = use commit, but meld into previous commit # f, fixup \u003ccommit\u003e = like \"squash\", but discard this commit's log message # x, exec \u003ccommand\u003e = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop \u003ccommit\u003e = remove commit # l, label \u003clabel\u003e = label current HEAD with a name # t, reset \u003clabel\u003e = reset HEAD to a label # m, merge [-C \u003ccommit\u003e | -c \u003ccommit\u003e] \u003clabel\u003e [# \u003coneline\u003e] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c \u003ccommit\u003e to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out 需要重点注意的是相对于正常使用的 log 命令，这些提交显示的顺序是相反的。 运行一次 log 命令，会看到类似这样的东西： $ git log --pretty=format:\"%h %s\" HEAD~3..HEAD a5f4a0d added cat-file 310154e updated README formatting and added blame f7f3f6d changed my name a bit 注意其中的反序显示。 交互式变基给你一个它将会运行的脚本。 它将会从你在命令行中指定的提交（HEAD~3）开始，从上到下的依次重演每一个提交引入的修改。 它将最旧的而不是最新的列在上面，因为那会是第一个将要重演的。 你需要修改脚本来让它停留在你想修改的变更上。 要达到这个目的，你只要将你想修改的每一次提交前面的 ‘pick’ 改为 ‘edit’。 例如，只想修改第三次提交信息，可以像下面这样修改文件： edit f7f3f6d changed my name a bit pick 310154e updated README formatting and added blame pick a5f4a0d added cat-file 当保存并退出编辑器时，Git 将你带回到列表中的最后一次提交，把你送回命令行并提示以下信息： $ git rebase -i HEAD~3 Stopped at f7f3f6d... changed my name a bit You can amend the commit now, with git commit --amend Once you're satisfied with your changes, run git rebase --continue 这些指令准确地告诉你该做什么。 输入 $ git commit --amend 修改提交信息，然后退出编辑器。 然后，运行 $ git rebase --continue 这个命令将会自动地应用另外两个提交，然后就完成了。 如果需要将不止一处的 pick 改为 edit，需要在每一个修改为 edit 的提交上重复这些步骤。 每一次，Git 将会停止，让你修正提交，然后继续直到完成。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:2:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"重新排序提交 也可以使用交互式变基来重新排序或完全移除提交。 如果想要移除 “added cat-file” 提交然后修改另外两个提交引入的顺序，可以将变基脚本从这样： pick f7f3f6d changed my name a bit pick 310154e updated README formatting and added blame pick a5f4a0d added cat-file 改为这样： pick 310154e updated README formatting and added blame pick f7f3f6d changed my name a bit 当保存并退出编辑器时，Git 将你的分支带回这些提交的父提交，应用 310154e 然后应用 f7f3f6d，最后停止。 事实修改了那些提交的顺序并完全地移除了 “added cat-file” 提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:3:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"压缩提交 通过交互式变基工具，也可以将一连串提交压缩成一个单独的提交。 在变基信息中脚本给出了有用的指令： # # Commands: # p, pick \u003ccommit\u003e = use commit # r, reword \u003ccommit\u003e = use commit, but edit the commit message # e, edit \u003ccommit\u003e = use commit, but stop for amending # s, squash \u003ccommit\u003e = use commit, but meld into previous commit # f, fixup \u003ccommit\u003e = like \"squash\", but discard this commit's log message # x, exec \u003ccommand\u003e = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop \u003ccommit\u003e = remove commit # l, label \u003clabel\u003e = label current HEAD with a name # t, reset \u003clabel\u003e = reset HEAD to a label # m, merge [-C \u003ccommit\u003e | -c \u003ccommit\u003e] \u003clabel\u003e [# \u003coneline\u003e] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c \u003ccommit\u003e to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out 如果，指定 “squash” 而不是 “pick” 或 “edit”，Git 将应用两者的修改并合并提交信息在一起。 所以，如果想要这三次提交变为一个提交，可以这样修改脚本： pick f7f3f6d changed my name a bit squash 310154e updated README formatting and added blame squash a5f4a0d added cat-file 当保存并退出编辑器时，Git 应用所有的三次修改然后将你放到编辑器中来合并三次提交信息： # This is a combination of 3 commits. # The first commit's message is: changed my name a bit # This is the 2nd commit message: updated README formatting and added blame # This is the 3rd commit message: added cat-file 当你保存之后，你就拥有了一个包含前三次提交的全部变更的提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:4:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"拆分提交 拆分一个提交会撤消这个提交，然后多次地部分地暂存与提交直到完成你所需次数的提交。 例如，假设想要拆分三次提交的中间那次提交。 想要将它拆分为两次提交：第一个 “updated README formatting”，第二个 “added blame” 来代替原来的 “updated README formatting and added blame”。 可以通过修改 rebase -i 的脚本来做到这点，将要拆分的提交的指令修改为 “edit”： pick f7f3f6d changed my name a bit edit 310154e updated README formatting and added blame pick a5f4a0d added cat-file 然后，当脚本带你进入到命令行时，重置那个提交，拿到被重置的修改，从中创建几次提交。 当保存并退出编辑器时，Git 带你到列表中第一个提交的父提交，应用第一个提交（f7f3f6d）， 应用第二个提交（310154e），然后让你进入命令行。 那里，可以通过 git reset HEAD^ 做一次针对那个提交的混合重置，实际上将会撤消那次提交并将修改的文件取消暂存。 现在可以暂存并提交文件直到有几个提交，然后当完成时运行 git rebase --continue： $ git reset HEAD^ $ git add README $ git commit -m 'updated README formatting' $ git add lib/simplegit.rb $ git commit -m 'added blame' $ git rebase --continue Git 在脚本中应用最后一次提交（a5f4a0d），历史记录看起来像这样： $ git log -4 --pretty=format:\"%h %s\" 1c002dd added cat-file 9b29157 added blame 35cfb2b updated README formatting f3cc40e changed my name a bit 再次强调，这些改动了所有在列表中的提交的 SHA-1 校验和，所以要确保列表中的提交还没有推送到共享仓库中。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:5:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"核武器级选项：filter-branch 有另一个历史改写的选项，如果想要通过脚本的方式改写大量提交的话可以使用它——例如，全局修改你的邮箱地址或从每一个提交中移除一个文件。 这个命令是 filter-branch，它可以改写历史中大量的提交，除非你的项目还没有公开并且其他人没有基于要改写的工作的提交做的工作，否则你不应当使用它。 然而，它可以很有用。 你将会学习到几个常用的用途，这样就得到了它适合使用地方的想法。 Caution git filter-branch 有很多陷阱，不再推荐使用它来重写历史。 请考虑使用 git-filter-repo，它是一个 Python 脚本，相比大多数使用 filter-branch 的应用来说，它做得要更好。它的文档和源码可访问 https://github.com/newren/git-filter-repo 获取。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:6:0","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"从每一个提交中移除一个文件 这经常发生。 有人粗心地通过 git add . 提交了一个巨大的二进制文件，你想要从所有地方删除。 可能偶然地提交了一个包括一个密码的文件，然而你想要开源项目。 filter-branch 是一个可能会用来擦洗整个提交历史的工具。 为了从整个提交历史中移除一个叫做 passwords.txt 的文件，可以使用 --tree-filter 选项给 filter-branch： $ git filter-branch --tree-filter 'rm -f passwords.txt' HEAD Rewrite 6b9b3cf04e7c5686a9cb838c3f36a8cb6a0fc2bd (21/21) Ref 'refs/heads/master' was rewritten --tree-filter 选项在检出项目的每一个提交后运行指定的命令然后重新提交结果。 在本例中，你从每一个快照中移除了一个叫作 passwords.txt 的文件，无论它是否存在。 如果想要移除所有偶然提交的编辑器备份文件，可以运行类似 git filter-branch --tree-filter 'rm -f *~' HEAD 的命令。 最后将可以看到 Git 重写树与提交然后移动分支指针。 通常一个好的想法是在一个测试分支中做这件事，然后当你决定最终结果是真正想要的，可以硬重置 master 分支。 为了让 filter-branch 在所有分支上运行，可以给命令传递 --all 选项。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:6:1","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"使一个子目录做为新的根目录 假设已经从另一个源代码控制系统中导入，并且有几个没意义的子目录（trunk、tags 等等）。 如果想要让 trunk 子目录作为每一个提交的新的项目根目录，filter-branch 也可以帮助你那么做： $ git filter-branch --subdirectory-filter trunk HEAD Rewrite 856f0bf61e41a27326cdae8f09fe708d679f596f (12/12) Ref 'refs/heads/master' was rewritten 现在新项目根目录是 trunk 子目录了。 Git 会自动移除所有不影响子目录的提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:6:2","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"全局修改邮箱地址 另一个常见的情形是在你开始工作时忘记运行 git config 来设置你的名字与邮箱地址， 或者你想要开源一个项目并且修改所有你的工作邮箱地址为你的个人邮箱地址。 任何情形下，你也可以通过 filter-branch 来一次性修改多个提交中的邮箱地址。 需要小心的是只修改你自己的邮箱地址，所以你使用 --commit-filter： $ git filter-branch --commit-filter ' if [ \"$GIT_AUTHOR_EMAIL\" = \"schacon@localhost\" ]; then GIT_AUTHOR_NAME=\"Scott Chacon\"; GIT_AUTHOR_EMAIL=\"schacon@example.com\"; git commit-tree \"$@\"; else git commit-tree \"$@\"; fi' HEAD 这会遍历并重写每一个提交来包含你的新邮箱地址。 因为提交包含了它们父提交的 SHA-1 校验和，这个命令会修改你的历史中的每一个提交的 SHA-1 校验和， 而不仅仅只是那些匹配邮箱地址的提交。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/:6:3","tags":["Git"],"title":"Git工具-重写历史","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2/"},{"categories":["《Git》学习笔记"],"content":"Git 工具 - 重置揭密 在继续了解更专业的工具前，我们先探讨一下 Git 的 reset 和 checkout 命令。 在初遇的 Git 命令中，这两个是最让人困惑的。 它们能做很多事情，所以看起来我们很难真正地理解并恰当地运用它们。 针对这一点，我们先来做一个简单的比喻。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:0:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"三棵树 理解 reset 和 checkout 的最简方法，就是以 Git 的思维框架（将其作为内容管理器）来管理三棵不同的树。 “树” 在我们这里的实际意思是 “文件的集合”，而不是指特定的数据结构。 （在某些情况下索引看起来并不像一棵树，不过我们现在的目的是用简单的方式思考它。） Git 作为一个系统，是以它的一般操作来管理并操纵这三棵树的： 树 用途 HEAD 上一次提交的快照，下一次提交的父结点 Index 预期的下一次提交的快照 Working Directory 沙盒 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:1:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"HEAD HEAD 是当前分支引用的指针，它总是指向该分支上的最后一次提交。 这表示 HEAD 将是下一次提交的父结点。 通常，理解 HEAD 的最简方式，就是将它看做 该分支上的最后一次提交 的快照。 其实，查看快照的样子很容易。 下例就显示了 HEAD 快照实际的目录列表，以及其中每个文件的 SHA-1 校验和： $ git cat-file -p HEAD tree cfda3bf379e4f8dba8717dee55aab78aef7f4daf author Scott Chacon 1301511835 -0700 committer Scott Chacon 1301511835 -0700 initial commit $ git ls-tree -r HEAD 100644 blob a906cb2a4a904a152... README 100644 blob 8f94139338f9404f2... Rakefile 040000 tree 99f1a6d12cb4b6f19... lib Git 的 cat-file 和 ls-tree 是底层命令，它们一般用于底层工作，在日常工作中并不使用。 不过它们能帮助我们了解到底发生了什么。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:1:1","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"索引 索引是你的 预期的下一次提交。 我们也会将这个概念引用为 Git 的“暂存区”，这就是当你运行 git commit 时 Git 看起来的样子。 Git 将上一次检出到工作目录中的所有文件填充到索引区，它们看起来就像最初被检出时的样子。 之后你会将其中一些文件替换为新版本，接着通过 git commit 将它们转换为树来用作新的提交。 $ git ls-files -s 100644 a906cb2a4a904a152e80877d4088654daad0c859 0 README 100644 8f94139338f9404f26296befa88755fc2598c289 0 Rakefile 100644 47c6340d6459e05787f644c2447d2595f5d3a54b 0 lib/simplegit.rb 再说一次，我们在这里又用到了 git ls-files 这个幕后的命令，它会显示出索引当前的样子。 确切来说，索引在技术上并非树结构，它其实是以扁平的清单实现的。不过对我们而言，把它当做树就够了。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:1:2","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"工作目录 最后，你就有了自己的 工作目录（通常也叫 工作区）。 另外两棵树以一种高效但并不直观的方式，将它们的内容存储在 .git 文件夹中。 工作目录会将它们解包为实际的文件以便编辑。 你可以把工作目录当做 沙盒。在你将修改提交到暂存区并记录到历史之前，可以随意更改。 $ tree . ├── README ├── Rakefile └── lib └── simplegit.rb 1 directory, 3 files ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:1:3","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"工作流程 经典的 Git 工作流程是通过操纵这三个区域来以更加连续的状态记录项目快照的。 让我们来可视化这个过程：假设我们进入到一个新目录，其中有一个文件。 我们称其为该文件的 v1 版本，将它标记为蓝色。 现在运行 git init，这会创建一个 Git 仓库，其中的 HEAD 引用指向未创建的 master 分支。 此时，只有工作目录有内容。 现在我们想要提交这个文件，所以用 git add 来获取工作目录中的内容，并将其复制到索引中。 接着运行 git commit，它会取得索引中的内容并将它保存为一个永久的快照， 然后创建一个指向该快照的提交对象，最后更新 master 来指向本次提交。 此时如果我们运行 git status，会发现没有任何改动，因为现在三棵树完全相同。 现在我们想要对文件进行修改然后提交它。 我们将会经历同样的过程；首先在工作目录中修改文件。 我们称其为该文件的 v2 版本，并将它标记为红色。 如果现在运行 git status，我们会看到文件显示在 “Changes not staged for commit” 下面并被标记为红色，因为该条目在索引与工作目录之间存在不同。 接着我们运行 git add 来将它暂存到索引中。 此时，由于索引和 HEAD 不同，若运行 git status 的话就会看到 “Changes to be committed” 下的该文件变为绿色 ——也就是说，现在预期的下一次提交与上一次提交不同。 最后，我们运行 git commit 来完成提交。 现在运行 git status 会没有输出，因为三棵树又变得相同了。 切换分支或克隆的过程也类似。 当检出一个分支时，它会修改 HEAD 指向新的分支引用，将 索引 填充为该次提交的快照， 然后将 索引 的内容复制到 工作目录 中。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:2:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"重置的作用 在以下情景中观察 reset 命令会更有意义。 为了演示这些例子，假设我们再次修改了 file.txt 文件并第三次提交它。 现在的历史看起来是这样的： 让我们跟着 reset 看看它都做了什么。 它以一种简单可预见的方式直接操纵这三棵树。 它做了三个基本操作。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:3:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"第 1 步：移动 HEAD reset 做的第一件事是移动 HEAD 的指向。 这与改变 HEAD 自身不同（checkout 所做的）；reset 移动 HEAD 指向的分支。 这意味着如果 HEAD 设置为 master 分支（例如，你正在 master 分支上）， 运行 git reset 9e5e6a4 将会使 master 指向 9e5e6a4。 无论你调用了何种形式的带有一个提交的 reset，它首先都会尝试这样做。 使用 reset --soft，它将仅仅停在那儿。 现在看一眼上图，理解一下发生的事情：它本质上是撤销了上一次 git commit 命令。 当你在运行 git commit 时，Git 会创建一个新的提交，并移动 HEAD 所指向的分支来使其指向该提交。 当你将它 reset 回 HEAD~（HEAD 的父结点）时，其实就是把该分支移动回原来的位置，而不会改变索引和工作目录。 现在你可以更新索引并再次运行 git commit 来完成 git commit --amend 所要做的事情了（见 修改最后一次提交）。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:3:1","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"第 2 步：更新索引（–mixed） 注意，如果你现在运行 git status 的话，就会看到新的 HEAD 和以绿色标出的它和索引之间的区别。 接下来，reset 会用 HEAD 指向的当前快照的内容来更新索引。 如果指定 --mixed 选项，reset 将会在这时停止。 这也是默认行为，所以如果没有指定任何选项（在本例中只是 git reset HEAD~），这就是命令将会停止的地方。 现在再看一眼上图，理解一下发生的事情：它依然会撤销一上次 提交，但还会 取消暂存 所有的东西。 于是，我们回滚到了所有 git add 和 git commit 的命令执行之前。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:3:2","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"第 3 步：更新工作目录（–hard） reset 要做的的第三件事情就是让工作目录看起来像索引。 如果使用 --hard 选项，它将会继续这一步。 现在让我们回想一下刚才发生的事情。 你撤销了最后的提交、git add 和 git commit 命令 以及 工作目录中的所有工作。 必须注意，--hard 标记是 reset 命令唯一的危险用法，它也是 Git 会真正地销毁数据的仅有的几个操作之一。 其他任何形式的 reset 调用都可以轻松撤消，但是 --hard 选项不能，因为它强制覆盖了工作目录中的文件。 在这种特殊情况下，我们的 Git 数据库中的一个提交内还留有该文件的 v3 版本， 我们可以通过 reflog 来找回它。但是若该文件还未提交，Git 仍会覆盖它从而导致无法恢复。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:3:3","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"回顾 reset 命令会以特定的顺序重写这三棵树，在你指定以下选项时停止： 移动 HEAD 分支的指向 （若指定了 --soft，则到此停止） 使索引看起来像 HEAD （若未指定 --hard，则到此停止） 使工作目录看起来像索引 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:3:4","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"通过路径来重置 前面讲述了 reset 基本形式的行为，不过你还可以给它提供一个作用路径。 若指定了一个路径，reset 将会跳过第 1 步，并且将它的作用范围限定为指定的文件或文件集合。 这样做自然有它的道理，因为 HEAD 只是一个指针，你无法让它同时指向两个提交中各自的一部分。 不过索引和工作目录 可以部分更新，所以重置会继续进行第 2、3 步。 现在，假如我们运行 git reset file.txt （这其实是 git reset --mixed HEAD file.txt 的简写形式，因为你既没有指定一个提交的 SHA-1 或分支，也没有指定 --soft 或 --hard），它会： 移动 HEAD 分支的指向 （已跳过） 让索引看起来像 HEAD （到此处停止） 所以它本质上只是将 file.txt 从 HEAD 复制到索引中。 它还有 取消暂存文件 的实际效果。 如果我们查看该命令的示意图，然后再想想 git add 所做的事，就会发现它们正好相反。 这就是为什么 git status 命令的输出会建议运行此命令来取消暂存一个文件。 （查看 取消暂存的文件 来了解更多。） 我们可以不让 Git 从 HEAD 拉取数据，而是通过具体指定一个提交来拉取该文件的对应版本。 我们只需运行类似于 git reset eb43bf file.txt 的命令即可。 它其实做了同样的事情，也就是把工作目录中的文件恢复到 v1 版本，运行 git add 添加它， 然后再将它恢复到 v3 版本（只是不用真的过一遍这些步骤）。 如果我们现在运行 git commit，它就会记录一条“将该文件恢复到 v1 版本”的更改， 尽管我们并未在工作目录中真正地再次拥有它。 还有一点同 git add 一样，就是 reset 命令也可以接受一个 --patch 选项来一块一块地取消暂存的内容。 这样你就可以根据选择来取消暂存或恢复内容了。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:4:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"压缩 我们来看看如何利用这种新的功能来做一些有趣的事情——压缩提交。 假设你的一系列提交信息中有 “oops.”“WIP” 和 “forgot this file”， 聪明的你就能使用 reset 来轻松快速地将它们压缩成单个提交，也显出你的聪明。 （压缩提交 展示了另一种方式，不过在本例中用 reset 更简单。） 假设你有一个项目，第一次提交中有一个文件，第二次提交增加了一个新的文件并修改了第一个文件，第三次提交再次修改了第一个文件。 由于第二次提交是一个未完成的工作，因此你想要压缩它。 那么可以运行 git reset --soft HEAD~2 来将 HEAD 分支移动到一个旧一点的提交上（即你想要保留的最近的提交）： 然后只需再次运行 git commit： 现在你可以查看可到达的历史，即将会推送的历史，现在看起来有个 v1 版 file-a.txt 的提交， 接着第二个提交将 file-a.txt 修改成了 v3 版并增加了 file-b.txt。 包含 v2 版本的文件已经不在历史中了。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:5:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"检出 最后，你大概还想知道 checkout 和 reset 之间的区别。 和 reset 一样，checkout 也操纵三棵树，不过它有一点不同，这取决于你是否传给该命令一个文件路径。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:6:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"不带路径 运行 git checkout [branch] 与运行 git reset --hard [branch] 非常相似，它会更新所有三棵树使其看起来像 [branch]，不过有两点重要的区别。 首先不同于 reset --hard，checkout 对工作目录是安全的，它会通过检查来确保不会将已更改的文件弄丢。 其实它还更聪明一些。它会在工作目录中先试着简单合并一下，这样所有_还未修改过的_文件都会被更新。 而 reset --hard 则会不做检查就全面地替换所有东西。 第二个重要的区别是 checkout 如何更新 HEAD。 reset 会移动 HEAD 分支的指向，而 checkout 只会移动 HEAD 自身来指向另一个分支。 例如，假设我们有 master 和 develop 分支，它们分别指向不同的提交；我们现在在 develop 上（所以 HEAD 指向它）。 如果我们运行 git reset master，那么 develop 自身现在会和 master 指向同一个提交。 而如果我们运行 git checkout master 的话，develop 不会移动，HEAD 自身会移动。 现在 HEAD 将会指向 master。 所以，虽然在这两种情况下我们都移动 HEAD 使其指向了提交 A，但_做法_是非常不同的。 reset 会移动 HEAD 分支的指向，而 checkout 则移动 HEAD 自身。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:6:1","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"带路径 运行 checkout 的另一种方式就是指定一个文件路径，这会像 reset 一样不会移动 HEAD。 它就像 git reset [branch] file 那样用该次提交中的那个文件来更新索引，但是它也会覆盖工作目录中对应的文件。 它就像是 git reset --hard [branch] file（如果 reset 允许你这样运行的话）， 这样对工作目录并不安全，它也不会移动 HEAD。 此外，同 git reset 和 git add 一样，checkout 也接受一个 --patch 选项，允许你根据选择一块一块地恢复文件内容。 ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:6:2","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"总结 希望你现在熟悉并理解了 reset 命令，不过关于它和 checkout 之间的区别，你可能还是会有点困惑，毕竟不太可能记住不同调用的所有规则。 下面的速查表列出了命令对树的影响。 “HEAD” 一列中的 “REF” 表示该命令移动了 HEAD 指向的分支引用，而 “HEAD” 则表示只移动了 HEAD 自身。 特别注意 WD Safe? 一列——如果它标记为 NO，那么运行该命令之前请考虑一下。 HEAD Index Workdir WD Safe? Commit Level reset --soft [commit] REF NO NO YES reset [commit] REF YES NO YES reset --hard [commit] REF YES YES NO checkout \u003ccommit\u003e HEAD YES YES YES File Level reset [commit] \u003cpaths\u003e NO YES NO YES checkout [commit] \u003cpaths\u003e NO YES YES NO ","date":"2020-11-18","objectID":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/:7:0","tags":["Git"],"title":"Git工具-重置揭密","uri":"/posts/git%E5%B7%A5%E5%85%B7-%E9%87%8D%E7%BD%AE%E6%8F%AD%E5%AF%86/"},{"categories":["《Git》学习笔记"],"content":"Git基础与命令 官方文档（中文）：https://git-scm.com/book/zh/v2 本文档是根据官方文档来编写的，以官方文档为准。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:0:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"Git基础 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"全局配置 git config --global user.name 'your name' git config --global user.email 'xxx@xx.com' 自报家门 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:1","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"检查配置信息 git config --list ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:2","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"获取帮助 # 获取全局帮助手册 git help # 获取特定命令的详细版帮助手册 (两个命令是等价的) git help \u003c某个命令\u003e git \u003c某个命令\u003e --help # 两个横杠 # 获取特定命令的简明版帮助手册 git \u003c某个命令\u003e -h # 一个横杠 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:3","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"初始化仓库 # 本地目录初始化仓库 git init 如果你是从远程仓库clone的项目，则该项目是已经初始化好的git仓库 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:4","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"克隆远程仓库 # 克隆 git clone \u003curl\u003e # 克隆同时修改目录名 git clone \u003curl\u003e \u003cname\u003e 初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态，因为 Git 刚刚检出了它们， 而你尚未编辑过它们 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:5","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"检查文件状态 # 查看详细状态说明 git status # 查看简明状态说明 git status -s # -s 或 --short M README # 已修改，但未暂存 （M的位置靠右，红色） MM Rakefile # 已修改，暂存后又作了修改（有暂存和未暂存） A lib/git.rb # 新添加到暂存区，未提交 M lib/simplegit.rb # 已修改，已暂存 （M的位置靠左，绿色） ?? LICENSE.txt # 新添加，未跟踪 git目录中的文件状态包含：是否跟踪、是否修改、是否已存入暂存区 参数的一个横杠表示缩写，两个横杠表示全称。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:6","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"加入暂存区 (跟踪文件) # 文件加入暂存区（跟踪指定文件) git add \u003cfiles\u003e git add 命令使用文件或目录的路径作为参数；如果参数是目录的路径，该命令将递归地跟踪该目录下的所有文件。 add 命令是将文件加入到暂存区，commit 命令的提交到本地仓库，push 命令是推送到远程仓库。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:7","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"忽略文件 添加一个名为 .gitignore 的文件，列出要忽略的文件的模式 *.[oa] # 忽略以 .o 或 .a 结尾的文件（一般这类文件是编译过程出现） *~ # 忽略以 ~ 结尾的文件（一般是文本编辑软件保存的副本） 文件 .gitignore 的格式规范如下： 所有空行或者以 # 开头的行都会被 Git 忽略（注释符号）。 可以使用标准的 glob 模式匹配，它会递归地应用在整个工作区中。 glob 模式是指 shell 所使用的简化了的正则表达式 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上叹号（!）取反。 星号（*）匹配零个或多个任意字符 [abc] 匹配任何一个列在方括号中的字符 （这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c） 问号（?）只匹配一个任意字符 [0-9] 表示匹配所有 0 到 9 的数字。在方括号中使用短划线分隔两个字符， 表示所有在这两个字符范围内的都可以匹配 使用两个星号（**）表示匹配任意中间目录，比如 a/**/z 可以匹配 a/z 、 a/b/z 或 a/b/c/z 等。 # 忽略所有的 .a 文件 *.a # 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件 !lib.a # 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO /TODO # 忽略任何目录下名为 build 的文件夹 build/ # 忽略 doc/notes.txt，但不忽略 doc/server/arch.txt doc/*.txt # 忽略 doc/ 目录及其所有子目录下的 .pdf 文件 doc/**/*.pdf GitHub 有一个十分详细的针对数十种项目及语言的 .gitignore 文件列表， 你可以在 https://github.com/github/gitignore 找到它。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:8","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"查看修改的具体内容 git diff # 比较修改之后还没有暂存起来的变化内容。 git diff --staged # 查看已暂存的将要添加到下次提交里的内容 git status 只能查看文件变动的状态，并不能查看具体修改了哪些内容。使用git diff可以看到具体变动的内容。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:9","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"提交更新 git commit # 未带参数的会打开默认文本编辑器让你输入提交说明 git commit -m '提交说明' # 带-m参数直接输入提交说明 使用git commit提交更新，在此之前，务必确认所有变动已经被git add添加到暂存区。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:10","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"跳过使用暂存区域 git commit -a -m '提交说明' 添加-a选项可以跳过git add 步骤，把已经跟踪过的文件一并提交。 注意：这个操作无法提交未跟踪的文件。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:1:11","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"Git 基础 - 查看提交历史 git log 不传入任何参数的默认情况下，git log 会按时间先后顺序列出所有的提交，最近的更新排在最上面。 此命令打印的数据中有一项是一长串的 SHA-1 校验码。 带入-p或--patch 查看提交的具体差异： git log -p -2 # -p显示差异 -2显示最近的提交次数 --stat 显示每次提交的差异统计 git log --stat --pretty 这个选项可以使用不同于默认格式的方式展示提交历史 这个选项有一些内建的子选项供你使用。 比如 oneline 会将每个提交放在一行显示，在浏览大量的提交时非常有用。 另外还有 short，full 和 fuller 选项，它们展示信息的格式基本一致，但是详尽程度不一： $ git log --pretty=oneline ca82a6dff817ec66f44342007202690a93763949 changed the version number 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary test a11bef06a3f659402fe7563abf99ad00de2209e6 first commit 最有意思的是 format ，可以定制记录的显示格式。 这样的输出对后期提取分析格外有用——因为你知道输出的格式不会随着 Git 的更新而发生改变： $ git log --pretty=format:\"%h - %an, %ar : %s\" ca82a6d - Scott Chacon, 6 years ago : changed the version number 085bb3b - Scott Chacon, 6 years ago : removed unnecessary test a11bef0 - Scott Chacon, 6 years ago : first commit git log --pretty=format 常用的选项 列出了 format 接受的常用格式占位符的写法及其代表的意义。 当 oneline 或 format 与另一个 log 选项 --graph 结合使用时尤其有用。 这个选项添加了一些 ASCII 字符串来形象地展示你的分支、合并历史： $ git log --pretty=format:\"%h %s\" --graph * 2d3acf9 ignore errors from SIGCHLD on trap * 5e3ee11 Merge branch 'master' of git://github.com/dustin/grit |\\ | * 420eac9 Added a method for getting the current branch. * | 30e367c timeout code and tests * | 5a09431 add timeout protection to grit * | e1193f8 support for heads with slashes in them |/ * d6016bc require time for xmlschema * 11d191e Merge branch 'defunkt' into local ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:2:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"Git 基础 - 撤消操作 你提交后发现忘记了暂存某些需要的修改，可以像下面这样操作： $ git commit -m 'initial commit' $ git add forgotten_file $ git commit --amend # 重新提交，且只有一次提交记录 最终你只会有一个提交——第二次提交将代替第一次提交的结果。 更多撤销操作请了解 reset命令。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:3:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"Git 基础 - 远程仓库的使用 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"查看远程仓库 git remote # 仅显示远程仓库的名称 git remote -v # 显示远程仓库的名称 + 地址 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:1","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"添加远程仓库 git remote add \u003c远程仓库名\u003e \u003curl\u003e ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:2","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"从远程仓库中抓取与拉取 就如刚才所见，从远程仓库中获得数据，可以执行： git fetch \u003cremote\u003e 这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 注意： git fetch 命令只会将数据下载到你的本地仓库——它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 git pull 用 git pull 命令来自动抓取后合并该远程分支到当前分支。 这或许是个更加简单舒服的工作流程。默认情况下，git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master 分支（或其它名字的默认分支）。 运行 git pull 通常会从最初克隆的服务器上抓取数据并自动尝试合并到当前所在的分支。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:3","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"推送到远程仓库 git push \u003cremote\u003e \u003cbranch\u003e # git push origin master ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:4","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"查看某个远程仓库 git remote show \u003cremote\u003e # git remote show origin 查看远程仓库的详细信息。这个命令列出了当你在特定的分支上执行 git push 会自动地推送到哪一个远程分支 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:5","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"远程仓库的重命名与移除 git remote rename \u003c原名\u003e \u003c新名\u003e # 重命名 git remote remove paul \u003cremote\u003e# 移除远程仓库 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:4:6","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"Git 基础 - 打标签 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"列出标签 git tag # 完整标签列表 git tag -l \"v2.0*\" # 只显示包含 v2.0 的标签。 注意加星号(*) -l 或 --list 都可以。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:1","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"创建标签 Git 支持两种标签：轻量标签（lightweight）与附注标签（annotated）。 轻量标签很像一个不会改变的分支——它只是某个特定提交的引用。 而附注标签是存储在 Git 数据库中的一个完整对象， 它们是可以被校验的，其中包含打标签者的名字、电子邮件地址、日期时间， 此外还有一个标签信息，并且可以使用 GNU Privacy Guard （GPG）签名并验证。 通常会建议创建附注标签，这样你可以拥有以上所有信息。但是如果你只是想用一个临时的标签， 或者因为某些原因不想要保存这些信息，那么也可以用轻量标签。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:2","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"附注标签 git tag -a v1.4 -m \"my version 1.4\" # -a表示add， -m 表示附件信息 通过使用 git show 命令可以看到标签信息和与之对应的提交信息： git show v1.4 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:3","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"轻量标签 轻量标签本质上是将提交校验和存储到一个文件中——没有保存任何其他信息。 创建轻量标签，不需要使用 -a、-s 或 -m 选项，只需要提供标签名字： git tag v1.4-lw # 不需要添加选项 这时，如果在标签上运行 git show，你不会看到额外的标签信息。 命令只会显示出提交信息： $ git show v1.4-lw commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon \u003cschacon@gee-mail.com\u003e Date: Mon Mar 17 21:52:11 2008 -0700 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:4","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"后期打标签 你也可以对过去的提交打标签。 假设提交历史是这样的： $ git log --pretty=oneline 166ae0c4d3f420721acbb115cc33848dfcc2121a started write support 9fceb02d0ae598e95dc970b74767f19372d61af8 updated rakefile 8a5cbc430f1a9c3d00faaeffd07798508422908a updated readme 现在，假设在 v1.2 时你忘记给项目打标签，也就是在 “updated rakefile” 提交。 你可以在之后补上标签。 要在那个提交上打标签，你需要在命令的末尾指定提交的校验和（或部分校验和）： $ git tag -a v1.2 9fceb02 # 打的标签属于附注标签 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:5","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"共享标签 git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。 这个过程就像共享远程分支一样——你可以运行 git push origin \u003ctagname\u003e。 git push origin v1.5 # 显式地推送标签到远程仓库 git push origin --tags # 一次性推送所有不在远程仓库上的标签 现在，当其他人从仓库中克隆或拉取，他们也能得到你的那些标签。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:6","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"删除标签 要删除掉你本地仓库上的标签，可以使用命令 git tag -d \u003ctagname\u003e。 例如，可以使用以下命令删除一个轻量标签： $ git tag -d v1.4-lw Deleted tag 'v1.4-lw' (was e7d5add) 注意上述命令并不会从任何远程仓库中移除这个标签，你必须用 git push \u003cremote\u003e :refs/tags/\u003ctagname\u003e 来更新你的远程仓库： 第一种变体是 git push \u003cremote\u003e :refs/tags/\u003ctagname\u003e ： $ git push origin :refs/tags/v1.4-lw To /git@github.com:schacon/simplegit.git - [deleted] v1.4-lw 上面这种操作的含义是，将冒号前面的空值推送到远程标签名，从而高效地删除它。 第二种更直观的删除远程标签的方式是： $ git push origin --delete \u003ctagname\u003e ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:7","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"检出标签 如果你想查看某个标签所指向的文件版本，可以使用 git checkout 命令， 虽然这会使你的仓库处于“分离头指针（detached HEAD）”的状态——这个状态有些不好的副作用： $ git checkout 2.0.0 Note: checking out '2.0.0'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b \u003cnew-branch\u003e HEAD is now at 99ada87... Merge pull request #89 from schacon/appendix-final $ git checkout 2.0-beta-0.1 Previous HEAD position was 99ada87... Merge pull request #89 from schacon/appendix-final HEAD is now at df3f601... add atlas.json and cover image 在“分离头指针”状态下，如果你做了某些更改然后提交它们，标签不会发生变化， 但你的新提交将不属于任何分支，并且将无法访问，除非通过确切的提交哈希才能访问。 因此，如果你需要进行更改，比如你要修复旧版本中的错误，那么通常需要创建一个新分支： $ git checkout -b version2 v2.0.0 Switched to a new branch 'version2' 如果在这之后又进行了一次提交，version2 分支就会因为这个改动向前移动， 此时它就会和 v2.0.0 标签稍微有些不同，这时就要当心了。 ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:5:8","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"Git 命令别名 Git 并不会在你输入部分命令时自动推断出你想要的命令。 如果不想每次都输入完整的 Git 命令，可以通过 git config 文件来轻松地为每一个命令设置一个别名。 这里有一些例子你可以试试： $ git config --global alias.co checkout $ git config --global alias.br branch $ git config --global alias.ci commit $ git config --global alias.st status 这意味着，当要输入 git commit 时，只需要输入 git ci。 在创建你认为应该存在的命令时这个技术会很有用。 例如，为了解决取消暂存文件的易用性问题，可以向 Git 中添加你自己的取消暂存别名： $ git config --global alias.unstage 'reset HEAD --' 这会使下面的两个命令等价： $ git unstage fileA $ git reset HEAD -- fileA 这样看起来更清楚一些。 通常也会添加一个 last 命令，像这样： $ git config --global alias.last 'log -1 HEAD' 这样，可以轻松地看到最后一次提交： $ git last commit 66938dae3329c7aebe598c2246a8e6af90d04646 Author: Josh Goebel \u003cdreamer3@example.com\u003e Date: Tue Aug 26 19:48:51 2008 +0800 test for current head Signed-off-by: Scott Chacon \u003cschacon@example.com\u003e 可以看出，Git 只是简单地将别名替换为对应的命令。 然而，你可能想要执行外部命令，而不是一个 Git 子命令。 如果是那样的话，可以在命令前面加入 ! 符号。 如果你自己要写一些与 Git 仓库协作的工具的话，那会很有用。 我们现在演示将 git visual 定义为 gitk 的别名： $ git config --global alias.visual '!gitk' ","date":"2020-11-18","objectID":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/:6:0","tags":["Git"],"title":"Git基础与命令","uri":"/posts/git%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%91%BD%E4%BB%A4/"},{"categories":["《Git》学习笔记"],"content":"常用Git命令清单 一般来说，日常使用只要记住下图6个命令，就可以了。但是熟练使用，恐怕要记住60～100个命令。 下面是我整理的常用 Git 命令清单。几个专用名词的译名如下。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:0:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"一、新建代码库 # 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:1:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"二、配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name \"[name]\" $ git config [--global] user.email \"[email address]\" ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:2:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"三、增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:3:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"四、代码提交 # 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:4:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"五、分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:5:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"六、标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:6:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"七、查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其\"提交说明\"必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat \"@{0 day ago}\" # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:7:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"八、远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:8:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"九、撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:9:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"十、常用操作组合 ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:10:0","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":["《Git》学习笔记"],"content":"1. 修改本地分支名和远程分支名 git branch -m old_branch new_branch # 重命名本地分支 git push origin :old_branch # 删除远程旧分支（分支名前有冒号） git push --set-upstream origin new_branch # 推送新的分支，并设置本地分支跟踪新的远程分支 相关文章： 《如何撤销 Git 操作？》 《git cherry-pick 教程》 复制某分支上的部分提交到另一个分支上（相对于可以选择指定提交的 rebase 操作）。 命令清单来源：https://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html ","date":"2020-11-18","objectID":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/:10:1","tags":["Git"],"title":"常用Git命令清单","uri":"/posts/%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"},{"categories":null,"content":"一般情况下我们部署的 POD 是通过集群自动调度选择某个节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 POD 的调度，比如我们内部的一些服务 gitlab 之类的也是跑在Kubernetes集群上的，我们就不希望对外的一些服务和内部的服务跑在同一个节点上了，害怕内部服务对外部的服务产生影响；有的时候呢我们两个服务直接交流比较频繁，又希望能够将这两个服务的 POD 调度到同样的节点上。这就需要用到 Kubernetes 里面的一个概念：亲和性，亲和性主要分为两类：nodeAffinity和podAffinity。 ","date":"2018-03-08","objectID":"/posts/understand-kubernetes-affinity/:0:0","tags":["kubernetes","affinity","调度"],"title":"理解 Kubernetes 的亲和性调度","uri":"/posts/understand-kubernetes-affinity/"},{"categories":null,"content":"nodeSelector 我们知道label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的一个就是 service 通过匹配 label 去选择 POD 的。而 POD 的调度也可以根据节点的 label 进行特定的部署。 我们可以通过下面的命令查看我们的 node 的 label： $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS 192.168.1.140 Ready \u003cnone\u003e 42d v1.8.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.1.140 192.168.1.161 Ready \u003cnone\u003e 118d v1.8.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/cluster-service=true,kubernetes.io/hostname=192.168.1.161 192.168.1.170 Ready \u003cnone\u003e 118d v1.8.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/cluster-service=true,kubernetes.io/hostname=192.168.1.170 192.168.1.172 Ready \u003cnone\u003e 114d v1.8.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/cluster-service=true,kubernetes.io/hostname=192.168.1.172 现在我们先给节点192.168.1.140增加一个source=qikqiak的标签，命令如下： $ kubectl label nodes 192.168.1.140 source=qikqiak node \"192.168.1.140\" labeled 我们可以通过上面的--show-labels参数可以查看上述标签是否生效。当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 POD 的 spec 字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label。例如，下面是我们之前的一个默认的 busybox POD 的 YAML 文件： apiVersion: v1 kind: Pod metadata: labels: app: busybox-pod name: test-busybox spec: containers: - command: - sleep - \"3600\" image: busybox imagePullPolicy: Always name: test-busybox 然后我需要让上面的 POD 被调度到140的节点上，那么最简单的方法就是去匹配140上面的 label，如下： apiVersion: v1 kind: Pod metadata: labels: app: busybox-pod name: test-busybox spec: containers: - command: - sleep - \"3600\" image: busybox imagePullPolicy: Always name: test-busybox nodeSelector: source: qikqiak 然后我们可以通过 describe 命令查看调度结果： $ kubectl describe pod test-busybox ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 49s default-scheduler Successfully assigned test-busybox to 192.168.1.140 Normal SuccessfulMountVolume 49s kubelet, 192.168.1.140 MountVolume.SetUp succeeded for volume \"default-token-hmpbz\" Normal Pulling 49s kubelet, 192.168.1.140 pulling image \"busybox\" Normal Pulled 41s kubelet, 192.168.1.140 Successfully pulled image \"busybox\" Normal Created 41s kubelet, 192.168.1.140 Created container Normal Started 41s kubelet, 192.168.1.140 Started container 我们可以看到 Events 下面的信息，上面的 POD 被正确的调度到了140节点。通过上面的例子我们可以感受到nodeSelector的方式比较直观，但是还够灵活，控制粒度偏大，下面我们再看另外一种更加灵活的方式：nodeAffinity。 ","date":"2018-03-08","objectID":"/posts/understand-kubernetes-affinity/:1:0","tags":["kubernetes","affinity","调度"],"title":"理解 Kubernetes 的亲和性调度","uri":"/posts/understand-kubernetes-affinity/"},{"categories":null,"content":"nodeAffinity nodeAffinity就是节点亲和性，相对应的是Anti-Affinity，就是反亲和性，这种方法比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。 调度可以分成软策略和硬策略两种方式，软策略就是如果你没有满足调度要求的节点的话，POD 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓了的策略；而硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不干的策略。 nodeAffinity就有两上面两种策略：preferredDuringSchedulingIgnoredDuringExecution和requiredDuringSchedulingIgnoredDuringExecution，前面的就是软策略，后面的就是硬策略。 如下例子：（test-node-affinity.yaml） apiVersion: v1 kind: Pod metadata: name: with-node-affinity labels: app: node-affinity-pod spec: containers: - name: with-node-affinity image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - 192.168.1.140 - 192.168.1.161 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: source operator: In values: - qikqiak 上面这个 POD 首先是要求 POD 不能运行在140和161两个节点上，如果有个节点满足source=qikqiak的话就优先调度到这个节点上，同样的我们可以使用descirbe命令查看具体的调度情况是否满足我们的要求。这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 POD。 ","date":"2018-03-08","objectID":"/posts/understand-kubernetes-affinity/:2:0","tags":["kubernetes","affinity","调度"],"title":"理解 Kubernetes 的亲和性调度","uri":"/posts/understand-kubernetes-affinity/"},{"categories":null,"content":"podAffinity 上面两种方式都是让 POD 去选择节点的，有的时候我们也希望能够根据 POD 之间的关系进行调度，Kubernetes在1.4版本引入的podAffinity概念就可以实现我们这个需求。 和nodeAffinity类似，podAffinity也有requiredDuringSchedulingIgnoredDuringExecution和 preferredDuringSchedulingIgnoredDuringExecution两种调度策略，唯一不同的是如果要使用互斥性，我们需要使用podAntiAffinity字段。 如下例子，我们希望with-pod-affinity和busybox-pod能够就近部署，而不希望和node-affinity-pod部署在同一个拓扑域下面：（test-pod-affinity.yaml） apiVersion: v1 kind: Pod metadata: name: with-pod-affinity labels: app: pod-affinity-pod spec: containers: - name: with-pod-affinity image: nginx affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - busybox-pod topologyKey: kubernetes.io/hostname podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - node-affinity-pod topologyKey: kubernetes.io/hostname 上面这个例子中的 POD 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 POD：这个 POD 有一个app=busybox-pod的 label。podAntiAffinity则是希望最好不要调度到这样的节点：这个节点上运行了某个 POD，而这个 POD 有app=node-affinity-pod的 label。根据前面两个 POD 的定义，我们可以预见上面这个 POD 应该会被调度到140的节点上，因为busybox-pod被调度到了140节点，而node-affinity-pod被调度到了140以为的节点，正好满足上面的需求。通过describe查看： $ kubectl describe pod with-pod-affinity ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 8s default-scheduler Successfully assigned with-pod-affinity to 192.168.1.140 Normal SuccessfulMountVolume 7s kubelet, 192.168.1.140 MountVolume.SetUp succeeded for volume \"default-token-lcl77\" Normal Pulling 7s kubelet, 192.168.1.140 pulling image \"nginx\" 上面的事件信息也验证了我们的想法。 在labelSelector和 topologyKey的同级，还可以定义 namespaces 列表，表示匹配哪些 namespace 里面的 pod，默认情况下，会匹配定义的 pod 所在的 namespace；如果定义了这个字段，但是它的值为空，则匹配所有的 namespaces。 查看上面我们定义的3个 POD 结果： $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE test-busybox 1/1 Running 0 8m 172.30.95.18 192.168.1.140 with-node-affinity 1/1 Running 0 10m 172.30.81.25 192.168.1.172 with-pod-affinity 1/1 Running 0 8m 172.30.95.17 192.168.1.140 亲和性/反亲和性调度策略比较如下： 调度策略 匹配标签 操作符 拓扑域支持 调度目标 nodeAffinity 主机 In, NotIn, Exists, DoesNotExist, Gt, Lt 否 指定主机 podAffinity POD In, NotIn, Exists, DoesNotExist 是 POD与指定POD同一拓扑域 podAnitAffinity POD In, NotIn, Exists, DoesNotExist 是 POD与指定POD不在同一拓扑域 ","date":"2018-03-08","objectID":"/posts/understand-kubernetes-affinity/:3:0","tags":["kubernetes","affinity","调度"],"title":"理解 Kubernetes 的亲和性调度","uri":"/posts/understand-kubernetes-affinity/"},{"categories":null,"content":"污点（Taints）与容忍（tolerations） 对于nodeAffinity无论是硬策略还是软策略方式，都是调度 POD 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 POD 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度pod。 比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 POD，则污点就很有用了，POD 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下： $ kubectl taint nodes 192.168.1.40 key=value:NoSchedule node \"192.168.1.40\" tainted 如果仍然希望某个 POD 调度到 taint 节点上，则必须在 Spec 中做出Toleration定义，才能调度到该节点，举例如下： tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\" effect 共有三个可选项，可按实际需求进行设置： NoSchedule：POD 不会被调度到标记为 taints 节点。 PreferNoSchedule：NoSchedule 的软策略版本。 NoExecute：该选项意味着一旦 Taint 生效，如该节点内正在运行的 POD 没有对应 Tolerate 设置，会直接被逐出。 ","date":"2018-03-08","objectID":"/posts/understand-kubernetes-affinity/:4:0","tags":["kubernetes","affinity","调度"],"title":"理解 Kubernetes 的亲和性调度","uri":"/posts/understand-kubernetes-affinity/"},{"categories":null,"content":"参考资料 https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ https://coreos.com/fleet/docs/latest/affinity.html ","date":"2018-03-08","objectID":"/posts/understand-kubernetes-affinity/:5:0","tags":["kubernetes","affinity","调度"],"title":"理解 Kubernetes 的亲和性调度","uri":"/posts/understand-kubernetes-affinity/"},{"categories":null,"content":"离线 - 蜷缩的蜗牛","date":"0001-01-01","objectID":"/offline/","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"Hey Welcome here 👋 ","date":"0001-01-01","objectID":"/offline/:1:0","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"目前正在学习Service Mesh ","date":"0001-01-01","objectID":"/offline/:2:0","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"汇编和工具: 🛠 Check for a detailed stats here :point_right: Sourcerer ","date":"0001-01-01","objectID":"/offline/:3:0","tags":null,"title":"","uri":"/offline/"},{"categories":null,"content":"结构体转换 YAML 转 GO (新增): www.printlove.cn/tools/yaml2go JSON 转 GO: www.printlove.cn/tools/json2go SQL 转 GORM Model: www.printlove.cn/tools/sql2gorm SQL 转 entgo schema: printlove.cn/tools/sql2ent SQL 转 go-zero Model: printlove.cn/tools/sql2gozero ","date":"0001-01-01","objectID":"/posts/:0:1","tags":null,"title":"","uri":"/posts/"}]